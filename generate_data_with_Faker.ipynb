{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need test data?\n",
    "\n",
    "Here's a quick way to generate some fake data using the Python `Faker` library ([https://faker.readthedocs.io/](https://faker.readthedocs.io/)).\n",
    "\n",
    "**Note:** this is not _synthetic_ data as it is generated with simple methods and will most likely not fit any real-life distribution. Still, it can be useful for test purposes when no data is at hand.\n",
    "\n",
    "Import `Faker` and set a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "# Set the seed value of the shared `random.Random` object\n",
    "# across all internal generators that will ever be created\n",
    "Faker.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fake` is a fake data generator with `DE_de` locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker('de_DE')\n",
    "fake.seed_locale('de_DE', 42)\n",
    "# Creates and seeds a unique `random.Random` object for\n",
    "# each internal generator of this `Faker` instance\n",
    "fake.seed_instance(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pandas to dave data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_row_faker` creates one row of fake data. Here we choose to generate a row containing the following fields:\n",
    " - `fake.name()`\n",
    " - `fake.postcode()`\n",
    " - `fake.email()`\n",
    " - `fake.country()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_faker(num=1):\n",
    "    output = [{\"name\": fake.name(),\n",
    "               \"age\": fake.random_int(0, 100),\n",
    "               \"postcode\": fake.postcode(),\n",
    "               \"email\": fake.email(),\n",
    "               \"nationality\": fake.country(),\n",
    "              } for x in range(num)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Alida Harloff',\n",
       "  'age': 28,\n",
       "  'postcode': '18196',\n",
       "  'email': 'gnatzdajana@rosenow.net',\n",
       "  'nationality': 'Grönland'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_row_faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataframe `df_fake` of 5000 rows using `create_row_faker`. \n",
    "\n",
    "We're using the _cell magic_ `%%time` to time the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 s, sys: 6.27 ms, total: 2.64 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_fake = pd.DataFrame(create_row_faker(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>postcode</th>\n",
       "      <th>email</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prof. Carolina Kade</td>\n",
       "      <td>35</td>\n",
       "      <td>51161</td>\n",
       "      <td>jochembeer@dippel.com</td>\n",
       "      <td>Osttimor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gisela Flantz B.Sc.</td>\n",
       "      <td>84</td>\n",
       "      <td>41316</td>\n",
       "      <td>olaf55@haering.de</td>\n",
       "      <td>Niue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nuri Ehlert</td>\n",
       "      <td>48</td>\n",
       "      <td>83503</td>\n",
       "      <td>djunck@yahoo.de</td>\n",
       "      <td>Namibia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deborah Kroker</td>\n",
       "      <td>82</td>\n",
       "      <td>42388</td>\n",
       "      <td>polina65@zorbach.com</td>\n",
       "      <td>Antigua und Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonardo Schottin</td>\n",
       "      <td>54</td>\n",
       "      <td>66978</td>\n",
       "      <td>sonja18@schlosser.com</td>\n",
       "      <td>Guernsey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Ing. Angelique Bender</td>\n",
       "      <td>51</td>\n",
       "      <td>29000</td>\n",
       "      <td>ecarsten@zaenker.net</td>\n",
       "      <td>Serbien und Montenegro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Dr. Dietmar Kabus B.Eng.</td>\n",
       "      <td>9</td>\n",
       "      <td>20621</td>\n",
       "      <td>hahnclarissa@loewer.com</td>\n",
       "      <td>Brasilien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Rose Cichorius B.Sc.</td>\n",
       "      <td>16</td>\n",
       "      <td>12841</td>\n",
       "      <td>sieglindestahr@googlemail.com</td>\n",
       "      <td>Äthiopien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Prof. Rigo Seifert</td>\n",
       "      <td>46</td>\n",
       "      <td>24254</td>\n",
       "      <td>brittkoehler@gmail.com</td>\n",
       "      <td>Suriname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Philipp Losekann</td>\n",
       "      <td>53</td>\n",
       "      <td>87528</td>\n",
       "      <td>gfaust@aol.de</td>\n",
       "      <td>Türkei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  age postcode                          email  \\\n",
       "0          Prof. Carolina Kade   35    51161          jochembeer@dippel.com   \n",
       "1          Gisela Flantz B.Sc.   84    41316              olaf55@haering.de   \n",
       "2                  Nuri Ehlert   48    83503                djunck@yahoo.de   \n",
       "3               Deborah Kroker   82    42388           polina65@zorbach.com   \n",
       "4            Leonardo Schottin   54    66978          sonja18@schlosser.com   \n",
       "...                        ...  ...      ...                            ...   \n",
       "4995     Ing. Angelique Bender   51    29000           ecarsten@zaenker.net   \n",
       "4996  Dr. Dietmar Kabus B.Eng.    9    20621        hahnclarissa@loewer.com   \n",
       "4997      Rose Cichorius B.Sc.   16    12841  sieglindestahr@googlemail.com   \n",
       "4998        Prof. Rigo Seifert   46    24254         brittkoehler@gmail.com   \n",
       "4999          Philipp Losekann   53    87528                  gfaust@aol.de   \n",
       "\n",
       "                 nationality  \n",
       "0                   Osttimor  \n",
       "1                       Niue  \n",
       "2                    Namibia  \n",
       "3        Antigua und Barbuda  \n",
       "4                   Guernsey  \n",
       "...                      ...  \n",
       "4995  Serbien und Montenegro  \n",
       "4996               Brasilien  \n",
       "4997               Äthiopien  \n",
       "4998                Suriname  \n",
       "4999                  Türkei  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more fake data generators see Faker's [standard providers](https://faker.readthedocs.io/en/master/providers.html#standard-providers) as well as [community providers](https://faker.readthedocs.io/en/master/communityproviders.html#community-providers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataLAB demo fake data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(create_row_faker(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid getting the warning, either use [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row) and let Spark infer datatypes or create a schema for the dataframe specifying the datatypes of all fields (here's the list of all [datatypes](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField('name', StringType()),\n",
    "                     StructField('age',IntegerType()),\n",
    "                     StructField('postcode',StringType()),\n",
    "                     StructField('email', StringType()), \n",
    "                     StructField('nationality',StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(create_row_faker(5000), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some more data (dataframe with $5\\cdot10^4$ rows). The file will be partitioned by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 s, sys: 137 ms, total: 27.1 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 5*10**4\n",
    "df = spark.createDataFrame(create_row_faker(n), schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a long time (27 sec. for 50000 rows)!\n",
    "\n",
    "Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_row_faker()` returns a list. This is not efficient, what we need is a _generator_ instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = create_row_faker(5)\n",
    "# what type is d?\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `d` is a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ({\"name\": fake.name(),\n",
    "      \"age\": fake.random_int(0, 100),\n",
    "      \"postcode\": fake.postcode(),\n",
    "      \"email\": fake.email(),\n",
    "      \"nationality\": fake.country()} for i in range(5))\n",
    "# what type is d?\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 158 ms, total: 27.6 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 5*10**4\n",
    "d = ({\"name\": fake.name(),\n",
    "      \"age\": fake.random_int(0, 100),\n",
    "      \"postcode\": fake.postcode(),\n",
    "      \"email\": fake.email(),\n",
    "      \"nationality\": fake.country()} \n",
    "     for i in range(n))\n",
    "df = spark.createDataFrame(d, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wasn't faster.\n",
    "\n",
    "I will look into how one can leverage Hadoop's parallelism to generate dataframes and speed the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first five records in the dataframe of fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------+--------------------+----------------+\n",
      "|                name|age|postcode|               email|     nationality|\n",
      "+--------------------+---+--------+--------------------+----------------+\n",
      "|     Natalija Textor| 91|   27481|  kreising@gmail.com|         Eritrea|\n",
      "| Dietmar Preiß-Nette| 79|   78578|rosabiggen@doersc...|Äquatorialguinea|\n",
      "|    Stanislav Scholz|  7|   57091|fjockel@googlemai...|      Martinique|\n",
      "|         Marian Gute| 64|   91681|emilhartmann@goog...|            Guam|\n",
      "|Dipl.-Ing. Klaus ...| 63|   98220|jacobi-jaeckelges...|       Nicaragua|\n",
      "+--------------------+---+--------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some data aggregation:\n",
    " - group by postcode\n",
    " - count the number of persons and the average age for each postcode\n",
    " - filter out postcodes with less than 4 persons\n",
    " - sort by average age descending\n",
    " - show the first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------+\n",
      "|postcode|Count|Average age|\n",
      "+--------+-----+-----------+\n",
      "|   86678|    4|       90.0|\n",
      "|   23084|    4|       87.5|\n",
      "|   89884|    4|      86.75|\n",
      "|   99646|    4|       84.5|\n",
      "|   96353|    4|       82.0|\n",
      "+--------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.groupBy('postcode') \\\n",
    "  .agg(F.count('postcode').alias('Count'), F.round(F.avg('age'), 2).alias('Average age')) \\\n",
    "  .filter('Count>3') \\\n",
    "  .orderBy('Average age', ascending=False) \\\n",
    "  .show(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postcode $86678$ has the highest average age ($90$). Show all entries for postcode $86678$ using `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------+--------------------+--------------------+\n",
      "|                name|age|postcode|               email|         nationality|\n",
      "+--------------------+---+--------+--------------------+--------------------+\n",
      "|  Klaus Peter Weller| 90|   86678|dgeissler@benthin...|Russische Föderation|\n",
      "|Anthony Wohlgemut...| 92|   86678|rmangold@googlema...|           Venezuela|\n",
      "|Ing. Albertine Sc...| 84|   86678|awieloch@mosemann.de|      Marshallinseln|\n",
      "|    Klaus-Peter Kaul| 94|   86678|  angelasalz@roht.de|               China|\n",
      "+--------------------+---+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('postcode==86678').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "We are going to use multiple locales with weights (following the [examples](https://faker.readthedocs.io/en/master/fakerclass.html#examples) in the documentation). \n",
    "\n",
    "Here's the [list of all available locales](https://faker.readthedocs.io/en/master/locales.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "# set a seed for the random generator\n",
    "Faker.seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de_DE', 'de_AT']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "locales = OrderedDict([\n",
    "    ('de_DE', 5), \n",
    "    ('de_AT', 2),\n",
    "])\n",
    "fake = Faker(locales)\n",
    "fake.seed_instance(42)\n",
    "fake.locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.seed_locale('de_DE', 0)\n",
    "fake.seed_locale('de_AT', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_location': (Decimal('73.9837235'), Decimal('163.824695')),\n",
       " 'blood_group': 'O-',\n",
       " 'name': 'Eva Enzinger',\n",
       " 'sex': 'F',\n",
       " 'mail': 'rvogl@kabsi.at',\n",
       " 'birthdate': datetime.date(2017, 10, 27)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.profile(fields=['name', 'birthdate', 'sex', 'blood_group', \n",
    "                     'mail', 'current_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "location = StructField('current_location',\n",
    "                       StructType([StructField('lat', DecimalType()),\n",
    "                                   StructField('lon', DecimalType())])\n",
    "                      )\n",
    "schema = StructType([StructField('name', StringType()),\n",
    "                     StructField('birthdate', DateType()),\n",
    "                     StructField('sex', StringType()),\n",
    "                     StructField('blood_group', StringType()),\n",
    "                     StructField('mail', StringType()), \n",
    "                     location\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_location': (Decimal('-50.7121925'), Decimal('-62.546840')),\n",
       " 'blood_group': 'A+',\n",
       " 'name': 'Aleksander Wiesinger',\n",
       " 'sex': 'M',\n",
       " 'mail': 'marctischler@gmail.com',\n",
       " 'birthdate': datetime.date(1982, 6, 29)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.profile(fields=['name', 'birthdate', 'sex', 'blood_group', \n",
    "                     'mail', 'current_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataLAB demo fake data - part 2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with $5\\cdot10^3$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.45 s, sys: 50 ms, total: 9.5 s\n",
      "Wall time: 9.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 5*10**3\n",
    "d = (fake.profile(fields=['name', 'birthdate', 'sex', 'blood_group', \n",
    "                          'mail', 'current_location']) \n",
    "     for i in range(n))\n",
    "df = spark.createDataFrame(d, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- blood_group: string (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- current_location: struct (nullable = true)\n",
      " |    |-- lat: decimal(10,0) (nullable = true)\n",
      " |    |-- lon: decimal(10,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `location` represents a _tuple_ data structure (a `StructType` of `StructField`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+-----------+--------------------+----------------+\n",
      "|                name| birthdate|sex|blood_group|                mail|current_location|\n",
      "+--------------------+----------+---+-----------+--------------------+----------------+\n",
      "|         Ilija Stroh|1986-02-06|  M|        AB-|emueller@googlema...|       [-5, 148]|\n",
      "|     Philomena Hesse|2002-09-06|  F|         A+|naserhans-hermann...|        [82, 12]|\n",
      "| Prof. Annelise Mude|1910-03-30|  F|         A-|ftschentscher@aol.de|       [29, 177]|\n",
      "|       Branka Hamann|1970-06-08|  F|         A+|fheinz@googlemail...|      [78, -136]|\n",
      "|       Lilli Lercher|1996-12-01|  F|         A-|arnoldlouise@kabs...|     [-15, -108]|\n",
      "|  Hans-Karl Fröhlich|2000-12-23|  M|         A+|    wlosekann@aol.de|       [-32, 47]|\n",
      "|Hanife Mitschke MBA.|1986-11-15|  F|        AB-|    hkramer@yahoo.de|      [64, -131]|\n",
      "|Ing. Susi Weiß B....|1914-07-09|  F|         O-|        lwiek@aol.de|      [85, -113]|\n",
      "|         Anika Knoll|2007-01-02|  F|         O+|     dprem@chello.at|      [62, -157]|\n",
      "|Hans-Jochen Röhri...|1998-08-11|  M|        AB-|oschuchhardt@goog...|      [-10, 117]|\n",
      "+--------------------+----------+---+-----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Write to parquet](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=write#pyspark.sql.DataFrameWriter.parquet) file ([Parquet](http://parquet.apache.org/) is a compressed, efficient columnar data representation compatible with all frameworks in the Hadoop ecosystem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"fakedata.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of parquet file (it is actually a directory containing the partitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2020-12-12 12:28 fakedata.parquet/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup     73.6 K 2020-12-12 12:28 fakedata.parquet/part-00000-81067930-f1a5-41c4-a699-1ca436547bf9-c000.snappy.parquet\n",
      "-rw-r--r--   3 datalab supergroup    103.4 K 2020-12-12 12:28 fakedata.parquet/part-00001-81067930-f1a5-41c4-a699-1ca436547bf9-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h fakedata.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to close the Spark session when you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
