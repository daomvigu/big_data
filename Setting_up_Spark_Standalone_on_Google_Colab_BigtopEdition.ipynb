{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DeF-OtwDrXVY",
        "SCoxn5xDimni",
        "JaaWPSEqsAyO",
        "m8hiZOGUr2FD",
        "7WND_44npS_d",
        "n8q4BIe316Ae",
        "ewBl-o1aH5hh",
        "LoPO2VrqEVCk",
        "AZuz6m48UyoO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Setting_up_Spark_Standalone_on_Google_Colab_BigtopEdition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\" alt=\"Logo Big Data for Beginners\"></div></a>\n",
        "# Install and run Spark in standalone mode—Apache Bigtop edition <div><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></div>\n",
        "\n",
        "<br>\n",
        "\n",
        "We will install Apache Spark on a single machine (the virtual machine hosting this notebook) in _standalone mode_, meaning it will run without any cluster manager like YARN, Mesos, or Kubernetes. For more information, see the [types of cluster managers supported by Spark](https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)).\n",
        "\n",
        "We're following the official [Spark Standalone documentation](https://spark.apache.org/docs/latest/spark-standalone.html), using Apache Bigtop's Spark distribution, which conveniently packages Spark's start scripts as services.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Before running this notebook, you may want to update the Bigtop version (currently version `3.2.1` from  2023-08-22 [with Hadoop 3.3.5](https://bigtop.apache.org/release-notes.html), see also the [full list of releases](https://bigtop.apache.org/download.html))."
      ],
      "metadata": {
        "id": "DeF-OtwDrXVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A side note:"
      ],
      "metadata": {
        "id": "Ayk0pz16sxMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://spark.apache.org/\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></a>\n",
        "<a href=\"https://bigtop.apache.org/\"><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"></a>\n",
        "\n",
        "\n",
        "I recently discovered [a website](https://www.apache.org/logos/) where you can find all Apache project logos, including Spark, with transparent backgrounds. It’s a great resource for anyone needing these assets for presentations or documentation. <p>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uribYroGLPlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark from Bigtop repository"
      ],
      "metadata": {
        "id": "8XOex6ixsSml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Add Bigtop repository\n",
        "echo \"Adding Bigtop repository...\"\n",
        "curl -o /etc/apt/sources.list.d/bigtop-3.2.1.list https://archive.apache.org/dist/bigtop/bigtop-3.2.1/repos/$(lsb_release -is | tr '[:upper:]' '[:lower:]')-$(lsb_release -rs)/bigtop.list\n",
        "\n",
        "# Download and add the Bigtop GPG key\n",
        "echo \"Adding Bigtop GPG key...\"\n",
        "wget --no-clobber -qO - https://archive.apache.org/dist/bigtop/bigtop-3.2.1/repos/GPG-KEY-bigtop | sudo apt-key add -\n",
        "\n",
        "# Update package cache\n",
        "echo \"Updating package cache...\"\n",
        "apt update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFoSvMF9rlW",
        "outputId": "21025522-b0ed-49c1-8d6d-8bc0cecdd674"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Bigtop repository...\n",
            "Adding Bigtop GPG key...\n",
            "OK\n",
            "Updating package cache...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop InRelease [2,502 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Ign:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,650 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,372 kB]\n",
            "Get:17 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 Packages [18.2 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,032 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,596 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,403 kB]\n",
            "Fetched 18.9 MB in 5s (3,560 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    86  100    86    0     0    153      0 --:--:-- --:--:-- --:--:--   153\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64/dists/bigtop/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Bigtop and Spark packages"
      ],
      "metadata": {
        "id": "oShuNuhgtJwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo 'List all available packages that match \"bigtop\"'\n",
        "apt search bigtop\n",
        "\n",
        "echo 'List all available packages that match \"spark\"'\n",
        "apt search  spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNfPDnHqBE4p",
        "outputId": "8774a255-f982-460c-efa0-62d63fe1c026"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all available packages that match \"bigtop\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "bigtop-ambari-mpack/stable 2.7.5.0-1 all\n",
            "  Ambari Mpack\n",
            "\n",
            "bigtop-groovy/stable 2.5.4-1 all\n",
            "  An agile and dynamic language for the Java Virtual Machine\n",
            "\n",
            "bigtop-jsvc/stable 1.2.4-1 amd64\n",
            "  Application to launch java daemon\n",
            "\n",
            "bigtop-utils/stable 3.2.1-1 all\n",
            "  Collection of useful tools for Bigtop\n",
            "\n",
            "List all available packages that match \"spark\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "alluxio/stable 2.8.0-2 all\n",
            "  Reliable file sharing at memory speed across cluster frameworks\n",
            "\n",
            "libjs-jquery.sparkline/jammy 2.1.2-3 all\n",
            "  library for jQuery to generate sparklines\n",
            "\n",
            "libsparkline-php/jammy 0.2-7 all\n",
            "  sparkline graphing library for php\n",
            "\n",
            "livy/stable 0.7.1-1 all\n",
            "  Livy is an open source REST interface for interacting with Apache Spark from anywhere.\n",
            "\n",
            "node-sparkles/jammy 1.0.1-2 all\n",
            "  Namespaced global event emitter\n",
            "\n",
            "nspark/jammy 1.7.8B2+git20210317.cb30779-2 amd64\n",
            "  Unarchiver for Spark and ArcFS files\n",
            "\n",
            "pcp-export-pcp2spark/jammy 5.3.6-1build1 amd64\n",
            "  Tool for exporting data from PCP to Apache Spark\n",
            "\n",
            "python3-sahara-plugin-spark/jammy 7.0.0-0ubuntu1 all\n",
            "  OpenStack data processing cluster as a service - Spark plugin\n",
            "\n",
            "python3-sparkpost/jammy 1.3.10-1 all\n",
            "  SparkPost Python API client (Python 3)\n",
            "\n",
            "r-cran-analysispipelines/jammy 1.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'analysisPipelines' (Compose Interoperable Analysis Pipelines & Put Them inProduction)\n",
            "\n",
            "r-cran-apache.sedona/jammy 1.6.1-1.ca2204.1 all\n",
            "  CRAN Package 'apache.sedona' (R Interface for Apache Sedona)\n",
            "\n",
            "r-cran-catalog/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'catalog' (Access the 'Spark Catalog' API via 'sparklyr')\n",
            "\n",
            "r-cran-databaseconnector/jammy 6.3.2-1.ca2204.1 all\n",
            "  CRAN Package 'DatabaseConnector' (Connecting to Various Database Platforms)\n",
            "\n",
            "r-cran-geospark/jammy 0.3.1-1.ca2204.1 all\n",
            "  CRAN Package 'geospark' (Bring Local Sf to Spark)\n",
            "\n",
            "r-cran-ggspark/jammy 0.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'ggspark' ('ggplot2' Functions to Create Tufte Style Sparklines)\n",
            "\n",
            "r-cran-graphframes/jammy 0.1.2-1.ca2204.1 all\n",
            "  CRAN Package 'graphframes' (Interface for 'GraphFrames')\n",
            "\n",
            "r-cran-ibmdbr/jammy 1.51.0-1.ca2204.1 all\n",
            "  CRAN Package 'ibmdbR' (IBM in-Database Analytics for R)\n",
            "\n",
            "r-cran-ltxsparklines/jammy 1.1.3-1.ca2204.1 all\n",
            "  CRAN Package 'ltxsparklines' (Lightweight Sparklines for a LaTeX Document)\n",
            "\n",
            "r-cran-microplot/jammy 1.0-45-1.ca2204.1 all\n",
            "  CRAN Package 'microplot' (Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel')\n",
            "\n",
            "r-cran-notebookutils/jammy 1.5.3-1.ca2204.1 all\n",
            "  CRAN Package 'notebookutils' (Dummy R APIs Used in 'Azure Synapse Analytics' for LocalDevelopments)\n",
            "\n",
            "r-cran-oenokpm/jammy 2.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'OenoKPM' (Modeling the Kinetics of Carbon Dioxide Production in AlcoholicFermentation)\n",
            "\n",
            "r-cran-parsnip/jammy 1.2.1-1.ca2204.1 all\n",
            "  CRAN Package 'parsnip' (A Common API to Modeling and Analysis Functions)\n",
            "\n",
            "r-cran-paws.analytics/jammy 0.7.0-1.ca2204.1 all\n",
            "  CRAN Package 'paws.analytics' ('Amazon Web Services' Analytics Services)\n",
            "\n",
            "r-cran-pmmltransformations/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'pmmlTransformations' (Transforms Input Data from a PMML Perspective)\n",
            "\n",
            "r-cran-pointblank/jammy 0.12.1-1.ca2204.1 all\n",
            "  CRAN Package 'pointblank' (Data Validation and Organization of Metadata for Local andRemote Tables)\n",
            "\n",
            "r-cran-pysparklyr/jammy 0.1.5-1.ca2204.1 all\n",
            "  CRAN Package 'pysparklyr' (Provides a 'PySpark' Back-End for the 'sparklyr' Package)\n",
            "\n",
            "r-cran-reactablefmtr/jammy 2.0.0-1.ca2204.1 all\n",
            "  CRAN Package 'reactablefmtr' (Streamlined Table Styling and Formatting for Reactable)\n",
            "\n",
            "r-cran-rquery/jammy 1.4.99-1.ca2204.1 all\n",
            "  CRAN Package 'rquery' (Relational Query Generator for Data Manipulation at Scale)\n",
            "\n",
            "r-cran-rsparkling/jammy 0.2.19-1.ca2204.1 all\n",
            "  CRAN Package 'rsparkling' (R Interface for H2O Sparkling Water)\n",
            "\n",
            "r-cran-s3.resourcer/jammy 1.1.1-1.ca2204.1 all\n",
            "  CRAN Package 's3.resourcer' (S3 Resource Resolver)\n",
            "\n",
            "r-cran-shinyml/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'shinyML' (Compare Supervised Machine Learning Models Using Shiny App)\n",
            "\n",
            "r-cran-skimr/jammy 2.1.5-1.ca2204.1 all\n",
            "  CRAN Package 'skimr' (Compact and Flexible Summaries of Data)\n",
            "\n",
            "r-cran-spark.sas7bdat/jammy 1.4-1.ca2204.1 all\n",
            "  CRAN Package 'spark.sas7bdat' (Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkavro/jammy 0.3.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkavro' (Load Avro file into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkbq/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkbq' (Google 'BigQuery' Support for 'sparklyr')\n",
            "\n",
            "r-cran-sparkhail/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkhail' (A 'Sparklyr' Extension for 'Hail')\n",
            "\n",
            "r-cran-sparkline/jammy 2.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkline' ('jQuery' Sparkline 'htmlwidget')\n",
            "\n",
            "r-cran-sparklyr/jammy 1.8.6-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr' (R Interface to Apache Spark)\n",
            "\n",
            "r-cran-sparklyr.flint/jammy 0.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.flint' (Sparklyr Extension for 'Flint')\n",
            "\n",
            "r-cran-sparklyr.nested/jammy 0.0.4-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.nested' (A 'sparklyr' Extension for Nested Data)\n",
            "\n",
            "r-cran-sparktex/jammy 0.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparktex' (Generate LaTeX sparklines in R)\n",
            "\n",
            "r-cran-sparktf/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparktf' (Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark')\n",
            "\n",
            "r-cran-sparkwarc/jammy 0.1.6-1.ca2204.1 amd64\n",
            "  CRAN Package 'sparkwarc' (Load WARC Files into Apache Spark)\n",
            "\n",
            "r-cran-sparkxgb/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkxgb' (Interface for 'XGBoost' on 'Apache Spark')\n",
            "\n",
            "r-cran-sqlrender/jammy 1.19.0-1.ca2204.1 all\n",
            "  CRAN Package 'SqlRender' (Rendering Parameterized SQL and Translation to Dialects)\n",
            "\n",
            "r-cran-tidier/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'tidier' (Enhanced 'mutate')\n",
            "\n",
            "r-cran-timevizpro/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'TimeVizPro' (Dynamic Data Explorer: Visualize and Forecast with 'TimeVizPro')\n",
            "\n",
            "r-cran-variantspark/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'variantspark' (A 'Sparklyr' Extension for 'VariantSpark')\n",
            "\n",
            "spark-core/stable 3.2.3-1 all\n",
            "  Lightning-Fast Cluster Computing\n",
            "\n",
            "spark-datanucleus/stable 3.2.3-1 all\n",
            "  DataNucleus libraries for Apache Spark\n",
            "\n",
            "spark-external/stable 3.2.3-1 all\n",
            "  External libraries for Apache Spark\n",
            "\n",
            "spark-history-server/stable 3.2.3-1 all\n",
            "  History server for Apache Spark\n",
            "\n",
            "spark-master/stable 3.2.3-1 all\n",
            "  Server for Spark master\n",
            "\n",
            "spark-python/stable 3.2.3-1 all\n",
            "  Python client for Spark\n",
            "\n",
            "spark-sparkr/stable 3.2.3-1 all\n",
            "  R package for Apache Spark\n",
            "\n",
            "spark-thriftserver/stable 3.2.3-1 all\n",
            "  Thrift server for Spark SQL\n",
            "\n",
            "spark-worker/stable 3.2.3-1 all\n",
            "  Server for Spark worker\n",
            "\n",
            "spark-yarn-shuffle/stable 3.2.3-1 all\n",
            "  Spark YARN Shuffle Service\n",
            "\n",
            "sparkleshare/jammy 3.28+git20190525+cf446c0-3 all\n",
            "  distributed collaboration and sharing tool\n",
            "\n",
            "zeppelin/stable 0.10.1-1 all\n",
            "  Web-based notebook for data analysts\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the essential packages\n",
        "\n",
        "In order to run a Spark job, we need the core libraries as well as the Spark master and Spark worker. Master and worker in this case are going to run both on the same machine, the localhost.\n",
        "\n",
        "The package `bigtop-utils` will be used to start the services."
      ],
      "metadata": {
        "id": "j8_0tUQ8tesJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-core spark-master spark-worker bigtop-utils; do\n",
        "  echo \"🛠️ Installing $p\"\n",
        "  apt install -y $p\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCNjbZUe-69F",
        "outputId": "1cee713f-8c4d-4748-d4ee-d2ca197e3767"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛠️ Installing spark-core\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-client hadoop-hdfs hadoop-mapreduce\n",
            "  hadoop-yarn netcat-openbsd zookeeper\n",
            "The following NEW packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-client hadoop-hdfs hadoop-mapreduce\n",
            "  hadoop-yarn netcat-openbsd spark-core zookeeper\n",
            "0 upgraded, 11 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 707 MB of archives.\n",
            "After this operation, 869 MB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-utils all 3.2.1-1 [5,432 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netcat-openbsd amd64 1.218-4ubuntu1 [39.4 kB]\n",
            "Get:3 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-groovy all 2.5.4-1 [4,832 kB]\n",
            "Get:4 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-jsvc amd64 1.2.4-1 [29.0 kB]\n",
            "Get:5 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 zookeeper all 3.5.9-2 [16.7 MB]\n",
            "Get:6 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop amd64 3.3.5-1 [310 MB]\n",
            "Get:7 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-hdfs amd64 3.3.5-1 [57.7 MB]\n",
            "Get:8 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-yarn amd64 3.3.5-1 [52.0 MB]\n",
            "Get:9 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-mapreduce amd64 3.3.5-1 [4,874 kB]\n",
            "Get:10 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-client amd64 3.3.5-1 [5,360 B]\n",
            "Get:11 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-core all 3.2.3-1 [261 MB]\n",
            "Fetched 707 MB in 27s (26.0 MB/s)\n",
            "Selecting previously unselected package netcat-openbsd.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123629 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-netcat-openbsd_1.218-4ubuntu1_amd64.deb ...\r\n",
            "Unpacking netcat-openbsd (1.218-4ubuntu1) ...\r\n",
            "Selecting previously unselected package bigtop-utils.\r\n",
            "Preparing to unpack .../01-bigtop-utils_3.2.1-1_all.deb ...\r\n",
            "Unpacking bigtop-utils (3.2.1-1) ...\r\n",
            "Selecting previously unselected package bigtop-groovy.\r\n",
            "Preparing to unpack .../02-bigtop-groovy_2.5.4-1_all.deb ...\r\n",
            "Unpacking bigtop-groovy (2.5.4-1) ...\r\n",
            "Selecting previously unselected package bigtop-jsvc.\r\n",
            "Preparing to unpack .../03-bigtop-jsvc_1.2.4-1_amd64.deb ...\r\n",
            "Unpacking bigtop-jsvc (1.2.4-1) ...\r\n",
            "Selecting previously unselected package zookeeper.\r\n",
            "Preparing to unpack .../04-zookeeper_3.5.9-2_all.deb ...\r\n",
            "Unpacking zookeeper (3.5.9-2) ...\r\n",
            "Selecting previously unselected package hadoop.\r\n",
            "Preparing to unpack .../05-hadoop_3.3.5-1_amd64.deb ...\r\n",
            "Unpacking hadoop (3.3.5-1) ...\r\n",
            "Selecting previously unselected package hadoop-hdfs.\r\n",
            "Preparing to unpack .../06-hadoop-hdfs_3.3.5-1_amd64.deb ...\r\n",
            "Unpacking hadoop-hdfs (3.3.5-1) ...\r\n",
            "Selecting previously unselected package hadoop-yarn.\r\n",
            "Preparing to unpack .../07-hadoop-yarn_3.3.5-1_amd64.deb ...\r\n",
            "Unpacking hadoop-yarn (3.3.5-1) ...\r\n",
            "Selecting previously unselected package hadoop-mapreduce.\r\n",
            "Preparing to unpack .../08-hadoop-mapreduce_3.3.5-1_amd64.deb ...\r\n",
            "Unpacking hadoop-mapreduce (3.3.5-1) ...\r\n",
            "Selecting previously unselected package hadoop-client.\r\n",
            "Preparing to unpack .../09-hadoop-client_3.3.5-1_amd64.deb ...\r\n",
            "Unpacking hadoop-client (3.3.5-1) ...\r\n",
            "Selecting previously unselected package spark-core.\r\n",
            "Preparing to unpack .../10-spark-core_3.2.3-1_all.deb ...\r\n",
            "Unpacking spark-core (3.2.3-1) ...\r\n",
            "Setting up netcat-openbsd (1.218-4ubuntu1) ...\r\n",
            "update-alternatives: using /bin/nc.openbsd to provide /bin/nc (nc) in auto mode\r\n",
            "Setting up bigtop-utils (3.2.1-1) ...\r\n",
            "Setting up zookeeper (3.5.9-2) ...\r\n",
            "update-alternatives: using /etc/zookeeper/conf.dist to provide /etc/zookeeper/conf (zookeeper-conf) in auto mode\r\n",
            "Setting up bigtop-groovy (2.5.4-1) ...\r\n",
            "Setting up hadoop (3.3.5-1) ...\r\n",
            "update-alternatives: using /etc/hadoop/conf.empty to provide /etc/hadoop/conf (hadoop-conf) in auto mode\r\n",
            "Setting up bigtop-jsvc (1.2.4-1) ...\r\n",
            "Setting up hadoop-yarn (3.3.5-1) ...\r\n",
            "Setting up hadoop-hdfs (3.3.5-1) ...\r\n",
            "Setting up hadoop-mapreduce (3.3.5-1) ...\r\n",
            "Setting up hadoop-client (3.3.5-1) ...\r\n",
            "Setting up spark-core (3.2.3-1) ...\r\n",
            "update-alternatives: using /etc/spark/conf.dist to provide /etc/spark/conf (spark-conf) in auto mode\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "🛠️ Installing spark-master\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  spark-master\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,692 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-master all 3.2.3-1 [4,692 B]\n",
            "Fetched 4,692 B in 0s (41.2 kB/s)\n",
            "Selecting previously unselected package spark-master.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126974 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-master_3.2.3-1_all.deb ...\r\n",
            "Unpacking spark-master (3.2.3-1) ...\r\n",
            "Setting up spark-master (3.2.3-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "🛠️ Installing spark-worker\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  spark-worker\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,662 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.2.1/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-worker all 3.2.3-1 [4,662 B]\n",
            "Fetched 4,662 B in 0s (24.4 kB/s)\n",
            "Selecting previously unselected package spark-worker.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126978 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-worker_3.2.3-1_all.deb ...\r\n",
            "Unpacking spark-worker (3.2.3-1) ...\r\n",
            "Setting up spark-worker (3.2.3-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "🛠️ Installing bigtop-utils\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "bigtop-utils is already the newest version (3.2.1-1).\n",
            "bigtop-utils set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note;** in a future version of this notebook we are going to use an alternative to `apt` for installing packages in order to avoid the warning\n",
        "\n",
        "```\n",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "IFxPmikEu0An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Spark services\n",
        "\n",
        "Thanks to the Bigtop's utilities, we can now start the Spark master and Spark worker as services. Normally one would use `systemctl` but since this is not allowed on Colab, we are going to resort to `service`."
      ],
      "metadata": {
        "id": "3jx-t17PvHGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-master spark-worker; do\n",
        "  echo \"Starting $p\"\n",
        "  # systemctl start $p\n",
        "  service $p start\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6lMkulx-X2I",
        "outputId": "e71138c7-efce-483e-e192-6cf90d4099a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting spark-master\n",
            " * Starting Spark master (spark-master): \n",
            "Starting spark-worker\n",
            " * Starting Spark worker (spark-worker): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the `pi` example"
      ],
      "metadata": {
        "id": "6erQkYR6v7nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step may take some time.\n",
        "\n",
        "We'll run the `SparkPi` demo from the examples included in the Spark distribution, which are packaged in the `spark-examples*.jar` file.\n",
        "\n",
        "We'll submit the job using [`spark-submit`](https://spark.apache.org/docs/latest/submitting-applications.html), and the output will be an approximation of π (for more details, see the [official Spark examples](https://spark.apache.org/examples.html).\n",
        "\n",
        "\n",
        "The following code defines the variable `$EXAMPLE_JAR`, which points to the archive containing all the examples from the Spark distribution.\n",
        "\n",
        "The following command submits the SparkPi application (located in the `org.apache.spark.examples.SparkPi` class) to the Spark master at `spark://${HOSTNAME}:7077` using `spark-submit`:\n",
        "\n",
        "```\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100\n",
        "```\n",
        "\n",
        "In this example, the number $100$ represents the number of iterations used to compute an approximation of π by calculating the ratio of points inside versus outside the unit circle."
      ],
      "metadata": {
        "id": "DyfvrPtVVCfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export EXAMPLES_JAR=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100"
      ],
      "metadata": {
        "id": "a4COXhlrR1mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d96322f-f88d-47cc-cb54-86029df536ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi is roughly 3.1423215142321514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "2024-10-21 11:30:54,510 INFO spark.SparkContext: Running Spark version 3.2.3\n",
            "2024-10-21 11:30:55,572 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "2024-10-21 11:30:56,049 INFO resource.ResourceUtils: ==============================================================\n",
            "2024-10-21 11:30:56,053 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
            "2024-10-21 11:30:56,055 INFO resource.ResourceUtils: ==============================================================\n",
            "2024-10-21 11:30:56,061 INFO spark.SparkContext: Submitted application: Spark Pi\n",
            "2024-10-21 11:30:56,236 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "2024-10-21 11:30:56,317 INFO resource.ResourceProfile: Limiting resource is cpu\n",
            "2024-10-21 11:30:56,321 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
            "2024-10-21 11:30:56,590 INFO spark.SecurityManager: Changing view acls to: root\n",
            "2024-10-21 11:30:56,590 INFO spark.SecurityManager: Changing modify acls to: root\n",
            "2024-10-21 11:30:56,606 INFO spark.SecurityManager: Changing view acls groups to: \n",
            "2024-10-21 11:30:56,607 INFO spark.SecurityManager: Changing modify acls groups to: \n",
            "2024-10-21 11:30:56,609 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "2024-10-21 11:30:58,303 INFO util.Utils: Successfully started service 'sparkDriver' on port 42907.\n",
            "2024-10-21 11:30:58,391 INFO spark.SparkEnv: Registering MapOutputTracker\n",
            "2024-10-21 11:30:58,449 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
            "2024-10-21 11:30:58,486 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "2024-10-21 11:30:58,487 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "2024-10-21 11:30:58,494 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "2024-10-21 11:30:58,537 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-61039088-6a50-407d-9bfc-3468355d5978\n",
            "2024-10-21 11:30:58,574 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "2024-10-21 11:30:58,602 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "2024-10-21 11:30:58,765 INFO util.log: Logging initialized @9826ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "2024-10-21 11:30:58,885 INFO server.Server: jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.24+8-post-Ubuntu-1ubuntu322.04\n",
            "2024-10-21 11:30:58,915 INFO server.Server: Started @9977ms\n",
            "2024-10-21 11:30:58,972 INFO server.AbstractConnector: Started ServerConnector@138068d5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
            "2024-10-21 11:30:58,972 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "2024-10-21 11:30:59,015 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c46dcbe{/jobs,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,020 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71f0b72e{/jobs/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,022 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f508f3c{/jobs/job,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,027 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46731692{/jobs/job/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,030 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db663d0{/stages,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,033 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2de50ee4{/stages/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,035 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@47fbc56{/stages/stage,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,042 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@149c3204{/stages/stage/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,044 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@497aec8c{/stages/pool,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,047 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e6f2bb5{/stages/pool/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,053 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f628ce9{/storage,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,054 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26d96e5{/storage/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,056 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1846579f{/storage/rdd,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,060 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2650f79{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fac521d{/environment,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,067 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@129bd55d{/environment/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,069 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3abfe845{/executors,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3672276e{/executors/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f08caf{/executors/threadDump,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,079 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2330e3e0{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,093 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27a2a089{/static,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,095 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ac5eb4{/,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,098 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@715d6168{/api,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,100 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a685eba{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@107f4980{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:30:59,104 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://23ba3df63994:4040\n",
            "2024-10-21 11:30:59,141 INFO spark.SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.2.3.jar at spark://23ba3df63994:42907/jars/spark-examples_2.12-3.2.3.jar with timestamp 1729510254453\n",
            "2024-10-21 11:30:59,635 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://23ba3df63994:7077...\n",
            "2024-10-21 11:30:59,738 INFO client.TransportClientFactory: Successfully created connection to 23ba3df63994/172.28.0.12:7077 after 63 ms (0 ms spent in bootstraps)\n",
            "2024-10-21 11:31:00,109 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241021113100-0000\n",
            "2024-10-21 11:31:00,131 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32963.\n",
            "2024-10-21 11:31:00,131 INFO netty.NettyBlockTransferService: Server created on 23ba3df63994:32963\n",
            "2024-10-21 11:31:00,134 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "2024-10-21 11:31:00,179 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 23ba3df63994, 32963, None)\n",
            "2024-10-21 11:31:00,192 INFO storage.BlockManagerMasterEndpoint: Registering block manager 23ba3df63994:32963 with 434.4 MiB RAM, BlockManagerId(driver, 23ba3df63994, 32963, None)\n",
            "2024-10-21 11:31:00,197 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 23ba3df63994, 32963, None)\n",
            "2024-10-21 11:31:00,201 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 23ba3df63994, 32963, None)\n",
            "2024-10-21 11:31:00,244 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20241021113100-0000/0 on worker-20241021113051-172.28.0.12-7078 (172.28.0.12:7078) with 2 core(s)\n",
            "2024-10-21 11:31:00,261 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20241021113100-0000/0 on hostPort 172.28.0.12:7078 with 2 core(s), 1024.0 MiB RAM\n",
            "2024-10-21 11:31:00,893 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20241021113100-0000/0 is now RUNNING\n",
            "2024-10-21 11:31:00,946 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2aa7399c{/metrics/json,null,AVAILABLE,@Spark}\n",
            "2024-10-21 11:31:01,123 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
            "2024-10-21 11:31:02,691 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:38\n",
            "2024-10-21 11:31:02,740 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 100 output partitions\n",
            "2024-10-21 11:31:02,741 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
            "2024-10-21 11:31:02,747 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
            "2024-10-21 11:31:02,752 INFO scheduler.DAGScheduler: Missing parents: List()\n",
            "2024-10-21 11:31:02,777 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
            "2024-10-21 11:31:03,196 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)\n",
            "2024-10-21 11:31:03,353 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)\n",
            "2024-10-21 11:31:03,362 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 23ba3df63994:32963 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "2024-10-21 11:31:03,373 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478\n",
            "2024-10-21 11:31:03,465 INFO scheduler.DAGScheduler: Submitting 100 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
            "2024-10-21 11:31:03,480 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 100 tasks resource profile 0\n",
            "2024-10-21 11:31:06,931 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:42900) with ID 0,  ResourceProfileId 0\n",
            "2024-10-21 11:31:07,296 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.28.0.12:38459 with 434.4 MiB RAM, BlockManagerId(0, 172.28.0.12, 38459, None)\n",
            "2024-10-21 11:31:07,551 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.28.0.12, executor 0, partition 0, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:07,570 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.28.0.12, executor 0, partition 1, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:08,377 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.28.0.12:38459 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "2024-10-21 11:31:09,278 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.28.0.12, executor 0, partition 2, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,287 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.28.0.12, executor 0, partition 3, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,348 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1812 ms on 172.28.0.12 (executor 0) (1/100)\n",
            "2024-10-21 11:31:09,354 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1785 ms on 172.28.0.12 (executor 0) (2/100)\n",
            "2024-10-21 11:31:09,542 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.28.0.12, executor 0, partition 4, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,561 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.28.0.12, executor 0, partition 5, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,568 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 281 ms on 172.28.0.12 (executor 0) (3/100)\n",
            "2024-10-21 11:31:09,582 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 305 ms on 172.28.0.12 (executor 0) (4/100)\n",
            "2024-10-21 11:31:09,746 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.28.0.12, executor 0, partition 6, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,749 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 189 ms on 172.28.0.12 (executor 0) (5/100)\n",
            "2024-10-21 11:31:09,777 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 236 ms on 172.28.0.12 (executor 0) (6/100)\n",
            "2024-10-21 11:31:09,780 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.28.0.12, executor 0, partition 7, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,966 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.28.0.12, executor 0, partition 8, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:09,967 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 222 ms on 172.28.0.12 (executor 0) (7/100)\n",
            "2024-10-21 11:31:09,995 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 216 ms on 172.28.0.12 (executor 0) (8/100)\n",
            "2024-10-21 11:31:10,004 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.28.0.12, executor 0, partition 9, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,112 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.28.0.12, executor 0, partition 10, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,114 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 149 ms on 172.28.0.12 (executor 0) (9/100)\n",
            "2024-10-21 11:31:10,148 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.28.0.12, executor 0, partition 11, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,151 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 148 ms on 172.28.0.12 (executor 0) (10/100)\n",
            "2024-10-21 11:31:10,224 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.28.0.12, executor 0, partition 12, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,225 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 113 ms on 172.28.0.12 (executor 0) (11/100)\n",
            "2024-10-21 11:31:10,264 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.28.0.12, executor 0, partition 13, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,271 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 124 ms on 172.28.0.12 (executor 0) (12/100)\n",
            "2024-10-21 11:31:10,349 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.28.0.12, executor 0, partition 14, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,352 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 129 ms on 172.28.0.12 (executor 0) (13/100)\n",
            "2024-10-21 11:31:10,375 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.28.0.12, executor 0, partition 15, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,377 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 114 ms on 172.28.0.12 (executor 0) (14/100)\n",
            "2024-10-21 11:31:10,450 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.28.0.12, executor 0, partition 16, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,454 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 106 ms on 172.28.0.12 (executor 0) (15/100)\n",
            "2024-10-21 11:31:10,487 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.28.0.12, executor 0, partition 17, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,488 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 119 ms on 172.28.0.12 (executor 0) (16/100)\n",
            "2024-10-21 11:31:10,541 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.28.0.12, executor 0, partition 18, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,542 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 55 ms on 172.28.0.12 (executor 0) (17/100)\n",
            "2024-10-21 11:31:10,578 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.28.0.12, executor 0, partition 19, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,579 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 130 ms on 172.28.0.12 (executor 0) (18/100)\n",
            "2024-10-21 11:31:10,632 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.28.0.12, executor 0, partition 20, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,634 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 94 ms on 172.28.0.12 (executor 0) (19/100)\n",
            "2024-10-21 11:31:10,689 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.28.0.12, executor 0, partition 21, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,691 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 114 ms on 172.28.0.12 (executor 0) (20/100)\n",
            "2024-10-21 11:31:10,798 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.28.0.12, executor 0, partition 22, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,799 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 168 ms on 172.28.0.12 (executor 0) (21/100)\n",
            "2024-10-21 11:31:10,829 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 140 ms on 172.28.0.12 (executor 0) (22/100)\n",
            "2024-10-21 11:31:10,831 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.28.0.12, executor 0, partition 23, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,890 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.28.0.12, executor 0, partition 24, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,891 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 94 ms on 172.28.0.12 (executor 0) (23/100)\n",
            "2024-10-21 11:31:10,932 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.28.0.12, executor 0, partition 25, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,934 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 103 ms on 172.28.0.12 (executor 0) (24/100)\n",
            "2024-10-21 11:31:10,968 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.28.0.12, executor 0, partition 26, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:10,970 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 80 ms on 172.28.0.12 (executor 0) (25/100)\n",
            "2024-10-21 11:31:11,025 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.28.0.12, executor 0, partition 27, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,028 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 96 ms on 172.28.0.12 (executor 0) (26/100)\n",
            "2024-10-21 11:31:11,104 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.28.0.12, executor 0, partition 28, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,108 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 140 ms on 172.28.0.12 (executor 0) (27/100)\n",
            "2024-10-21 11:31:11,113 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 89 ms on 172.28.0.12 (executor 0) (28/100)\n",
            "2024-10-21 11:31:11,116 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.28.0.12, executor 0, partition 29, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,167 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.28.0.12, executor 0, partition 30, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,172 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 70 ms on 172.28.0.12 (executor 0) (29/100)\n",
            "2024-10-21 11:31:11,218 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.28.0.12, executor 0, partition 31, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,225 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 109 ms on 172.28.0.12 (executor 0) (30/100)\n",
            "2024-10-21 11:31:11,232 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.28.0.12, executor 0, partition 32, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,248 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 82 ms on 172.28.0.12 (executor 0) (31/100)\n",
            "2024-10-21 11:31:11,306 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.28.0.12, executor 0, partition 33, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,307 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 75 ms on 172.28.0.12 (executor 0) (32/100)\n",
            "2024-10-21 11:31:11,321 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 103 ms on 172.28.0.12 (executor 0) (33/100)\n",
            "2024-10-21 11:31:11,326 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.28.0.12, executor 0, partition 34, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,388 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.28.0.12, executor 0, partition 35, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,391 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 86 ms on 172.28.0.12 (executor 0) (34/100)\n",
            "2024-10-21 11:31:11,420 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.28.0.12, executor 0, partition 36, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,422 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 96 ms on 172.28.0.12 (executor 0) (35/100)\n",
            "2024-10-21 11:31:11,452 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.28.0.12, executor 0, partition 37, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,454 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 67 ms on 172.28.0.12 (executor 0) (36/100)\n",
            "2024-10-21 11:31:11,491 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.28.0.12, executor 0, partition 38, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,491 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 72 ms on 172.28.0.12 (executor 0) (37/100)\n",
            "2024-10-21 11:31:11,514 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.28.0.12, executor 0, partition 39, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,515 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 64 ms on 172.28.0.12 (executor 0) (38/100)\n",
            "2024-10-21 11:31:11,538 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.28.0.12, executor 0, partition 40, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,541 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 51 ms on 172.28.0.12 (executor 0) (39/100)\n",
            "2024-10-21 11:31:11,571 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.28.0.12, executor 0, partition 41, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,576 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 63 ms on 172.28.0.12 (executor 0) (40/100)\n",
            "2024-10-21 11:31:11,578 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 40 ms on 172.28.0.12 (executor 0) (41/100)\n",
            "2024-10-21 11:31:11,580 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.28.0.12, executor 0, partition 42, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,637 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.28.0.12, executor 0, partition 43, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,637 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 57 ms on 172.28.0.12 (executor 0) (42/100)\n",
            "2024-10-21 11:31:11,643 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.28.0.12, executor 0, partition 44, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,649 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 78 ms on 172.28.0.12 (executor 0) (43/100)\n",
            "2024-10-21 11:31:11,697 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.28.0.12, executor 0, partition 45, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,698 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 56 ms on 172.28.0.12 (executor 0) (44/100)\n",
            "2024-10-21 11:31:11,713 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.28.0.12, executor 0, partition 46, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,716 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 79 ms on 172.28.0.12 (executor 0) (45/100)\n",
            "2024-10-21 11:31:11,758 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.28.0.12, executor 0, partition 47, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,759 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 46 ms on 172.28.0.12 (executor 0) (46/100)\n",
            "2024-10-21 11:31:11,788 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.28.0.12, executor 0, partition 48, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,788 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 31 ms on 172.28.0.12 (executor 0) (47/100)\n",
            "2024-10-21 11:31:11,806 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 110 ms on 172.28.0.12 (executor 0) (48/100)\n",
            "2024-10-21 11:31:11,808 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.28.0.12, executor 0, partition 49, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,847 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.28.0.12, executor 0, partition 50, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,847 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 60 ms on 172.28.0.12 (executor 0) (49/100)\n",
            "2024-10-21 11:31:11,863 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.28.0.12, executor 0, partition 51, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,864 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 57 ms on 172.28.0.12 (executor 0) (50/100)\n",
            "2024-10-21 11:31:11,904 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.28.0.12, executor 0, partition 52, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,906 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 60 ms on 172.28.0.12 (executor 0) (51/100)\n",
            "2024-10-21 11:31:11,923 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.28.0.12, executor 0, partition 53, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,924 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 62 ms on 172.28.0.12 (executor 0) (52/100)\n",
            "2024-10-21 11:31:11,976 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.28.0.12, executor 0, partition 54, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:11,976 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 73 ms on 172.28.0.12 (executor 0) (53/100)\n",
            "2024-10-21 11:31:12,015 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.28.0.12, executor 0, partition 55, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,017 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 94 ms on 172.28.0.12 (executor 0) (54/100)\n",
            "2024-10-21 11:31:12,042 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.28.0.12, executor 0, partition 56, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,044 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 69 ms on 172.28.0.12 (executor 0) (55/100)\n",
            "2024-10-21 11:31:12,098 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.28.0.12, executor 0, partition 57, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,100 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 85 ms on 172.28.0.12 (executor 0) (56/100)\n",
            "2024-10-21 11:31:12,111 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.28.0.12, executor 0, partition 58, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,113 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 70 ms on 172.28.0.12 (executor 0) (57/100)\n",
            "2024-10-21 11:31:12,178 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.28.0.12, executor 0, partition 59, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,180 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 70 ms on 172.28.0.12 (executor 0) (58/100)\n",
            "2024-10-21 11:31:12,184 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.28.0.12, executor 0, partition 60, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,187 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 89 ms on 172.28.0.12 (executor 0) (59/100)\n",
            "2024-10-21 11:31:12,276 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.28.0.12, executor 0, partition 61, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,277 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 101 ms on 172.28.0.12 (executor 0) (60/100)\n",
            "2024-10-21 11:31:12,285 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.28.0.12, executor 0, partition 62, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,287 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 104 ms on 172.28.0.12 (executor 0) (61/100)\n",
            "2024-10-21 11:31:12,325 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.28.0.12, executor 0, partition 63, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,325 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 50 ms on 172.28.0.12 (executor 0) (62/100)\n",
            "2024-10-21 11:31:12,341 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.28.0.12, executor 0, partition 64, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,342 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 57 ms on 172.28.0.12 (executor 0) (63/100)\n",
            "2024-10-21 11:31:12,365 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.28.0.12, executor 0, partition 65, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,366 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 42 ms on 172.28.0.12 (executor 0) (64/100)\n",
            "2024-10-21 11:31:12,404 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.28.0.12, executor 0, partition 66, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,404 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 39 ms on 172.28.0.12 (executor 0) (65/100)\n",
            "2024-10-21 11:31:12,410 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.28.0.12, executor 0, partition 67, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,411 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 70 ms on 172.28.0.12 (executor 0) (66/100)\n",
            "2024-10-21 11:31:12,449 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.28.0.12, executor 0, partition 68, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,450 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 40 ms on 172.28.0.12 (executor 0) (67/100)\n",
            "2024-10-21 11:31:12,468 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.28.0.12, executor 0, partition 69, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,470 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 66 ms on 172.28.0.12 (executor 0) (68/100)\n",
            "2024-10-21 11:31:12,488 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.28.0.12, executor 0, partition 70, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,488 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 39 ms on 172.28.0.12 (executor 0) (69/100)\n",
            "2024-10-21 11:31:12,522 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.28.0.12, executor 0, partition 71, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,523 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 55 ms on 172.28.0.12 (executor 0) (70/100)\n",
            "2024-10-21 11:31:12,544 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.28.0.12, executor 0, partition 72, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,545 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 58 ms on 172.28.0.12 (executor 0) (71/100)\n",
            "2024-10-21 11:31:12,568 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.28.0.12, executor 0, partition 73, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,569 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 47 ms on 172.28.0.12 (executor 0) (72/100)\n",
            "2024-10-21 11:31:12,585 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.28.0.12, executor 0, partition 74, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,586 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 42 ms on 172.28.0.12 (executor 0) (73/100)\n",
            "2024-10-21 11:31:12,621 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.28.0.12, executor 0, partition 75, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,621 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 36 ms on 172.28.0.12 (executor 0) (74/100)\n",
            "2024-10-21 11:31:12,627 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (172.28.0.12, executor 0, partition 76, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,628 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 61 ms on 172.28.0.12 (executor 0) (75/100)\n",
            "2024-10-21 11:31:12,666 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (172.28.0.12, executor 0, partition 77, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,667 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 41 ms on 172.28.0.12 (executor 0) (76/100)\n",
            "2024-10-21 11:31:12,691 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (172.28.0.12, executor 0, partition 78, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,692 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 72 ms on 172.28.0.12 (executor 0) (77/100)\n",
            "2024-10-21 11:31:12,721 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (172.28.0.12, executor 0, partition 79, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,723 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 57 ms on 172.28.0.12 (executor 0) (78/100)\n",
            "2024-10-21 11:31:12,734 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (172.28.0.12, executor 0, partition 80, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,739 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 49 ms on 172.28.0.12 (executor 0) (79/100)\n",
            "2024-10-21 11:31:12,760 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (172.28.0.12, executor 0, partition 81, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,761 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 40 ms on 172.28.0.12 (executor 0) (80/100)\n",
            "2024-10-21 11:31:12,797 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (172.28.0.12, executor 0, partition 82, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,798 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 64 ms on 172.28.0.12 (executor 0) (81/100)\n",
            "2024-10-21 11:31:12,820 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (172.28.0.12, executor 0, partition 83, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,821 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 62 ms on 172.28.0.12 (executor 0) (82/100)\n",
            "2024-10-21 11:31:12,853 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (172.28.0.12, executor 0, partition 84, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,854 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 57 ms on 172.28.0.12 (executor 0) (83/100)\n",
            "2024-10-21 11:31:12,858 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (172.28.0.12, executor 0, partition 85, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,859 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 40 ms on 172.28.0.12 (executor 0) (84/100)\n",
            "2024-10-21 11:31:12,896 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (172.28.0.12, executor 0, partition 86, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,899 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 41 ms on 172.28.0.12 (executor 0) (85/100)\n",
            "2024-10-21 11:31:12,909 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (172.28.0.12, executor 0, partition 87, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,910 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 57 ms on 172.28.0.12 (executor 0) (86/100)\n",
            "2024-10-21 11:31:12,939 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (172.28.0.12, executor 0, partition 88, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,940 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 43 ms on 172.28.0.12 (executor 0) (87/100)\n",
            "2024-10-21 11:31:12,944 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (172.28.0.12, executor 0, partition 89, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,945 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 36 ms on 172.28.0.12 (executor 0) (88/100)\n",
            "2024-10-21 11:31:12,995 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (172.28.0.12, executor 0, partition 90, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,995 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 51 ms on 172.28.0.12 (executor 0) (89/100)\n",
            "2024-10-21 11:31:12,996 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (172.28.0.12, executor 0, partition 91, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:12,997 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 58 ms on 172.28.0.12 (executor 0) (90/100)\n",
            "2024-10-21 11:31:13,034 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (172.28.0.12, executor 0, partition 92, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,035 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 41 ms on 172.28.0.12 (executor 0) (91/100)\n",
            "2024-10-21 11:31:13,053 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (172.28.0.12, executor 0, partition 93, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,059 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 63 ms on 172.28.0.12 (executor 0) (92/100)\n",
            "2024-10-21 11:31:13,088 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (172.28.0.12, executor 0, partition 94, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,092 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 57 ms on 172.28.0.12 (executor 0) (93/100)\n",
            "2024-10-21 11:31:13,123 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (172.28.0.12, executor 0, partition 95, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,124 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 36 ms on 172.28.0.12 (executor 0) (94/100)\n",
            "2024-10-21 11:31:13,137 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (172.28.0.12, executor 0, partition 96, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,137 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 85 ms on 172.28.0.12 (executor 0) (95/100)\n",
            "2024-10-21 11:31:13,172 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (172.28.0.12, executor 0, partition 97, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,173 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 50 ms on 172.28.0.12 (executor 0) (96/100)\n",
            "2024-10-21 11:31:13,183 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (172.28.0.12, executor 0, partition 98, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,184 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 48 ms on 172.28.0.12 (executor 0) (97/100)\n",
            "2024-10-21 11:31:13,228 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (172.28.0.12, executor 0, partition 99, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "2024-10-21 11:31:13,228 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 45 ms on 172.28.0.12 (executor 0) (98/100)\n",
            "2024-10-21 11:31:13,256 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 84 ms on 172.28.0.12 (executor 0) (99/100)\n",
            "2024-10-21 11:31:13,266 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 39 ms on 172.28.0.12 (executor 0) (100/100)\n",
            "2024-10-21 11:31:13,270 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 10.398 s\n",
            "2024-10-21 11:31:13,268 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "2024-10-21 11:31:13,276 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "2024-10-21 11:31:13,277 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "2024-10-21 11:31:13,286 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 10.594144 s\n",
            "2024-10-21 11:31:13,320 INFO server.AbstractConnector: Stopped Spark@138068d5{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
            "2024-10-21 11:31:13,327 INFO ui.SparkUI: Stopped Spark web UI at http://23ba3df63994:4040\n",
            "2024-10-21 11:31:13,339 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
            "2024-10-21 11:31:13,340 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
            "2024-10-21 11:31:13,395 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "2024-10-21 11:31:13,478 INFO memory.MemoryStore: MemoryStore cleared\n",
            "2024-10-21 11:31:13,478 INFO storage.BlockManager: BlockManager stopped\n",
            "2024-10-21 11:31:13,491 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
            "2024-10-21 11:31:13,505 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "2024-10-21 11:31:13,564 INFO spark.SparkContext: Successfully stopped SparkContext\n",
            "2024-10-21 11:31:13,599 INFO util.ShutdownHookManager: Shutdown hook called\n",
            "2024-10-21 11:31:13,600 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b6d99019-945b-4baa-a67d-d4ee6caf941e\n",
            "2024-10-21 11:31:13,631 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6d922ba7-7589-4eaf-b258-e84e278371a8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Java Random Forest Regressor example\n",
        "\n",
        "Next, we will run the Java Random Forest Regressor example. Source: [JavaRandomForestRegressorExample.java](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java)."
      ],
      "metadata": {
        "id": "mLsH_Xpdyl97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        " j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        " echo \"Jar file containing examples: $j\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8JwPHSSJCJ8",
        "outputId": "37bd1b22-c270-4673-c497-09770b7083ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jar file containing examples: /usr/lib/spark/examples/jars/spark-examples.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run\n",
        "\n",
        "```\n",
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j\n",
        "```\n",
        "\n",
        "you'll get an error message telling you that the file `/content/data/mllib/sample_libsvm_data.txt` is missing. We are just going to create this file, but first we need to find it!"
      ],
      "metadata": {
        "id": "d1idj6w9y493"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name 'sample_libsvm_data.txt' 2> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS1eO8PtLHwl",
        "outputId": "7adbaaf0-b25f-4e02-da24-9b709cc29519"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/spark/data/mllib/sample_libsvm_data.txt\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/data/mllib/sample_libsvm_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the datafile to the desired location"
      ],
      "metadata": {
        "id": "ADBuYvjezWZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/data/mllib/\n",
        "cp /usr/lib/spark/data/mllib/sample_libsvm_data.txt /content/data/mllib/sample_libsvm_data.txt"
      ],
      "metadata": {
        "id": "E5vwn1iNMHKQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the JavaRandomForestRegressorExample example."
      ],
      "metadata": {
        "id": "snh_A1DFzj7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuvY9Pt_MUcw",
        "outputId": "123814cb-918f-475e-fba7-71a151b8eb31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[126,127,128...|\n",
            "|       0.0|  0.0|(692,[126,127,128...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0.17159383568311662\n",
            "Learned regression forest model:\n",
            "RandomForestRegressionModel: uid=rfr_460e2087c372, numTrees=20, numFeatures=692\n",
            "  Tree 0 (weight 1.0):\n",
            "    If (feature 489 <= 5.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 489 > 5.5)\n",
            "     Predict: 1.0\n",
            "  Tree 1 (weight 1.0):\n",
            "    If (feature 517 <= 66.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 517 > 66.5)\n",
            "     Predict: 1.0\n",
            "  Tree 2 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 3 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 4 (weight 1.0):\n",
            "    If (feature 433 <= 66.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 66.5)\n",
            "     Predict: 1.0\n",
            "  Tree 5 (weight 1.0):\n",
            "    If (feature 461 <= 56.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 461 > 56.5)\n",
            "     Predict: 1.0\n",
            "  Tree 6 (weight 1.0):\n",
            "    If (feature 461 <= 56.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 461 > 56.5)\n",
            "     Predict: 1.0\n",
            "  Tree 7 (weight 1.0):\n",
            "    If (feature 483 <= 28.0)\n",
            "     If (feature 382 <= 66.5)\n",
            "      Predict: 1.0\n",
            "     Else (feature 382 > 66.5)\n",
            "      Predict: 0.0\n",
            "    Else (feature 483 > 28.0)\n",
            "     Predict: 0.0\n",
            "  Tree 8 (weight 1.0):\n",
            "    If (feature 461 <= 56.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 461 > 56.5)\n",
            "     Predict: 1.0\n",
            "  Tree 9 (weight 1.0):\n",
            "    If (feature 490 <= 51.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 51.0)\n",
            "     Predict: 1.0\n",
            "  Tree 10 (weight 1.0):\n",
            "    If (feature 434 <= 70.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 434 > 70.5)\n",
            "     Predict: 1.0\n",
            "  Tree 11 (weight 1.0):\n",
            "    If (feature 518 <= 28.5)\n",
            "     If (feature 459 <= 141.0)\n",
            "      Predict: 0.0\n",
            "     Else (feature 459 > 141.0)\n",
            "      Predict: 1.0\n",
            "    Else (feature 518 > 28.5)\n",
            "     If (feature 457 <= 13.5)\n",
            "      Predict: 1.0\n",
            "     Else (feature 457 > 13.5)\n",
            "      Predict: 0.0\n",
            "  Tree 12 (weight 1.0):\n",
            "    If (feature 490 <= 15.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 15.5)\n",
            "     Predict: 1.0\n",
            "  Tree 13 (weight 1.0):\n",
            "    If (feature 490 <= 15.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 15.5)\n",
            "     Predict: 1.0\n",
            "  Tree 14 (weight 1.0):\n",
            "    If (feature 489 <= 47.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 489 > 47.0)\n",
            "     Predict: 1.0\n",
            "  Tree 15 (weight 1.0):\n",
            "    If (feature 433 <= 66.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 66.5)\n",
            "     Predict: 1.0\n",
            "  Tree 16 (weight 1.0):\n",
            "    If (feature 540 <= 10.0)\n",
            "     Predict: 1.0\n",
            "    Else (feature 540 > 10.0)\n",
            "     Predict: 0.0\n",
            "  Tree 17 (weight 1.0):\n",
            "    If (feature 406 <= 36.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 36.0)\n",
            "     Predict: 1.0\n",
            "  Tree 18 (weight 1.0):\n",
            "    If (feature 406 <= 36.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 36.0)\n",
            "     Predict: 1.0\n",
            "  Tree 19 (weight 1.0):\n",
            "    If (feature 462 <= 70.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 462 > 70.0)\n",
            "     Predict: 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "24/10/21 11:31:32 INFO SparkContext: Running Spark version 3.5.3\n",
            "24/10/21 11:31:33 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "24/10/21 11:31:33 INFO SparkContext: Java version 11.0.24\n",
            "24/10/21 11:31:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/10/21 11:31:33 INFO ResourceUtils: ==============================================================\n",
            "24/10/21 11:31:33 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/10/21 11:31:33 INFO ResourceUtils: ==============================================================\n",
            "24/10/21 11:31:33 INFO SparkContext: Submitted application: JavaRandomForestRegressorExample\n",
            "24/10/21 11:31:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/10/21 11:31:33 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/10/21 11:31:33 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/10/21 11:31:33 INFO SecurityManager: Changing view acls to: root\n",
            "24/10/21 11:31:33 INFO SecurityManager: Changing modify acls to: root\n",
            "24/10/21 11:31:33 INFO SecurityManager: Changing view acls groups to: \n",
            "24/10/21 11:31:33 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/10/21 11:31:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/10/21 11:31:34 INFO Utils: Successfully started service 'sparkDriver' on port 37879.\n",
            "24/10/21 11:31:34 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/10/21 11:31:34 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/10/21 11:31:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/10/21 11:31:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/10/21 11:31:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/10/21 11:31:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd96a3ac-d5be-40d9-ac1d-0ec6647a02a9\n",
            "24/10/21 11:31:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/10/21 11:31:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/10/21 11:31:35 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/10/21 11:31:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/10/21 11:31:35 INFO SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.2.3.jar at spark://23ba3df63994:37879/jars/spark-examples_2.12-3.2.3.jar with timestamp 1729510292982\n",
            "24/10/21 11:31:35 INFO Executor: Starting executor ID driver on host 23ba3df63994\n",
            "24/10/21 11:31:35 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "24/10/21 11:31:35 INFO Executor: Java version 11.0.24\n",
            "24/10/21 11:31:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/10/21 11:31:35 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@71f1cc02 for default.\n",
            "24/10/21 11:31:35 INFO Executor: Fetching spark://23ba3df63994:37879/jars/spark-examples_2.12-3.2.3.jar with timestamp 1729510292982\n",
            "24/10/21 11:31:36 INFO TransportClientFactory: Successfully created connection to 23ba3df63994/172.28.0.12:37879 after 137 ms (0 ms spent in bootstraps)\n",
            "24/10/21 11:31:36 INFO Utils: Fetching spark://23ba3df63994:37879/jars/spark-examples_2.12-3.2.3.jar to /tmp/spark-b27ceaa8-1960-4342-acf2-e10027ce6924/userFiles-2d8aad61-0289-4574-a55f-b53c4c9d8809/fetchFileTemp10934010935256626650.tmp\n",
            "24/10/21 11:31:36 INFO Executor: Adding file:/tmp/spark-b27ceaa8-1960-4342-acf2-e10027ce6924/userFiles-2d8aad61-0289-4574-a55f-b53c4c9d8809/spark-examples_2.12-3.2.3.jar to class loader default\n",
            "24/10/21 11:31:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38639.\n",
            "24/10/21 11:31:36 INFO NettyBlockTransferService: Server created on 23ba3df63994:38639\n",
            "24/10/21 11:31:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/10/21 11:31:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 23ba3df63994, 38639, None)\n",
            "24/10/21 11:31:36 INFO BlockManagerMasterEndpoint: Registering block manager 23ba3df63994:38639 with 434.4 MiB RAM, BlockManagerId(driver, 23ba3df63994, 38639, None)\n",
            "24/10/21 11:31:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 23ba3df63994, 38639, None)\n",
            "24/10/21 11:31:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 23ba3df63994, 38639, None)\n",
            "24/10/21 11:31:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "24/10/21 11:31:37 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "24/10/21 11:31:39 INFO InMemoryFileIndex: It took 113 ms to list leaf files for 1 paths.\n",
            "24/10/21 11:31:39 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
            "24/10/21 11:31:39 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "24/10/21 11:31:43 INFO FileSourceStrategy: Pushed Filters: \n",
            "24/10/21 11:31:43 INFO FileSourceStrategy: Post-Scan Filters: NOT (length(trim(value#0, None)) = 0),NOT StartsWith(trim(value#0, None), #)\n",
            "24/10/21 11:31:44 INFO CodeGenerator: Code generated in 486.806702 ms\n",
            "24/10/21 11:31:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.4 KiB, free 434.2 MiB)\n",
            "24/10/21 11:31:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)\n",
            "24/10/21 11:31:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 23ba3df63994:38639 (size: 34.4 KiB, free: 434.4 MiB)\n",
            "24/10/21 11:31:44 INFO SparkContext: Created broadcast 0 from rdd at MLUtils.scala:124\n",
            "24/10/21 11:31:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "24/10/21 11:31:45 INFO SparkContext: Starting job: reduce at MLUtils.scala:94\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Got job 0 (reduce at MLUtils.scala:94) with 1 output partitions\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at MLUtils.scala:94)\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:92), which has no missing parents\n",
            "24/10/21 11:31:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.4 KiB, free 434.1 MiB)\n",
            "24/10/21 11:31:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 434.1 MiB)\n",
            "24/10/21 11:31:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 23ba3df63994:38639 (size: 13.6 KiB, free: 434.4 MiB)\n",
            "24/10/21 11:31:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:92) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/10/21 11:31:46 INFO CodeGenerator: Code generated in 107.748976 ms\n",
            "24/10/21 11:31:46 INFO CodeGenerator: Code generated in 30.089123 ms\n",
            "24/10/21 11:31:46 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:46 INFO CodeGenerator: Code generated in 46.578326 ms\n",
            "24/10/21 11:31:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1768 bytes result sent to driver\n",
            "24/10/21 11:31:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1165 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:47 INFO DAGScheduler: ResultStage 0 (reduce at MLUtils.scala:94) finished in 1.512 s\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Job 0 finished: reduce at MLUtils.scala:94, took 1.661541 s\n",
            "24/10/21 11:31:47 INFO FileSourceStrategy: Pushed Filters: \n",
            "24/10/21 11:31:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "24/10/21 11:31:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.3 KiB, free 433.9 MiB)\n",
            "24/10/21 11:31:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
            "24/10/21 11:31:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 23ba3df63994:38639 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:47 INFO SparkContext: Created broadcast 2 from broadcast at LibSVMRelation.scala:156\n",
            "24/10/21 11:31:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "24/10/21 11:31:47 INFO SparkContext: Starting job: treeReduce at VectorIndexer.scala:150\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Got job 1 (treeReduce at VectorIndexer.scala:150) with 1 output partitions\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Final stage: ResultStage 1 (treeReduce at VectorIndexer.scala:150)\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:150), which has no missing parents\n",
            "24/10/21 11:31:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.4 KiB, free 433.9 MiB)\n",
            "24/10/21 11:31:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 433.9 MiB)\n",
            "24/10/21 11:31:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 23ba3df63994:38639 (size: 11.8 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:150) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "24/10/21 11:31:48 INFO CodeGenerator: Code generated in 67.632259 ms\n",
            "24/10/21 11:31:48 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:48 INFO CodeGenerator: Code generated in 104.183791 ms\n",
            "24/10/21 11:31:48 INFO CodeGenerator: Code generated in 70.821275 ms\n",
            "24/10/21 11:31:48 INFO CodeGenerator: Code generated in 165.664042 ms\n",
            "24/10/21 11:31:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 427169 bytes result sent to driver\n",
            "24/10/21 11:31:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1043 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:48 INFO DAGScheduler: ResultStage 1 (treeReduce at VectorIndexer.scala:150) finished in 1.195 s\n",
            "24/10/21 11:31:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/10/21 11:31:48 INFO DAGScheduler: Job 1 finished: treeReduce at VectorIndexer.scala:150, took 1.221141 s\n",
            "24/10/21 11:31:50 INFO FileSourceStrategy: Pushed Filters: \n",
            "24/10/21 11:31:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "24/10/21 11:31:50 INFO CodeGenerator: Code generated in 128.153893 ms\n",
            "24/10/21 11:31:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.3 KiB, free 433.7 MiB)\n",
            "24/10/21 11:31:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.6 MiB)\n",
            "24/10/21 11:31:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 23ba3df63994:38639 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:50 INFO SparkContext: Created broadcast 4 from broadcast at LibSVMRelation.scala:156\n",
            "24/10/21 11:31:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "24/10/21 11:31:50 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 23ba3df63994:38639 in memory (size: 11.8 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 23ba3df63994:38639 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:50 INFO Instrumentation: [ac51ab98] Stage class: RandomForestRegressor\n",
            "24/10/21 11:31:50 INFO Instrumentation: [ac51ab98] Stage uid: rfr_460e2087c372\n",
            "24/10/21 11:31:50 INFO Instrumentation: [ac51ab98] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)\n",
            "24/10/21 11:31:50 INFO Instrumentation: [ac51ab98] {\"labelCol\":\"label\",\"featuresCol\":\"indexedFeatures\"}\n",
            "24/10/21 11:31:50 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Got job 2 (take at DecisionTreeMetadata.scala:119) with 1 output partitions\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Final stage: ResultStage 2 (take at DecisionTreeMetadata.scala:119)\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[25] at map at DecisionTreeMetadata.scala:119), which has no missing parents\n",
            "24/10/21 11:31:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 136.9 KiB, free 433.8 MiB)\n",
            "24/10/21 11:31:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 43.0 KiB, free 433.7 MiB)\n",
            "24/10/21 11:31:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 23ba3df63994:38639 (size: 43.0 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[25] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 132.585594 ms\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 56.488168 ms\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 19.677841 ms\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 33.655486 ms\n",
            "24/10/21 11:31:51 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 26.530459 ms\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 19.095658 ms\n",
            "24/10/21 11:31:51 INFO CodeGenerator: Code generated in 34.708285 ms\n",
            "24/10/21 11:31:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1802 bytes result sent to driver\n",
            "24/10/21 11:31:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 753 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:51 INFO DAGScheduler: ResultStage 2 (take at DecisionTreeMetadata.scala:119) finished in 0.786 s\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Job 2 finished: take at DecisionTreeMetadata.scala:119, took 0.800539 s\n",
            "24/10/21 11:31:51 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Got job 3 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Final stage: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125)\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[24] at retag at RandomForest.scala:274), which has no missing parents\n",
            "24/10/21 11:31:51 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 137.0 KiB, free 433.6 MiB)\n",
            "24/10/21 11:31:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 43.1 KiB, free 433.5 MiB)\n",
            "24/10/21 11:31:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 23ba3df63994:38639 (size: 43.1 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:51 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[24] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "24/10/21 11:31:51 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:51 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 23ba3df63994:38639 in memory (size: 43.0 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1960 bytes result sent to driver\n",
            "24/10/21 11:31:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 200 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:51 INFO DAGScheduler: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125) finished in 0.235 s\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "24/10/21 11:31:51 INFO DAGScheduler: Job 3 finished: aggregate at DecisionTreeMetadata.scala:125, took 0.245184 s\n",
            "24/10/21 11:31:52 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1054\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Registering RDD 27 (flatMap at RandomForest.scala:1039) as input to shuffle 0\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Got job 4 (collectAsMap at RandomForest.scala:1054) with 1 output partitions\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Final stage: ResultStage 5 (collectAsMap at RandomForest.scala:1054)\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[27] at flatMap at RandomForest.scala:1039), which has no missing parents\n",
            "24/10/21 11:31:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 144.8 KiB, free 433.6 MiB)\n",
            "24/10/21 11:31:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 46.1 KiB, free 433.5 MiB)\n",
            "24/10/21 11:31:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 23ba3df63994:38639 (size: 46.1 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[27] at flatMap at RandomForest.scala:1039) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:52 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:52 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9788 bytes) \n",
            "24/10/21 11:31:52 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "24/10/21 11:31:52 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2139 bytes result sent to driver\n",
            "24/10/21 11:31:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 762 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:52 INFO DAGScheduler: ShuffleMapStage 4 (flatMap at RandomForest.scala:1039) finished in 0.796 s\n",
            "24/10/21 11:31:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/10/21 11:31:52 INFO DAGScheduler: running: Set()\n",
            "24/10/21 11:31:52 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
            "24/10/21 11:31:52 INFO DAGScheduler: failed: Set()\n",
            "24/10/21 11:31:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at map at RandomForest.scala:1054), which has no missing parents\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.1 KiB, free 433.5 MiB)\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.5 MiB)\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 23ba3df63994:38639 (size: 6.3 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:53 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at map at RandomForest.scala:1054) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:53 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:53 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (23ba3df63994, executor driver, partition 0, NODE_LOCAL, 9005 bytes) \n",
            "24/10/21 11:31:53 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "24/10/21 11:31:53 INFO ShuffleBlockFetcherIterator: Getting 1 (82.3 KiB) non-empty blocks including 1 (82.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/10/21 11:31:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
            "24/10/21 11:31:53 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 108526 bytes result sent to driver\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 23ba3df63994:38639 in memory (size: 46.1 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 354 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:53 INFO DAGScheduler: ResultStage 5 (collectAsMap at RandomForest.scala:1054) finished in 0.376 s\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Job 4 finished: collectAsMap at RandomForest.scala:1054, took 1.253594 s\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 23ba3df63994:38639 in memory (size: 43.1 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 119.8 KiB, free 433.8 MiB)\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 433.7 MiB)\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 23ba3df63994:38639 (size: 27.2 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:53 INFO SparkContext: Created broadcast 9 from broadcast at RandomForest.scala:293\n",
            "24/10/21 11:31:53 INFO Instrumentation: [ac51ab98] {\"numFeatures\":692}\n",
            "24/10/21 11:31:53 INFO Instrumentation: [ac51ab98] {\"numClasses\":0}\n",
            "24/10/21 11:31:53 INFO Instrumentation: [ac51ab98] {\"numExamples\":55}\n",
            "24/10/21 11:31:53 INFO Instrumentation: [ac51ab98] {\"sumOfWeights\":55.0}\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 20.0 KiB, free 433.7 MiB)\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 433.7 MiB)\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 23ba3df63994:38639 (size: 13.0 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:53 INFO SparkContext: Created broadcast 10 from broadcast at RandomForest.scala:622\n",
            "24/10/21 11:31:53 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Registering RDD 32 (mapPartitions at RandomForest.scala:644) as input to shuffle 1\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Got job 5 (collectAsMap at RandomForest.scala:663) with 1 output partitions\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Final stage: ResultStage 7 (collectAsMap at RandomForest.scala:663)\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[32] at mapPartitions at RandomForest.scala:644), which has no missing parents\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 216.7 KiB, free 433.5 MiB)\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 79.1 KiB, free 433.4 MiB)\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 23ba3df63994:38639 (size: 79.1 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:53 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[32] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9788 bytes) \n",
            "24/10/21 11:31:53 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "24/10/21 11:31:53 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:53 INFO MemoryStore: Block rdd_31_0 stored as values in memory (estimated size 158.4 KiB, free 433.3 MiB)\n",
            "24/10/21 11:31:53 INFO BlockManagerInfo: Added rdd_31_0 in memory on 23ba3df63994:38639 (size: 158.4 KiB, free: 434.0 MiB)\n",
            "24/10/21 11:31:54 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2096 bytes result sent to driver\n",
            "24/10/21 11:31:54 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 392 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:54 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:54 INFO DAGScheduler: ShuffleMapStage 6 (mapPartitions at RandomForest.scala:644) finished in 0.435 s\n",
            "24/10/21 11:31:54 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/10/21 11:31:54 INFO DAGScheduler: running: Set()\n",
            "24/10/21 11:31:54 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
            "24/10/21 11:31:54 INFO DAGScheduler: failed: Set()\n",
            "24/10/21 11:31:54 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[34] at map at RandomForest.scala:663), which has no missing parents\n",
            "24/10/21 11:31:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.4 KiB, free 433.3 MiB)\n",
            "24/10/21 11:31:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 433.2 MiB)\n",
            "24/10/21 11:31:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 23ba3df63994:38639 (size: 3.8 KiB, free: 434.0 MiB)\n",
            "24/10/21 11:31:54 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[34] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:54 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (23ba3df63994, executor driver, partition 0, NODE_LOCAL, 9005 bytes) \n",
            "24/10/21 11:31:54 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "24/10/21 11:31:54 INFO ShuffleBlockFetcherIterator: Getting 1 (312.6 KiB) non-empty blocks including 1 (312.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/10/21 11:31:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/10/21 11:31:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 23ba3df63994:38639 in memory (size: 6.3 KiB, free: 434.0 MiB)\n",
            "24/10/21 11:31:54 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 23ba3df63994:38639 in memory (size: 79.1 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 6479 bytes result sent to driver\n",
            "24/10/21 11:31:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 862 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:54 INFO DAGScheduler: ResultStage 7 (collectAsMap at RandomForest.scala:663) finished in 0.882 s\n",
            "24/10/21 11:31:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "24/10/21 11:31:54 INFO DAGScheduler: Job 5 finished: collectAsMap at RandomForest.scala:663, took 1.344164 s\n",
            "24/10/21 11:31:54 INFO TorrentBroadcast: Destroying Broadcast(10) (from destroy at RandomForest.scala:674)\n",
            "24/10/21 11:31:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 2.9 KiB, free 433.6 MiB)\n",
            "24/10/21 11:31:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 433.6 MiB)\n",
            "24/10/21 11:31:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 23ba3df63994:38639 (size: 2.6 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:54 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 23ba3df63994:38639 in memory (size: 13.0 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:54 INFO SparkContext: Created broadcast 13 from broadcast at RandomForest.scala:622\n",
            "24/10/21 11:31:55 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Registering RDD 35 (mapPartitions at RandomForest.scala:644) as input to shuffle 2\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Got job 6 (collectAsMap at RandomForest.scala:663) with 1 output partitions\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsMap at RandomForest.scala:663)\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[35] at mapPartitions at RandomForest.scala:644), which has no missing parents\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.1 KiB, free 433.4 MiB)\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 68.8 KiB, free 433.3 MiB)\n",
            "24/10/21 11:31:55 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 23ba3df63994:38639 (size: 68.8 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:55 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[35] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:55 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:55 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9788 bytes) \n",
            "24/10/21 11:31:55 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "24/10/21 11:31:55 INFO BlockManager: Found block rdd_31_0 locally\n",
            "24/10/21 11:31:55 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2096 bytes result sent to driver\n",
            "24/10/21 11:31:55 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 65 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:55 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:55 INFO DAGScheduler: ShuffleMapStage 8 (mapPartitions at RandomForest.scala:644) finished in 0.094 s\n",
            "24/10/21 11:31:55 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/10/21 11:31:55 INFO DAGScheduler: running: Set()\n",
            "24/10/21 11:31:55 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
            "24/10/21 11:31:55 INFO DAGScheduler: failed: Set()\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at map at RandomForest.scala:663), which has no missing parents\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 7.6 KiB, free 433.3 MiB)\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 433.3 MiB)\n",
            "24/10/21 11:31:55 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 23ba3df63994:38639 (size: 4.0 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:55 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:55 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:55 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (23ba3df63994, executor driver, partition 0, NODE_LOCAL, 9005 bytes) \n",
            "24/10/21 11:31:55 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "24/10/21 11:31:55 INFO ShuffleBlockFetcherIterator: Getting 1 (31.7 KiB) non-empty blocks including 1 (31.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/10/21 11:31:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/10/21 11:31:55 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2934 bytes result sent to driver\n",
            "24/10/21 11:31:55 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 130 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:55 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:55 INFO DAGScheduler: ResultStage 9 (collectAsMap at RandomForest.scala:663) finished in 0.146 s\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "24/10/21 11:31:55 INFO DAGScheduler: Job 6 finished: collectAsMap at RandomForest.scala:663, took 0.259650 s\n",
            "24/10/21 11:31:55 INFO TorrentBroadcast: Destroying Broadcast(13) (from destroy at RandomForest.scala:674)\n",
            "24/10/21 11:31:55 INFO RandomForest: Internal timing for DecisionTree:\n",
            "24/10/21 11:31:55 INFO RandomForest:   init: 0.011336743\n",
            "  total: 1.864696891\n",
            "  findBestSplits: 1.809153095\n",
            "  chooseSplits: 1.800497186\n",
            "24/10/21 11:31:55 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 23ba3df63994:38639 in memory (size: 2.6 KiB, free: 434.1 MiB)\n",
            "24/10/21 11:31:55 INFO MapPartitionsRDD: Removing RDD 31 from persistence list\n",
            "24/10/21 11:31:55 INFO BlockManager: Removing RDD 31\n",
            "24/10/21 11:31:55 INFO TorrentBroadcast: Destroying Broadcast(9) (from destroy at RandomForest.scala:305)\n",
            "24/10/21 11:31:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 23ba3df63994:38639 in memory (size: 27.2 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:55 INFO Instrumentation: [ac51ab98] {\"numFeatures\":692}\n",
            "24/10/21 11:31:55 INFO Instrumentation: [ac51ab98] training finished\n",
            "24/10/21 11:31:55 INFO Instrumentation: [0353353b] training finished\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/spark-core_2.12-3.5.3.jar) to field java.nio.charset.Charset.name\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.5 MiB, free 414.1 MiB)\n",
            "24/10/21 11:31:55 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 414.1 MiB)\n",
            "24/10/21 11:31:55 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 23ba3df63994:38639 (size: 15.2 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:55 INFO SparkContext: Created broadcast 16 from broadcast at RandomForestRegressor.scala:238\n",
            "24/10/21 11:31:55 INFO Instrumentation: [ee94ffc1] training finished\n",
            "24/10/21 11:31:55 INFO FileSourceStrategy: Pushed Filters: \n",
            "24/10/21 11:31:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "24/10/21 11:31:55 INFO BlockManager: Removing RDD 31\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 23ba3df63994:38639 in memory (size: 68.8 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 40.247479 ms\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 199.3 KiB, free 414.2 MiB)\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 414.1 MiB)\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 23ba3df63994:38639 (size: 34.3 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:56 INFO SparkContext: Created broadcast 17 from broadcast at LibSVMRelation.scala:156\n",
            "24/10/21 11:31:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 23ba3df63994:38639 in memory (size: 3.8 KiB, free: 434.3 MiB)\n",
            "24/10/21 11:31:56 INFO SparkContext: Starting job: show at JavaRandomForestRegressorExample.java:74\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Got job 7 (show at JavaRandomForestRegressorExample.java:74) with 1 output partitions\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Final stage: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74)\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[41] at show at JavaRandomForestRegressorExample.java:74), which has no missing parents\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 96.2 KiB, free 414.0 MiB)\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 414.0 MiB)\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 23ba3df63994:38639 (size: 30.0 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:56 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[41] at show at JavaRandomForestRegressorExample.java:74) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:56 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:56 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:56 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 23ba3df63994:38639 in memory (size: 4.0 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 49.824521 ms\n",
            "24/10/21 11:31:56 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:56 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 7040 bytes result sent to driver\n",
            "24/10/21 11:31:56 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 237 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:56 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:56 INFO DAGScheduler: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74) finished in 0.270 s\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Job 7 finished: show at JavaRandomForestRegressorExample.java:74, took 0.287254 s\n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 8.762544 ms\n",
            "24/10/21 11:31:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "24/10/21 11:31:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 54.053944 ms\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 199.3 KiB, free 413.8 MiB)\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 413.8 MiB)\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 23ba3df63994:38639 (size: 34.3 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:56 INFO SparkContext: Created broadcast 19 from broadcast at LibSVMRelation.scala:156\n",
            "24/10/21 11:31:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "24/10/21 11:31:56 INFO SparkContext: Starting job: treeAggregate at Statistics.scala:58\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Got job 8 (treeAggregate at Statistics.scala:58) with 1 output partitions\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Final stage: ResultStage 11 (treeAggregate at Statistics.scala:58)\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Missing parents: List()\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[50] at treeAggregate at Statistics.scala:58), which has no missing parents\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 102.6 KiB, free 413.7 MiB)\n",
            "24/10/21 11:31:56 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 413.7 MiB)\n",
            "24/10/21 11:31:56 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 23ba3df63994:38639 (size: 33.1 KiB, free: 434.2 MiB)\n",
            "24/10/21 11:31:56 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585\n",
            "24/10/21 11:31:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[50] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\n",
            "24/10/21 11:31:56 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "24/10/21 11:31:56 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (23ba3df63994, executor driver, partition 0, PROCESS_LOCAL, 9799 bytes) \n",
            "24/10/21 11:31:56 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 49.460805 ms\n",
            "24/10/21 11:31:56 INFO CodeGenerator: Code generated in 12.40172 ms\n",
            "24/10/21 11:31:56 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "24/10/21 11:31:57 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3076 bytes result sent to driver\n",
            "24/10/21 11:31:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 219 ms on 23ba3df63994 (executor driver) (1/1)\n",
            "24/10/21 11:31:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
            "24/10/21 11:31:57 INFO DAGScheduler: ResultStage 11 (treeAggregate at Statistics.scala:58) finished in 0.242 s\n",
            "24/10/21 11:31:57 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/10/21 11:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "24/10/21 11:31:57 INFO DAGScheduler: Job 8 finished: treeAggregate at Statistics.scala:58, took 0.251175 s\n",
            "24/10/21 11:31:57 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/10/21 11:31:57 INFO SparkUI: Stopped Spark web UI at http://23ba3df63994:4040\n",
            "24/10/21 11:31:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/10/21 11:31:57 INFO MemoryStore: MemoryStore cleared\n",
            "24/10/21 11:31:57 INFO BlockManager: BlockManager stopped\n",
            "24/10/21 11:31:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/10/21 11:31:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/10/21 11:31:57 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/10/21 11:31:57 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/10/21 11:31:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-b27ceaa8-1960-4342-acf2-e10027ce6924\n",
            "24/10/21 11:31:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-87392112-3cbb-4f46-a4e9-d37a04db9378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this guide, we demonstrated how to install the essential Spark services—Spark Core, Spark Master, and Spark Worker—using the Bigtop distribution. We also explored how to leverage Bigtop's utilities to easily launch a Spark engine. Additionally, we executed two example jobs included in the Spark package."
      ],
      "metadata": {
        "id": "llnOxcrrzw36"
      }
    }
  ]
}