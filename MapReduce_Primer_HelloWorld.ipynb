{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOpT0soTGaqyxm/vlb8BfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using `IdentityMapper` `IdentityReducer`, thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n"
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.6.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "829df74f-efd1-4484-a374-44fcf2c95b2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "cdcb2d0e-e387-452f-d601-c3828bffe4ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.3.6\n",
            "PATH is hadoop-3.3.6/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "51c06892-7c62-460c-f16d-c8ffd51f2547"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "7ba38b06-d0ee-4606-974d-63d68835700e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-03-09 20:01:20,913 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 20:01:21,085 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 20:01:21,085 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 20:01:21,117 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:21,362 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 20:01:21,392 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 20:01:21,708 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2028994056_0001\n",
            "2024-03-09 20:01:21,708 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 20:01:21,907 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 20:01:21,909 INFO mapreduce.Job: Running job: job_local2028994056_0001\n",
            "2024-03-09 20:01:21,917 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 20:01:21,920 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 20:01:21,927 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:21,927 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:21,985 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 20:01:21,991 INFO mapred.LocalJobRunner: Starting task: attempt_local2028994056_0001_m_000000_0\n",
            "2024-03-09 20:01:22,024 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:22,026 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:22,049 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:22,060 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:22,072 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 20:01:22,121 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 20:01:22,121 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 20:01:22,121 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 20:01:22,121 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 20:01:22,121 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 20:01:22,126 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 20:01:22,129 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 20:01:22,142 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 20:01:22,145 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 20:01:22,145 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 20:01:22,145 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 20:01:22,146 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 20:01:22,146 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 20:01:22,147 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 20:01:22,147 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 20:01:22,148 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 20:01:22,148 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 20:01:22,149 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 20:01:22,149 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 20:01:22,168 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 20:01:22,173 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 20:01:22,174 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 20:01:22,174 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 20:01:22,177 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:22,177 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 20:01:22,177 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 20:01:22,177 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-03-09 20:01:22,178 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 20:01:22,186 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 20:01:22,199 INFO mapred.Task: Task:attempt_local2028994056_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:22,203 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 20:01:22,203 INFO mapred.Task: Task 'attempt_local2028994056_0001_m_000000_0' done.\n",
            "2024-03-09 20:01:22,211 INFO mapred.Task: Final Counters for attempt_local2028994056_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=781505\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=301989888\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 20:01:22,211 INFO mapred.LocalJobRunner: Finishing task: attempt_local2028994056_0001_m_000000_0\n",
            "2024-03-09 20:01:22,212 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 20:01:22,215 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 20:01:22,215 INFO mapred.LocalJobRunner: Starting task: attempt_local2028994056_0001_r_000000_0\n",
            "2024-03-09 20:01:22,224 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:22,224 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:22,224 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:22,226 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@668fb8f6\n",
            "2024-03-09 20:01:22,228 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:22,249 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 20:01:22,260 INFO reduce.EventFetcher: attempt_local2028994056_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 20:01:22,287 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2028994056_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-03-09 20:01:22,292 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local2028994056_0001_m_000000_0\n",
            "2024-03-09 20:01:22,296 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-03-09 20:01:22,301 WARN io.ReadaheadPool: Failed readahead on ifile\n",
            "EBADF: Bad file descriptor\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:426)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:296)\n",
            "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:220)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "2024-03-09 20:01:22,301 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 20:01:22,303 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:22,303 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 20:01:22,309 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 20:01:22,309 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 20:01:22,311 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 20:01:22,311 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-03-09 20:01:22,312 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 20:01:22,312 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 20:01:22,312 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 20:01:22,313 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:22,338 INFO mapred.Task: Task:attempt_local2028994056_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:22,340 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:22,340 INFO mapred.Task: Task attempt_local2028994056_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 20:01:22,346 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2028994056_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 20:01:22,348 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 20:01:22,348 INFO mapred.Task: Task 'attempt_local2028994056_0001_r_000000_0' done.\n",
            "2024-03-09 20:01:22,349 INFO mapred.Task: Final Counters for attempt_local2028994056_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141488\n",
            "\t\tFILE: Number of bytes written=781555\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=301989888\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 20:01:22,349 INFO mapred.LocalJobRunner: Finishing task: attempt_local2028994056_0001_r_000000_0\n",
            "2024-03-09 20:01:22,349 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 20:01:22,914 INFO mapreduce.Job: Job job_local2028994056_0001 running in uber mode : false\n",
            "2024-03-09 20:01:22,915 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 20:01:22,917 INFO mapreduce.Job: Job job_local2028994056_0001 completed successfully\n",
            "2024-03-09 20:01:22,928 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282898\n",
            "\t\tFILE: Number of bytes written=1563060\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=603979776\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 20:01:22,928 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "d8c3dd75-aca5-4165-aaeb-f57e4e377293"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "3ab8e401-6151-4241-c3b7-b951c71977c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-03-09 20:01 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-03-09 20:01 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "6e94b1ce-fea5-4e72-bbf7-a1c58b820a14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Mar  9 20:01 part-00000\n",
            "-rw-r--r-- 1 root root  0 Mar  9 20:01 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "d2340516-50fd-4945-bda1-4510f9a82885"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "3dfc9d86-85b9-497d-bfba-97f890a3424b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-09 20:01:26,991 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "9337fa36-87fd-4ec5-c846-98810959ea59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 20:01:28,469 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 20:01:30,472 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 20:01:30,738 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 20:01:30,738 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 20:01:30,769 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:31,048 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 20:01:31,087 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 20:01:31,358 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local32794142_0001\n",
            "2024-03-09 20:01:31,358 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 20:01:31,547 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 20:01:31,551 INFO mapreduce.Job: Running job: job_local32794142_0001\n",
            "2024-03-09 20:01:31,551 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 20:01:31,558 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 20:01:31,566 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:31,570 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:31,603 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 20:01:31,606 INFO mapred.LocalJobRunner: Starting task: attempt_local32794142_0001_m_000000_0\n",
            "2024-03-09 20:01:31,627 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:31,628 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:31,647 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:31,654 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:31,664 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 20:01:31,699 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 20:01:31,699 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 20:01:31,699 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 20:01:31,699 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 20:01:31,700 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 20:01:31,702 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 20:01:31,707 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:31,707 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 20:01:31,707 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 20:01:31,707 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-03-09 20:01:31,707 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 20:01:31,714 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 20:01:31,731 INFO mapred.Task: Task:attempt_local32794142_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:31,733 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:31,733 INFO mapred.Task: Task 'attempt_local32794142_0001_m_000000_0' done.\n",
            "2024-03-09 20:01:31,738 INFO mapred.Task: Final Counters for attempt_local32794142_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=773243\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 20:01:31,739 INFO mapred.LocalJobRunner: Finishing task: attempt_local32794142_0001_m_000000_0\n",
            "2024-03-09 20:01:31,739 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 20:01:31,742 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 20:01:31,745 INFO mapred.LocalJobRunner: Starting task: attempt_local32794142_0001_r_000000_0\n",
            "2024-03-09 20:01:31,752 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:31,752 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:31,752 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:31,756 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@368f4c06\n",
            "2024-03-09 20:01:31,757 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:31,772 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 20:01:31,780 INFO reduce.EventFetcher: attempt_local32794142_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 20:01:31,802 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local32794142_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-03-09 20:01:31,805 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local32794142_0001_m_000000_0\n",
            "2024-03-09 20:01:31,808 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-03-09 20:01:31,811 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 20:01:31,812 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:31,812 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 20:01:31,816 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 20:01:31,816 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 20:01:31,817 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 20:01:31,818 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-03-09 20:01:31,818 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 20:01:31,818 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 20:01:31,819 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 20:01:31,819 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:31,826 INFO mapred.Task: Task:attempt_local32794142_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:31,827 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 20:01:31,827 INFO mapred.Task: Task attempt_local32794142_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 20:01:31,829 INFO output.FileOutputCommitter: Saved output of task 'attempt_local32794142_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 20:01:31,830 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 20:01:31,830 INFO mapred.Task: Task 'attempt_local32794142_0001_r_000000_0' done.\n",
            "2024-03-09 20:01:31,832 INFO mapred.Task: Final Counters for attempt_local32794142_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141502\n",
            "\t\tFILE: Number of bytes written=773301\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 20:01:31,832 INFO mapred.LocalJobRunner: Finishing task: attempt_local32794142_0001_r_000000_0\n",
            "2024-03-09 20:01:31,832 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 20:01:32,556 INFO mapreduce.Job: Job job_local32794142_0001 running in uber mode : false\n",
            "2024-03-09 20:01:32,557 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 20:01:32,559 INFO mapreduce.Job: Job job_local32794142_0001 completed successfully\n",
            "2024-03-09 20:01:32,567 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282912\n",
            "\t\tFILE: Number of bytes written=1546544\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=687865856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 20:01:32,567 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "74f791c4-3aec-4f40-f4bb-72db1117c6b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "dfa0295f-e20a-4f5e-d674-72500bb959c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "38091c9f-72d8-4b20-fed7-29e5769be908"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 20:01:35,442 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 20:01:36,946 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 20:01:37,120 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 20:01:37,120 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 20:01:37,137 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:37,314 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 20:01:37,338 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 20:01:37,550 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local882439583_0001\n",
            "2024-03-09 20:01:37,550 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 20:01:37,732 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 20:01:37,733 INFO mapreduce.Job: Running job: job_local882439583_0001\n",
            "2024-03-09 20:01:37,756 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 20:01:37,760 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 20:01:37,765 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:37,765 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:37,818 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 20:01:37,822 INFO mapred.LocalJobRunner: Starting task: attempt_local882439583_0001_m_000000_0\n",
            "2024-03-09 20:01:37,854 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:37,856 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:37,889 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:37,896 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:37,907 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 20:01:37,920 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:37,928 INFO mapred.Task: Task:attempt_local882439583_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:37,931 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:37,931 INFO mapred.Task: Task attempt_local882439583_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 20:01:37,932 INFO output.FileOutputCommitter: Saved output of task 'attempt_local882439583_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 20:01:37,937 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:37,937 INFO mapred.Task: Task 'attempt_local882439583_0001_m_000000_0' done.\n",
            "2024-03-09 20:01:37,944 INFO mapred.Task: Final Counters for attempt_local882439583_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=776287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 20:01:37,944 INFO mapred.LocalJobRunner: Finishing task: attempt_local882439583_0001_m_000000_0\n",
            "2024-03-09 20:01:37,945 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 20:01:38,759 INFO mapreduce.Job: Job job_local882439583_0001 running in uber mode : false\n",
            "2024-03-09 20:01:38,762 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 20:01:38,765 INFO mapreduce.Job: Job job_local882439583_0001 completed successfully\n",
            "2024-03-09 20:01:38,774 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=776287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 20:01:38,775 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "b227027b-eb0f-46d1-f08d-d0d7992c5539"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "c0b66feb-bf48-42cd-ac38-909dfb638c2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 20:01:41,708 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 20:01:43,633 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 20:01:43,811 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 20:01:43,811 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 20:01:43,827 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 20:01:44,017 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 20:01:44,048 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 20:01:44,259 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local276455906_0001\n",
            "2024-03-09 20:01:44,259 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 20:01:44,464 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 20:01:44,465 INFO mapreduce.Job: Running job: job_local276455906_0001\n",
            "2024-03-09 20:01:44,472 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 20:01:44,476 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 20:01:44,485 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:44,485 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:44,540 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 20:01:44,545 INFO mapred.LocalJobRunner: Starting task: attempt_local276455906_0001_m_000000_0\n",
            "2024-03-09 20:01:44,578 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 20:01:44,579 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 20:01:44,600 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 20:01:44,611 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 20:01:44,624 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 20:01:44,633 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 20:01:44,638 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 20:01:44,641 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 20:01:44,641 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 20:01:44,641 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 20:01:44,642 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 20:01:44,642 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 20:01:44,643 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 20:01:44,644 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 20:01:44,644 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 20:01:44,644 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 20:01:44,644 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 20:01:44,645 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 20:01:44,665 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 20:01:44,669 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 20:01:44,670 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 20:01:44,670 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 20:01:44,673 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:44,680 INFO mapred.Task: Task:attempt_local276455906_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 20:01:44,681 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 20:01:44,681 INFO mapred.Task: Task attempt_local276455906_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 20:01:44,683 INFO output.FileOutputCommitter: Saved output of task 'attempt_local276455906_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 20:01:44,684 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 20:01:44,684 INFO mapred.Task: Task 'attempt_local276455906_0001_m_000000_0' done.\n",
            "2024-03-09 20:01:44,691 INFO mapred.Task: Final Counters for attempt_local276455906_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 20:01:44,691 INFO mapred.LocalJobRunner: Finishing task: attempt_local276455906_0001_m_000000_0\n",
            "2024-03-09 20:01:44,692 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 20:01:45,470 INFO mapreduce.Job: Job job_local276455906_0001 running in uber mode : false\n",
            "2024-03-09 20:01:45,472 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 20:01:45,475 INFO mapreduce.Job: Job job_local276455906_0001 completed successfully\n",
            "2024-03-09 20:01:45,481 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 20:01:45,481 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "3e1e2461-3181-4ab1-82dd-5931eebd38eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}
