{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53",
      "metadata": {
        "tags": [],
        "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53"
      },
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using `IdentityMapper` `IdentityReducer`, thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop\n",
        "\n"
      ],
      "metadata": {
        "id": "Uo12bOJ45USo"
      },
      "id": "Uo12bOJ45USo"
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.6.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lGm2Lw-5h7c",
        "outputId": "da704597-307b-4ba4-8cbd-323041bfa55d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.3.6\n"
          ]
        }
      ],
      "id": "5lGm2Lw-5h7c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "zKn5-bIX-yXW"
      },
      "id": "zKn5-bIX-yXW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "2zRCjJUsFlQ8"
      },
      "id": "2zRCjJUsFlQ8"
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAvCdRKI-3dr",
        "outputId": "d40e0f1e-b298-4160-84e4-6af0e93ce503"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.3.6\n",
            "PATH is hadoop-3.3.6/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "id": "ZAvCdRKI-3dr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "Qnlmws4mAwbO"
      },
      "id": "Qnlmws4mAwbO"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTOc3dH5A5uG",
        "outputId": "e7606328-c1ad-4965-cbfc-788d55c3a267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ],
      "id": "GTOc3dH5A5uG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "a1eZ9qrpFsAd"
      },
      "id": "a1eZ9qrpFsAd"
    },
    {
      "cell_type": "markdown",
      "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2",
      "metadata": {
        "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2"
      },
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c",
      "metadata": {
        "tags": [],
        "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c"
      },
      "outputs": [],
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56241317-96b3-40c8-b377-688df5dc0eeb",
      "metadata": {
        "id": "56241317-96b3-40c8-b377-688df5dc0eeb"
      },
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d40f86c-9287-442b-ba6b-b42489700910",
      "metadata": {
        "id": "1d40f86c-9287-442b-ba6b-b42489700910"
      },
      "source": [
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders bt design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a",
      "metadata": {
        "tags": [],
        "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a",
        "outputId": "4b083628-e5f0-450c-9430-aae471aa7ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-03-09 18:23:52,770 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 18:23:53,018 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 18:23:53,018 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 18:23:53,049 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:23:53,379 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 18:23:53,405 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 18:23:53,817 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local173048558_0001\n",
            "2024-03-09 18:23:53,817 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 18:23:54,103 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 18:23:54,106 INFO mapreduce.Job: Running job: job_local173048558_0001\n",
            "2024-03-09 18:23:54,141 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 18:23:54,148 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 18:23:54,164 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:23:54,165 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:23:54,272 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 18:23:54,277 INFO mapred.LocalJobRunner: Starting task: attempt_local173048558_0001_m_000000_0\n",
            "2024-03-09 18:23:54,389 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:23:54,391 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:23:54,448 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:23:54,470 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:23:54,518 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 18:23:54,677 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 18:23:54,677 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 18:23:54,677 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 18:23:54,677 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 18:23:54,678 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 18:23:54,683 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 18:23:54,686 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 18:23:54,703 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 18:23:54,721 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 18:23:54,721 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 18:23:54,722 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 18:23:54,723 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 18:23:54,723 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 18:23:54,724 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 18:23:54,726 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 18:23:54,726 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 18:23:54,734 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 18:23:54,735 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 18:23:54,736 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 18:23:54,791 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 18:23:54,796 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 18:23:54,801 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 18:23:54,802 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 18:23:54,806 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:23:54,806 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 18:23:54,806 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 18:23:54,806 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-03-09 18:23:54,806 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 18:23:54,816 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 18:23:54,862 INFO mapred.Task: Task:attempt_local173048558_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:23:54,867 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 18:23:54,867 INFO mapred.Task: Task 'attempt_local173048558_0001_m_000000_0' done.\n",
            "2024-03-09 18:23:54,876 INFO mapred.Task: Final Counters for attempt_local173048558_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=778415\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=411041792\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 18:23:54,876 INFO mapred.LocalJobRunner: Finishing task: attempt_local173048558_0001_m_000000_0\n",
            "2024-03-09 18:23:54,877 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 18:23:54,881 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 18:23:54,895 INFO mapred.LocalJobRunner: Starting task: attempt_local173048558_0001_r_000000_0\n",
            "2024-03-09 18:23:54,913 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:23:54,913 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:23:54,913 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:23:54,934 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3f9a981b\n",
            "2024-03-09 18:23:54,939 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:23:55,000 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 18:23:55,013 INFO reduce.EventFetcher: attempt_local173048558_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 18:23:55,098 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local173048558_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-03-09 18:23:55,115 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local173048558_0001_m_000000_0\n",
            "2024-03-09 18:23:55,118 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-03-09 18:23:55,128 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 18:23:55,130 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:23:55,130 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 18:23:55,142 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 18:23:55,143 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 18:23:55,145 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 18:23:55,145 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-03-09 18:23:55,146 INFO mapreduce.Job: Job job_local173048558_0001 running in uber mode : false\n",
            "2024-03-09 18:23:55,147 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 18:23:55,147 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 18:23:55,147 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 18:23:55,156 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 18:23:55,156 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:23:55,179 INFO mapred.Task: Task:attempt_local173048558_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:23:55,184 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:23:55,185 INFO mapred.Task: Task attempt_local173048558_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 18:23:55,200 INFO output.FileOutputCommitter: Saved output of task 'attempt_local173048558_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 18:23:55,202 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 18:23:55,203 INFO mapred.Task: Task 'attempt_local173048558_0001_r_000000_0' done.\n",
            "2024-03-09 18:23:55,203 INFO mapred.Task: Final Counters for attempt_local173048558_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141488\n",
            "\t\tFILE: Number of bytes written=778465\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=411041792\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:23:55,203 INFO mapred.LocalJobRunner: Finishing task: attempt_local173048558_0001_r_000000_0\n",
            "2024-03-09 18:23:55,203 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 18:23:56,149 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 18:23:56,149 INFO mapreduce.Job: Job job_local173048558_0001 completed successfully\n",
            "2024-03-09 18:23:56,161 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282898\n",
            "\t\tFILE: Number of bytes written=1556880\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=822083584\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:23:56,161 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebec78b-fc7a-483d-9408-ed507ac81180",
      "metadata": {
        "id": "8ebec78b-fc7a-483d-9408-ed507ac81180"
      },
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37ca3290-a8d9-44f0-9970-3809fce1aace",
      "metadata": {
        "tags": [],
        "id": "37ca3290-a8d9-44f0-9970-3809fce1aace",
        "outputId": "e1a1c1a1-5a76-49f6-ee63-790d3394929b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "4W1AYGwPMQZn"
      },
      "id": "4W1AYGwPMQZn"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E81rOKlPMB7U",
        "outputId": "522324bf-7149-4cec-b81a-2edd65a0ea26"
      },
      "id": "E81rOKlPMB7U",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-03-09 18:23 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-03-09 18:23 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4tGI1CMdHS",
        "outputId": "688922d2-9d75-482c-f7a0-3d0f9eeea5c6"
      },
      "id": "PK4tGI1CMdHS",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Mar  9 18:23 part-00000\n",
            "-rw-r--r-- 1 root root  0 Mar  9 18:23 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "Q5gEVXjD1Wca"
      },
      "id": "Q5gEVXjD1Wca"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb0vmdyMOleV",
        "outputId": "39095581-7e56-4eab-801c-57ee9cba9140"
      },
      "id": "Zb0vmdyMOleV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "XHWsKFuHOrUh"
      },
      "id": "XHWsKFuHOrUh"
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geo7HmAq2Gov",
        "outputId": "d7a55e4b-3399-437b-e9e2-2ff091f5e4d6"
      },
      "id": "Geo7HmAq2Gov",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-09 18:24:01,626 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR3Vo2YG5lw_",
        "outputId": "c403e001-339f-419d-b9c1-aec0f5cc541f"
      },
      "id": "dR3Vo2YG5lw_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 18:24:03,656 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 18:24:05,813 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 18:24:05,998 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 18:24:05,999 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 18:24:06,043 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:24:06,384 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 18:24:06,434 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 18:24:07,030 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local931089552_0001\n",
            "2024-03-09 18:24:07,030 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 18:24:07,586 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 18:24:07,588 INFO mapreduce.Job: Running job: job_local931089552_0001\n",
            "2024-03-09 18:24:07,602 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 18:24:07,605 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 18:24:07,625 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:07,628 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:07,739 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 18:24:07,751 INFO mapred.LocalJobRunner: Starting task: attempt_local931089552_0001_m_000000_0\n",
            "2024-03-09 18:24:07,819 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:07,822 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:07,882 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:24:07,904 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:24:07,925 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 18:24:08,059 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 18:24:08,060 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 18:24:08,060 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 18:24:08,060 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 18:24:08,060 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 18:24:08,072 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 18:24:08,083 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:24:08,084 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 18:24:08,084 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 18:24:08,084 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-03-09 18:24:08,084 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 18:24:08,103 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 18:24:08,171 INFO mapred.Task: Task:attempt_local931089552_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:24:08,176 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:24:08,176 INFO mapred.Task: Task 'attempt_local931089552_0001_m_000000_0' done.\n",
            "2024-03-09 18:24:08,187 INFO mapred.Task: Final Counters for attempt_local931089552_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=776323\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 18:24:08,187 INFO mapred.LocalJobRunner: Finishing task: attempt_local931089552_0001_m_000000_0\n",
            "2024-03-09 18:24:08,191 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 18:24:08,206 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 18:24:08,206 INFO mapred.LocalJobRunner: Starting task: attempt_local931089552_0001_r_000000_0\n",
            "2024-03-09 18:24:08,219 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:08,219 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:08,220 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:24:08,227 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@346cdbb3\n",
            "2024-03-09 18:24:08,233 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:24:08,285 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 18:24:08,317 INFO reduce.EventFetcher: attempt_local931089552_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 18:24:08,380 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local931089552_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-03-09 18:24:08,399 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local931089552_0001_m_000000_0\n",
            "2024-03-09 18:24:08,406 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-03-09 18:24:08,417 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 18:24:08,418 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:24:08,419 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 18:24:08,428 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 18:24:08,428 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 18:24:08,437 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 18:24:08,438 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-03-09 18:24:08,439 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 18:24:08,439 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 18:24:08,449 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 18:24:08,450 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:24:08,468 INFO mapred.Task: Task:attempt_local931089552_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:24:08,473 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 18:24:08,475 INFO mapred.Task: Task attempt_local931089552_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 18:24:08,479 INFO output.FileOutputCommitter: Saved output of task 'attempt_local931089552_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 18:24:08,486 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 18:24:08,486 INFO mapred.Task: Task 'attempt_local931089552_0001_r_000000_0' done.\n",
            "2024-03-09 18:24:08,488 INFO mapred.Task: Final Counters for attempt_local931089552_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141502\n",
            "\t\tFILE: Number of bytes written=776381\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 18:24:08,488 INFO mapred.LocalJobRunner: Finishing task: attempt_local931089552_0001_r_000000_0\n",
            "2024-03-09 18:24:08,488 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 18:24:08,620 INFO mapreduce.Job: Job job_local931089552_0001 running in uber mode : false\n",
            "2024-03-09 18:24:08,622 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 18:24:08,623 INFO mapreduce.Job: Job job_local931089552_0001 completed successfully\n",
            "2024-03-09 18:24:08,652 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282912\n",
            "\t\tFILE: Number of bytes written=1552704\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=717225984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 18:24:08,653 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "rEY6DxF8QRak"
      },
      "id": "rEY6DxF8QRak"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7DvyWpZz0kO",
        "outputId": "c5b13a24-d38c-4190-ce72-564d91249fea"
      },
      "id": "y7DvyWpZz0kO",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "u7dRikhCQhsY"
      },
      "id": "u7dRikhCQhsY"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAoEeKpyQlAG",
        "outputId": "be26f0d8-a895-4859-d27e-cbb83480cc22"
      },
      "id": "fAoEeKpyQlAG",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "U5s_wfkvQowG"
      },
      "id": "U5s_wfkvQowG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "48l28qe4SlSp"
      },
      "id": "48l28qe4SlSp"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs1DQ6OISoO6",
        "outputId": "e205df85-e0e7-4be6-9fd8-ea11d1220160"
      },
      "id": "Bs1DQ6OISoO6",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 18:24:12,810 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 18:24:14,973 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 18:24:15,175 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 18:24:15,175 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 18:24:15,200 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:24:15,482 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 18:24:15,517 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 18:24:15,828 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1489118623_0001\n",
            "2024-03-09 18:24:15,828 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 18:24:16,112 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 18:24:16,114 INFO mapreduce.Job: Running job: job_local1489118623_0001\n",
            "2024-03-09 18:24:16,128 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 18:24:16,131 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 18:24:16,139 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:16,139 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:16,199 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 18:24:16,205 INFO mapred.LocalJobRunner: Starting task: attempt_local1489118623_0001_m_000000_0\n",
            "2024-03-09 18:24:16,239 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:16,242 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:16,270 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:24:16,283 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:24:16,305 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 18:24:16,337 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:24:16,353 INFO mapred.Task: Task:attempt_local1489118623_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:24:16,355 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:24:16,355 INFO mapred.Task: Task attempt_local1489118623_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 18:24:16,366 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1489118623_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 18:24:16,372 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:24:16,372 INFO mapred.Task: Task 'attempt_local1489118623_0001_m_000000_0' done.\n",
            "2024-03-09 18:24:16,382 INFO mapred.Task: Final Counters for attempt_local1489118623_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 18:24:16,383 INFO mapred.LocalJobRunner: Finishing task: attempt_local1489118623_0001_m_000000_0\n",
            "2024-03-09 18:24:16,384 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 18:24:17,124 INFO mapreduce.Job: Job job_local1489118623_0001 running in uber mode : false\n",
            "2024-03-09 18:24:17,127 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 18:24:17,132 INFO mapreduce.Job: Job job_local1489118623_0001 completed successfully\n",
            "2024-03-09 18:24:17,139 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 18:24:17,140 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "s8B4g65TU0vn"
      },
      "id": "s8B4g65TU0vn"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTC44BEmSuw8",
        "outputId": "c97369d1-6791-4f74-9a0a-f6cedb5317a1"
      },
      "id": "kTC44BEmSuw8",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted."
      ],
      "metadata": {
        "id": "h9CzGcNkVBzJ"
      },
      "id": "h9CzGcNkVBzJ"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2qCacnSLX8KT"
      },
      "id": "2qCacnSLX8KT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvoil63X8nQ"
      },
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application"
      ],
      "id": "afvoil63X8nQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29yfN39VYSuh"
      },
      "source": [
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "id": "29yfN39VYSuh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": [],
        "outputId": "a8df9817-18bc-4941-914d-4e2688dc560b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pVHowpwYSuh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 18:24:22,155 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 18:24:24,349 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 18:24:24,548 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 18:24:24,548 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 18:24:24,572 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:24:24,879 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 18:24:24,904 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 18:24:25,180 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1191764771_0001\n",
            "2024-03-09 18:24:25,180 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 18:24:25,467 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 18:24:25,469 INFO mapreduce.Job: Running job: job_local1191764771_0001\n",
            "2024-03-09 18:24:25,483 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 18:24:25,487 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 18:24:25,502 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:25,502 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:25,558 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 18:24:25,564 INFO mapred.LocalJobRunner: Starting task: attempt_local1191764771_0001_m_000000_0\n",
            "2024-03-09 18:24:25,601 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:24:25,601 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:24:25,656 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:24:25,666 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:24:25,682 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 18:24:25,695 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 18:24:25,701 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 18:24:25,702 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 18:24:25,703 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 18:24:25,703 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 18:24:25,704 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 18:24:25,704 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 18:24:25,705 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 18:24:25,705 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 18:24:25,706 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 18:24:25,706 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 18:24:25,706 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 18:24:25,707 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 18:24:25,726 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 18:24:25,730 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 18:24:25,735 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 18:24:25,735 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 18:24:25,739 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:24:25,749 INFO mapred.Task: Task:attempt_local1191764771_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:24:25,750 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:24:25,751 INFO mapred.Task: Task attempt_local1191764771_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 18:24:25,753 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1191764771_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 18:24:25,754 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 18:24:25,754 INFO mapred.Task: Task 'attempt_local1191764771_0001_m_000000_0' done.\n",
            "2024-03-09 18:24:25,763 INFO mapred.Task: Final Counters for attempt_local1191764771_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=782329\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=424673280\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:24:25,763 INFO mapred.LocalJobRunner: Finishing task: attempt_local1191764771_0001_m_000000_0\n",
            "2024-03-09 18:24:25,764 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 18:24:26,479 INFO mapreduce.Job: Job job_local1191764771_0001 running in uber mode : false\n",
            "2024-03-09 18:24:26,481 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 18:24:26,484 INFO mapreduce.Job: Job job_local1191764771_0001 completed successfully\n",
            "2024-03-09 18:24:26,494 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=782329\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=424673280\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:24:26,494 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "id": "3pVHowpwYSuh"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0V6lcScZQG5",
        "outputId": "4375624c-d86c-4ab7-e3cd-974bb15447ff"
      },
      "id": "G0V6lcScZQG5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}