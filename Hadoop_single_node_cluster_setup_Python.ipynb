{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "qQsQTnh_OS0x",
        "McIME8-uo3vE",
        "cVJuMb2S_Z-6",
        "1Ez3vXkfABId",
        "ZNrU4cMEAKos",
        "UEHqbVByASWe",
        "wBlhhfoGAZur",
        "q_s9ZgjHOQxb",
        "33u0fS_0_Gut",
        "u6o4kQjNqyv7",
        "w3_gaoxYGm6R",
        "Cz1eDw4X6r_t",
        "UoZhdlmOeXna",
        "bGYNMicCLyBX",
        "zamj-W64wbVa"
      ],
      "authorship_tag": "ABX9TyPjxaapc1RK7/xGZU7Emc17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hadoop single-node cluster setup with Python"
      ],
      "metadata": {
        "id": "_7teiy-B-x8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "qQsQTnh_OS0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RJTcdb67Gafx"
      },
      "outputs": [],
      "source": [
        "# URL for downloading Hadoop\n",
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.4.tar.gz\"\n",
        "\n",
        "# logging level (should be one of: DEBUG, INFO, WARNING, ERROR, CRITICAL) \n",
        "LOGGING_LEVEL = \"DEBUG\" #@param [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n",
        "\n",
        "# JAVA PATH\n",
        "JAVA_PATH = '/usr/lib/jvm/java-11-openjdk-amd64'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup logging"
      ],
      "metadata": {
        "id": "McIME8-uo3vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging_level = getattr(logging, LOGGING_LEVEL.upper(), 10)\n",
        "\n",
        "logging.basicConfig(level=logging_level, \\\n",
        "                    format='%(asctime)s - %(levelname)s: %(message)s', \\\n",
        "                    datefmt='%d-%b-%y %I:%M:%S %p')\n",
        "\n",
        "logger = logging.getLogger('my_logger')"
      ],
      "metadata": {
        "id": "tZzz6S5aPdkY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start `sshd` server"
      ],
      "metadata": {
        "id": "cVJuMb2S_Z-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `openssh-server`"
      ],
      "metadata": {
        "id": "1Ez3vXkfABId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess"
      ],
      "metadata": {
        "id": "gbSclw8sS3rO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Installing {}\".format(\"openssh-server\"))\n",
        "cmd = [\"apt-get\", \"install\", \"openssh-server\"]\n",
        "result = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H47CJUEjPj_I",
        "outputId": "f4a0ee68-7d87-4286-f0f8-3d952175aded"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:04 PM - INFO: Installing openssh-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure `sshd` service"
      ],
      "metadata": {
        "id": "ZNrU4cMEAKos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ssh_config_file = \"/etc/ssh/ssh_config\"\n",
        "with open(ssh_config_file, \"r+\") as f:\n",
        "    var = 'StrictHostKeyChecking no'\n",
        "    line_found = any(line.strip().startswith(var) for line in f)\n",
        "    if not line_found:\n",
        "      f.seek(0, os.SEEK_END)\n",
        "      f.write(var +\"\\n\")"
      ],
      "metadata": {
        "id": "RsDuBdmkU6rJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start `sshd` service"
      ],
      "metadata": {
        "id": "UEHqbVByASWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Starting {}\".format(\"openssh-server\"))\n",
        "cmd = [\"/etc/init.d/ssh\", \"restart\"]\n",
        "result = subprocess.check_output(cmd, stderr=subprocess.STDOUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_S-0e0TP4Bm",
        "outputId": "04d871d0-c238-4e68-ccd7-f4e6c2d9b553"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:06 PM - INFO: Starting openssh-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create private/public key \n",
        "\n",
        "The private/public key pair is needed for passwordless authentication.\n",
        "\n",
        "Note: for some reason `Crypto` doesn't work without `pycryptodome` (see [ModuleNotFoundError: No module named 'Crypto' Error](https://stackoverflow.com/questions/51824628/modulenotfounderror-no-module-named-crypto-error)."
      ],
      "metadata": {
        "id": "wBlhhfoGAZur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install Crypto\n",
        "pip install pycryptodome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aYrKB1n2U0f",
        "outputId": "b89ebfbc-63d9-4820-efa9-3d0254464fbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Crypto in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: shellescape in /usr/local/lib/python3.7/dist-packages (from Crypto) (3.8.1)\n",
            "Requirement already satisfied: Naked in /usr/local/lib/python3.7/dist-packages (from Crypto) (0.1.31)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Naked->Crypto) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Naked->Crypto) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Naked->Crypto) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Naked->Crypto) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Naked->Crypto) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Naked->Crypto) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Crypto"
      ],
      "metadata": {
        "id": "6XdZ2nii2Z_e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Crypto.PublicKey import RSA\n",
        "key = RSA.generate(2048)\n",
        "if not os.path.exists(os.path.join(os.path.expanduser('~'), '.ssh')):\n",
        "  os.makedirs(os.path.join(os.path.expanduser('~'), '.ssh'), mode=700)\n",
        "#os.chmod(os.path.join(os.path.expanduser('~'),'.ssh'), int('700', base=8))\n",
        "\n",
        "with open(os.path.join(os.path.expanduser('~'), '.ssh/id_rsa'), 'wb') as f:\n",
        "  os.chmod(os.path.join(os.path.expanduser('~'),'.ssh/id_rsa'), int('0600', base=8))\n",
        "  f.write(key.export_key('PEM', passphrase=None))\n",
        "pubkey = key.publickey()\n",
        "\n",
        "with open(os.path.join(os.path.expanduser('~'), '.ssh/id_rsa.pub'), 'wb') as f:\n",
        "    f.write(pubkey.exportKey('OpenSSH'))"
      ],
      "metadata": {
        "id": "RfaHQ2mP2cnH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(os.path.expanduser('~'), '.ssh/authorized_keys'), 'w') as a:\n",
        "  with open(os.path.join(os.path.expanduser('~'),'.ssh/id_rsa.pub'), 'r') as f:\n",
        "    a.write(f.read())\n",
        "  os.chmod(os.path.join(os.path.expanduser('~'), '.ssh/authorized_keys'), int('600', base=8))\n",
        "  "
      ],
      "metadata": {
        "id": "LDbc--yKfLhk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test `ssh` connection\n",
        "\n",
        "The output should be 'hi!' if the passphraseless connection is working."
      ],
      "metadata": {
        "id": "27Haz3a4_pCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgHjpcywWHBj",
        "outputId": "48595382-0d4a-429e-9fcb-37e72684805c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Hadoop \n"
      ],
      "metadata": {
        "id": "q_s9ZgjHOQxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download core Hadoop\n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/."
      ],
      "metadata": {
        "id": "33u0fS_0_Gut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "file_name = os.path.basename(HADOOP_URL)\n",
        "if os.path.isfile(file_name):\n",
        "   logger.info(\"{} already exists, not downloading\".format(file_name))\n",
        "else:\n",
        "  logger.info(\"Downloading {}\".format(file_name))\n",
        "  urllib.request.urlretrieve(HADOOP_URL, file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anunz_InIgo3",
        "outputId": "dbb2a9b4-64ed-41a8-8ad5-7dd588b80668"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:18 PM - INFO: hadoop-3.3.4.tar.gz already exists, not downloading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uncompress archive"
      ],
      "metadata": {
        "id": "u6o4kQjNqyv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "dir_name = file_name[:-7]\n",
        "if os.path.exists(dir_name):\n",
        "  logger.info(\"{} is already uncompressed\".format(file_name))\n",
        "else:\n",
        "  logger.info(\"Uncompressing {}\".format(file_name))\n",
        "  tar = tarfile.open(file_name)\n",
        "  tar.extractall()\n",
        "  tar.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UQhLUNpO4tZ",
        "outputId": "9a07cd1e-9ce5-445a-d555-b2d3ededaa12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:18 PM - INFO: hadoop-3.3.4.tar.gz is already uncompressed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Hadoop\n",
        "\n"
      ],
      "metadata": {
        "id": "w3_gaoxYGm6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Environment variables"
      ],
      "metadata": {
        "id": "C2bsvigF1Ghb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HADOOP_HOME'] = os.path.join('/content', dir_name)\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])"
      ],
      "metadata": {
        "id": "K6aui59yGmYd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hadoop configuration files\n",
        "\n",
        "We're going to edit two important Hadoop configuration files:\n",
        "\n",
        "\n",
        "*   `core-site.xml` containing the *site-specific configuration* of the Hadoop core\n",
        "*   `hdfs-site.xml` containing the *site-specific configuration* of Hadoop's HDFS\n",
        "\n",
        "The file `hadoop-env.sh` is a helper file containing environment variables needed by the shell scripts used to start the cluster.\n",
        "\n",
        "All these files can be found in `/etc/hadoop` under `$HADOOP_HOME` (the Hadoop installation folder)."
      ],
      "metadata": {
        "id": "Cz1eDw4X6r_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core_site_file = os.path.join(os.environ['HADOOP_HOME'],'etc/hadoop/core-site.xml')\n",
        "hdfs_site_file = os.path.join(os.environ['HADOOP_HOME'],'etc/hadoop/hdfs-site.xml')\n",
        "hadoop_env_file = os.path.join(os.environ['HADOOP_HOME'],'etc/hadoop/hadoop-env.sh')"
      ],
      "metadata": {
        "id": "2CBsVIlb0MVY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import xml.dom.minidom"
      ],
      "metadata": {
        "id": "SYpaiUbKNgAE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_conf_file(file, propertyname, propertyvalue):\n",
        "  tree = ET.parse(file)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # remove all properties\n",
        "  for el in list(root):\n",
        "      root.remove(el)\n",
        "\n",
        "  logger.info(\"add property {} to {}\".format(propertyname, file))\n",
        "  property = ET.SubElement(root, 'property')\n",
        "  name = ET.SubElement(property, 'name')\n",
        "  name.text = propertyname\n",
        "  value = ET.SubElement(property, 'value')\n",
        "  value.text = propertyvalue\n",
        "  tree.write(file, encoding='utf-8', xml_declaration=True)\n",
        "\n",
        "  # pretty-print\n",
        "  dom = xml.dom.minidom.parse(file) \n",
        "  print(dom.toprettyxml())\n",
        "\n",
        "edit_conf_file(core_site_file, 'fs.defaultFS', 'hdfs://localhost:9000')\n",
        "edit_conf_file(hdfs_site_file, 'dfs.replication', '1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTy2wb3o1ECb",
        "outputId": "b5e36ed7-2d46-42e9-9c78-0d30d684ac99"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:18 PM - INFO: add property fs.defaultFS to /content/hadoop-3.3.4/etc/hadoop/core-site.xml\n",
            "27-Oct-22 09:44:18 PM - INFO: add property dfs.replication to /content/hadoop-3.3.4/etc/hadoop/hdfs-site.xml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" ?>\n",
            "<configuration>\n",
            "\t\n",
            "\n",
            "\t<property>\n",
            "\t\t<name>fs.defaultFS</name>\n",
            "\t\t<value>hdfs://localhost:9000</value>\n",
            "\t</property>\n",
            "</configuration>\n",
            "\n",
            "<?xml version=\"1.0\" ?>\n",
            "<configuration>\n",
            "\t\n",
            "\n",
            "\n",
            "\t<property>\n",
            "\t\t<name>dfs.replication</name>\n",
            "\t\t<value>1</value>\n",
            "\t</property>\n",
            "</configuration>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = ['HDFS_NAMENODE_USER', \\\n",
        "         'HDFS_DATANODE_USER', \\\n",
        "         'HDFS_SECONDARYNAMENODE_USER', \\\n",
        "         'YARN_RESOURCEMANAGER_USER', \\\n",
        "         'YARN_NODEMANAGER_USER']\n",
        "\n",
        "logger.info(\"Editing {}\".format(hadoop_env_file))\n",
        "\n",
        "with open(hadoop_env_file, \"r+\") as f:\n",
        "  for u in users:\n",
        "    var = 'export ' + u + '=root'\n",
        "    line_found = any(line.startswith(var) for line in f)\n",
        "    if not line_found:\n",
        "      f.seek(0, os.SEEK_END)\n",
        "      f.write(var +\"\\n\")\n",
        "  line_found = any(line.startswith('export JAVA_HOME=') for line in f)\n",
        "  if not line_found:\n",
        "      f.seek(0, os.SEEK_END)\n",
        "      f.write('export JAVA_HOME='+JAVA_PATH+\"\\n\")"
      ],
      "metadata": {
        "id": "37DUbYhJ2b-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4293fa-484e-423f-df60-7a55e8d5d704"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:18 PM - INFO: Editing /content/hadoop-3.3.4/etc/hadoop/hadoop-env.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A note on Hadoop's default configuration files\n",
        "\n",
        "Hadoop's default configuration files are:\n",
        "\n",
        "*  `core-default.xml`\n",
        "*  `hdfs-default.xml`\n",
        "*  `yarn-default.xml` \n",
        "*  `mapred-default.xml`\n",
        "\n",
        "They are read-only files that contain all default values for Hadoop properties (see [cluster setup](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Hadoop_in_Non-Secure_Mode)) and can be viewed at:\n"
      ],
      "metadata": {
        "id": "UoZhdlmOeXna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "list(glob.iglob(os.environ['HADOOP_HOME']+ '/**/*default.xml', recursive=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFbwYpOdgGQg",
        "outputId": "2308b8c8-ebd0-45d5-93f6-19769ff854f9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/hadoop-3.3.4/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml',\n",
              " '/content/hadoop-3.3.4/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml',\n",
              " '/content/hadoop-3.3.4/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml',\n",
              " '/content/hadoop-3.3.4/share/doc/hadoop/hadoop-yarn/hadoop-yarn-common/yarn-default.xml',\n",
              " '/content/hadoop-3.3.4/share/doc/hadoop/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By providing site-specific configuration files one can override any of the default properties.\n",
        "\n",
        "In our case we set `fs.defaultFS` (the URI of the default file system) to `hdfs://localhost:9000` and changed `dfs.replication` to $1$ (the default is $3$) in `hdfs-site.xml`.\n"
      ],
      "metadata": {
        "id": "6qmAmyeQlLQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "DK1-yry6BKI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the namenode"
      ],
      "metadata": {
        "id": "dLo0tEVpBbsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Formatting NameNode\")\n",
        "cmd = [\"hdfs\", \"namenode\", \"-format\", \"-nonInteractive\"]\n",
        "result = subprocess.call(cmd, stderr=subprocess.STDOUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_mDgPqTCi8D",
        "outputId": "c06c231e-f803-40a6-981a-28d0a0c730d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27-Oct-22 09:44:18 PM - INFO: Formatting NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Hadoop"
      ],
      "metadata": {
        "id": "bGYNMicCLyBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws9hy0B7Cj69",
        "outputId": "adeb62f6-3628-4be9-8b57-401cea76a3ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "localhost: namenode is running as process 1050.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "Starting datanodes\n",
            "localhost: datanode is running as process 1173.  Stop it first and ensure /tmp/hadoop-root-datanode.pid file is empty before retry.\n",
            "Starting secondary namenodes [3b8744d84cec]\n",
            "3b8744d84cec: secondarynamenode is running as process 1400.  Stop it first and ensure /tmp/hadoop-root-secondarynamenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check namenode"
      ],
      "metadata": {
        "id": "zamj-W64wbVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S14iqwbWaZu2",
        "outputId": "2810aaaf-e927-4f12-c98d-45952f4f4afc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 89248182272 (83.12 GB)\n",
            "DFS Remaining: 89248153600 (83.12 GB)\n",
            "DFS Used: 28672 (28 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: 3b8744d84cec\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 28672 (28 KB)\n",
            "Non DFS Used: 26393231360 (24.58 GB)\n",
            "DFS Remaining: 89248153600 (83.12 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 77.17%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 0\n",
            "Last contact: Thu Oct 27 21:47:09 UTC 2022\n",
            "Last Block Report: Thu Oct 27 21:40:00 UTC 2022\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYakF2Muw69p",
        "outputId": "14c193a8-8116-4dd0-f0f7-fbc8fbce2087"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 0.839s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lt --port 9870"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDT7og8wxiYA",
        "outputId": "f7180cb5-aaba-4aa4-eea0-1f591e971dc7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://shaggy-experts-wait-34-135-241-210.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}