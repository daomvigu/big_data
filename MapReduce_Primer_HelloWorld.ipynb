{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53",
        "tags": []
      },
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using `IdentityMapper` `IdentityReducer`, thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$."
      ],
      "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo12bOJ45USo"
      },
      "source": [
        "# Download core Hadoop\n",
        "\n"
      ],
      "id": "Uo12bOJ45USo"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:44:05.817858Z",
          "iopub.status.busy": "2024-03-09T18:44:05.817168Z",
          "iopub.status.idle": "2024-03-09T18:45:40.384190Z",
          "shell.execute_reply": "2024-03-09T18:45:40.384659Z"
        },
        "id": "5lGm2Lw-5h7c",
        "outputId": "2f782125-0e95-4973-b791-0d9a8cb07e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.3.6\n"
          ]
        }
      ],
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.6.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "id": "5lGm2Lw-5h7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKn5-bIX-yXW"
      },
      "source": [
        "# Set environment variables"
      ],
      "id": "zKn5-bIX-yXW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zRCjJUsFlQ8"
      },
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "id": "2zRCjJUsFlQ8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:40.391380Z",
          "iopub.status.busy": "2024-03-09T18:45:40.390559Z",
          "iopub.status.idle": "2024-03-09T18:45:40.393166Z",
          "shell.execute_reply": "2024-03-09T18:45:40.393628Z"
        },
        "id": "ZAvCdRKI-3dr",
        "outputId": "601aeafb-95d1-41f6-9836-5ed6123e2a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.3.6\n",
            "PATH is hadoop-3.3.6/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "id": "ZAvCdRKI-3dr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnlmws4mAwbO"
      },
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "id": "Qnlmws4mAwbO"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:40.401411Z",
          "iopub.status.busy": "2024-03-09T18:45:40.400490Z",
          "iopub.status.idle": "2024-03-09T18:45:40.404608Z",
          "shell.execute_reply": "2024-03-09T18:45:40.405110Z"
        },
        "id": "GTOc3dH5A5uG",
        "outputId": "e7ccc405-ab02-4430-d887-13dd56247f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "id": "GTOc3dH5A5uG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1eZ9qrpFsAd"
      },
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "id": "a1eZ9qrpFsAd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2"
      },
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:40.410172Z",
          "iopub.status.busy": "2024-03-09T18:45:40.409254Z",
          "iopub.status.idle": "2024-03-09T18:45:40.527685Z",
          "shell.execute_reply": "2024-03-09T18:45:40.528603Z"
        },
        "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56241317-96b3-40c8-b377-688df5dc0eeb"
      },
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application"
      ],
      "id": "56241317-96b3-40c8-b377-688df5dc0eeb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d40f86c-9287-442b-ba6b-b42489700910"
      },
      "source": [
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders bt design."
      ],
      "id": "1d40f86c-9287-442b-ba6b-b42489700910"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:40.538711Z",
          "iopub.status.busy": "2024-03-09T18:45:40.537504Z",
          "iopub.status.idle": "2024-03-09T18:45:45.599153Z",
          "shell.execute_reply": "2024-03-09T18:45:45.599771Z"
        },
        "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a",
        "outputId": "0424534e-6975-4542-e158-5367f479e4ff",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-03-09 19:25:25,923 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 19:25:26,187 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 19:25:26,187 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 19:25:26,217 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:26,555 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 19:25:26,597 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 19:25:27,032 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local451480222_0001\n",
            "2024-03-09 19:25:27,033 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 19:25:27,317 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 19:25:27,319 INFO mapreduce.Job: Running job: job_local451480222_0001\n",
            "2024-03-09 19:25:27,332 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 19:25:27,336 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 19:25:27,351 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:27,351 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:27,431 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 19:25:27,438 INFO mapred.LocalJobRunner: Starting task: attempt_local451480222_0001_m_000000_0\n",
            "2024-03-09 19:25:27,484 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:27,487 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:27,523 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:27,535 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:27,551 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 19:25:27,638 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 19:25:27,639 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 19:25:27,639 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 19:25:27,639 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 19:25:27,639 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 19:25:27,642 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 19:25:27,645 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 19:25:27,664 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 19:25:27,667 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 19:25:27,668 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 19:25:27,668 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 19:25:27,669 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 19:25:27,669 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 19:25:27,671 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 19:25:27,672 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 19:25:27,672 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 19:25:27,673 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 19:25:27,674 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 19:25:27,675 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 19:25:27,698 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 19:25:27,699 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 19:25:27,701 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 19:25:27,701 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 19:25:27,704 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:27,704 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 19:25:27,704 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 19:25:27,704 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-03-09 19:25:27,704 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 19:25:27,714 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 19:25:27,738 INFO mapred.Task: Task:attempt_local451480222_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:27,741 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 19:25:27,741 INFO mapred.Task: Task 'attempt_local451480222_0001_m_000000_0' done.\n",
            "2024-03-09 19:25:27,751 INFO mapred.Task: Final Counters for attempt_local451480222_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=778415\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=402653184\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 19:25:27,751 INFO mapred.LocalJobRunner: Finishing task: attempt_local451480222_0001_m_000000_0\n",
            "2024-03-09 19:25:27,752 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 19:25:27,756 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 19:25:27,757 INFO mapred.LocalJobRunner: Starting task: attempt_local451480222_0001_r_000000_0\n",
            "2024-03-09 19:25:27,769 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:27,769 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:27,770 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:27,776 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6e637f01\n",
            "2024-03-09 19:25:27,779 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:27,803 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 19:25:27,816 INFO reduce.EventFetcher: attempt_local451480222_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 19:25:27,853 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local451480222_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-03-09 19:25:27,859 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local451480222_0001_m_000000_0\n",
            "2024-03-09 19:25:27,865 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-03-09 19:25:27,869 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 19:25:27,870 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:27,870 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 19:25:27,882 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 19:25:27,883 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 19:25:27,885 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 19:25:27,885 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-03-09 19:25:27,886 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 19:25:27,886 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 19:25:27,887 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 19:25:27,888 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:27,897 INFO mapred.Task: Task:attempt_local451480222_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:27,898 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:27,898 INFO mapred.Task: Task attempt_local451480222_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 19:25:27,900 INFO output.FileOutputCommitter: Saved output of task 'attempt_local451480222_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 19:25:27,901 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 19:25:27,904 INFO mapred.Task: Task 'attempt_local451480222_0001_r_000000_0' done.\n",
            "2024-03-09 19:25:27,905 INFO mapred.Task: Final Counters for attempt_local451480222_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141488\n",
            "\t\tFILE: Number of bytes written=778465\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=402653184\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 19:25:27,906 INFO mapred.LocalJobRunner: Finishing task: attempt_local451480222_0001_r_000000_0\n",
            "2024-03-09 19:25:27,906 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 19:25:28,331 INFO mapreduce.Job: Job job_local451480222_0001 running in uber mode : false\n",
            "2024-03-09 19:25:28,332 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 19:25:28,334 INFO mapreduce.Job: Job job_local451480222_0001 completed successfully\n",
            "2024-03-09 19:25:28,345 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282898\n",
            "\t\tFILE: Number of bytes written=1556880\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=805306368\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 19:25:28,345 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ebec78b-fc7a-483d-9408-ed507ac81180"
      },
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "id": "8ebec78b-fc7a-483d-9408-ed507ac81180"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:45.608934Z",
          "iopub.status.busy": "2024-03-09T18:45:45.608125Z",
          "iopub.status.idle": "2024-03-09T18:45:46.648253Z",
          "shell.execute_reply": "2024-03-09T18:45:46.648937Z"
        },
        "id": "37ca3290-a8d9-44f0-9970-3809fce1aace",
        "outputId": "a3812599-8f62-4409-b650-e9b47561e6fe",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "id": "37ca3290-a8d9-44f0-9970-3809fce1aace"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W1AYGwPMQZn"
      },
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "id": "4W1AYGwPMQZn"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:46.654646Z",
          "iopub.status.busy": "2024-03-09T18:45:46.653775Z",
          "iopub.status.idle": "2024-03-09T18:45:47.968839Z",
          "shell.execute_reply": "2024-03-09T18:45:47.969574Z"
        },
        "id": "E81rOKlPMB7U",
        "outputId": "9932e0d9-262e-4d4b-d345-917b9bb043f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-03-09 19:25 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-03-09 19:25 my_output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "id": "E81rOKlPMB7U"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:47.975396Z",
          "iopub.status.busy": "2024-03-09T18:45:47.974474Z",
          "iopub.status.idle": "2024-03-09T18:45:48.106239Z",
          "shell.execute_reply": "2024-03-09T18:45:48.107369Z"
        },
        "id": "PK4tGI1CMdHS",
        "outputId": "e2c615aa-17f6-4551-9b6d-91af6ed019d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Mar  9 19:25 part-00000\n",
            "-rw-r--r-- 1 root root  0 Mar  9 19:25 _SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!ls -l my_output"
      ],
      "id": "PK4tGI1CMdHS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5gEVXjD1Wca"
      },
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "id": "Q5gEVXjD1Wca"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:48.115203Z",
          "iopub.status.busy": "2024-03-09T18:45:48.114140Z",
          "iopub.status.idle": "2024-03-09T18:45:48.241439Z",
          "shell.execute_reply": "2024-03-09T18:45:48.242230Z"
        },
        "id": "Zb0vmdyMOleV",
        "outputId": "c941343c-e044-42f4-cf70-fd6f6c26447d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ],
      "source": [
        "!cat my_output/part-00000"
      ],
      "id": "Zb0vmdyMOleV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHWsKFuHOrUh"
      },
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "id": "XHWsKFuHOrUh"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:48.249945Z",
          "iopub.status.busy": "2024-03-09T18:45:48.248871Z",
          "iopub.status.idle": "2024-03-09T18:45:49.036531Z",
          "shell.execute_reply": "2024-03-09T18:45:49.037164Z"
        },
        "id": "Geo7HmAq2Gov",
        "outputId": "65559a8f-0ac0-49fd-e7a8-db76313e58c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-09 19:25:33,805 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming -h"
      ],
      "id": "Geo7HmAq2Gov"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:49.044919Z",
          "iopub.status.busy": "2024-03-09T18:45:49.041718Z",
          "iopub.status.idle": "2024-03-09T18:45:52.639921Z",
          "shell.execute_reply": "2024-03-09T18:45:52.639204Z"
        },
        "id": "dR3Vo2YG5lw_",
        "outputId": "6e2e4a90-147f-4079-e5a3-f18e86fb5f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 19:25:35,879 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 19:25:39,177 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 19:25:39,347 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 19:25:39,347 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 19:25:39,389 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:39,668 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 19:25:39,702 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 19:25:39,986 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local342617641_0001\n",
            "2024-03-09 19:25:39,986 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 19:25:40,240 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 19:25:40,242 INFO mapreduce.Job: Running job: job_local342617641_0001\n",
            "2024-03-09 19:25:40,252 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 19:25:40,255 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 19:25:40,266 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:40,270 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:40,381 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 19:25:40,388 INFO mapred.LocalJobRunner: Starting task: attempt_local342617641_0001_m_000000_0\n",
            "2024-03-09 19:25:40,424 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:40,428 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:40,471 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:40,487 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:40,506 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 19:25:40,581 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 19:25:40,581 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 19:25:40,581 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 19:25:40,581 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 19:25:40,581 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 19:25:40,585 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 19:25:40,593 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:40,593 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 19:25:40,593 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 19:25:40,593 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-03-09 19:25:40,593 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 19:25:40,600 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 19:25:40,620 INFO mapred.Task: Task:attempt_local342617641_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:40,622 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:40,623 INFO mapred.Task: Task 'attempt_local342617641_0001_m_000000_0' done.\n",
            "2024-03-09 19:25:40,632 INFO mapred.Task: Final Counters for attempt_local342617641_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=776323\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=397410304\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 19:25:40,633 INFO mapred.LocalJobRunner: Finishing task: attempt_local342617641_0001_m_000000_0\n",
            "2024-03-09 19:25:40,636 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 19:25:40,642 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 19:25:40,644 INFO mapred.LocalJobRunner: Starting task: attempt_local342617641_0001_r_000000_0\n",
            "2024-03-09 19:25:40,655 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:40,655 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:40,656 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:40,660 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ce46bbc\n",
            "2024-03-09 19:25:40,662 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:40,685 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 19:25:40,706 INFO reduce.EventFetcher: attempt_local342617641_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 19:25:40,762 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local342617641_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-03-09 19:25:40,767 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local342617641_0001_m_000000_0\n",
            "2024-03-09 19:25:40,773 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-03-09 19:25:40,778 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 19:25:40,780 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:40,780 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 19:25:40,788 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 19:25:40,788 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 19:25:40,790 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 19:25:40,791 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-03-09 19:25:40,791 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 19:25:40,791 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 19:25:40,792 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 19:25:40,793 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:40,803 INFO mapred.Task: Task:attempt_local342617641_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:40,805 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 19:25:40,806 INFO mapred.Task: Task attempt_local342617641_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 19:25:40,809 INFO output.FileOutputCommitter: Saved output of task 'attempt_local342617641_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 19:25:40,811 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 19:25:40,812 INFO mapred.Task: Task 'attempt_local342617641_0001_r_000000_0' done.\n",
            "2024-03-09 19:25:40,812 INFO mapred.Task: Final Counters for attempt_local342617641_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141502\n",
            "\t\tFILE: Number of bytes written=776381\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=397410304\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 19:25:40,812 INFO mapred.LocalJobRunner: Finishing task: attempt_local342617641_0001_r_000000_0\n",
            "2024-03-09 19:25:40,813 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 19:25:41,248 INFO mapreduce.Job: Job job_local342617641_0001 running in uber mode : false\n",
            "2024-03-09 19:25:41,249 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 19:25:41,251 INFO mapreduce.Job: Job job_local342617641_0001 completed successfully\n",
            "2024-03-09 19:25:41,262 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282912\n",
            "\t\tFILE: Number of bytes written=1552704\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=794820608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 19:25:41,262 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "id": "dR3Vo2YG5lw_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEY6DxF8QRak"
      },
      "source": [
        "## Verify the result"
      ],
      "id": "rEY6DxF8QRak"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:52.645544Z",
          "iopub.status.busy": "2024-03-09T18:45:52.644438Z",
          "iopub.status.idle": "2024-03-09T18:45:53.595506Z",
          "shell.execute_reply": "2024-03-09T18:45:53.596134Z"
        },
        "id": "y7DvyWpZz0kO",
        "outputId": "a14c14ca-dc6a-4e6c-ea24-701a05f027f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "id": "y7DvyWpZz0kO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7dRikhCQhsY"
      },
      "source": [
        "Show output"
      ],
      "id": "u7dRikhCQhsY"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:53.602769Z",
          "iopub.status.busy": "2024-03-09T18:45:53.601919Z",
          "iopub.status.idle": "2024-03-09T18:45:53.720210Z",
          "shell.execute_reply": "2024-03-09T18:45:53.721651Z"
        },
        "id": "fAoEeKpyQlAG",
        "outputId": "5d9a7fdf-5a6c-4fed-ad9e-31cee47b5ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ],
      "source": [
        "!cat my_output/part-00000"
      ],
      "id": "fAoEeKpyQlAG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5s_wfkvQowG"
      },
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "id": "U5s_wfkvQowG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48l28qe4SlSp"
      },
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "id": "48l28qe4SlSp"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:53.731540Z",
          "iopub.status.busy": "2024-03-09T18:45:53.730393Z",
          "iopub.status.idle": "2024-03-09T18:45:57.777979Z",
          "shell.execute_reply": "2024-03-09T18:45:57.778352Z"
        },
        "id": "Bs1DQ6OISoO6",
        "outputId": "abd74baf-cf73-4a76-810b-712bf95b5620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 19:25:45,234 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 19:25:47,279 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 19:25:47,475 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 19:25:47,475 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 19:25:47,523 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:47,799 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 19:25:47,836 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 19:25:48,169 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1258585303_0001\n",
            "2024-03-09 19:25:48,169 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 19:25:48,434 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 19:25:48,435 INFO mapreduce.Job: Running job: job_local1258585303_0001\n",
            "2024-03-09 19:25:48,445 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 19:25:48,448 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 19:25:48,454 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:48,454 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:48,585 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 19:25:48,594 INFO mapred.LocalJobRunner: Starting task: attempt_local1258585303_0001_m_000000_0\n",
            "2024-03-09 19:25:48,638 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:48,642 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:48,696 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:48,717 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:48,751 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 19:25:48,827 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:48,852 INFO mapred.Task: Task:attempt_local1258585303_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:48,856 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:48,857 INFO mapred.Task: Task attempt_local1258585303_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 19:25:48,869 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1258585303_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 19:25:48,879 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:48,879 INFO mapred.Task: Task 'attempt_local1258585303_0001_m_000000_0' done.\n",
            "2024-03-09 19:25:48,903 INFO mapred.Task: Final Counters for attempt_local1258585303_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=390070272\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 19:25:48,903 INFO mapred.LocalJobRunner: Finishing task: attempt_local1258585303_0001_m_000000_0\n",
            "2024-03-09 19:25:48,905 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 19:25:49,443 INFO mapreduce.Job: Job job_local1258585303_0001 running in uber mode : false\n",
            "2024-03-09 19:25:49,446 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 19:25:49,449 INFO mapreduce.Job: Job job_local1258585303_0001 completed successfully\n",
            "2024-03-09 19:25:49,479 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=390070272\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 19:25:49,481 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "id": "Bs1DQ6OISoO6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8B4g65TU0vn"
      },
      "source": [
        "## Verify the result"
      ],
      "id": "s8B4g65TU0vn"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:57.782425Z",
          "iopub.status.busy": "2024-03-09T18:45:57.781839Z",
          "iopub.status.idle": "2024-03-09T18:45:58.841347Z",
          "shell.execute_reply": "2024-03-09T18:45:58.842571Z"
        },
        "id": "kTC44BEmSuw8",
        "outputId": "b0e0a8e4-bd63-4a4e-abd0-b8d2b0f7ceab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "id": "kTC44BEmSuw8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9CzGcNkVBzJ"
      },
      "source": [
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted."
      ],
      "id": "h9CzGcNkVBzJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCacnSLX8KT"
      },
      "source": [],
      "id": "2qCacnSLX8KT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvoil63X8nQ"
      },
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application"
      ],
      "id": "afvoil63X8nQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29yfN39VYSuh"
      },
      "source": [
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "id": "29yfN39VYSuh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:45:58.855488Z",
          "iopub.status.busy": "2024-03-09T18:45:58.851261Z",
          "iopub.status.idle": "2024-03-09T18:46:02.404386Z",
          "shell.execute_reply": "2024-03-09T18:46:02.405349Z"
        },
        "id": "3pVHowpwYSuh",
        "outputId": "0e991845-ec32-4da7-d142-f246c193e7e2",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 19:25:54,076 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 19:25:56,212 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 19:25:56,427 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 19:25:56,427 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 19:25:56,465 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 19:25:56,762 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 19:25:56,797 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 19:25:57,122 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local563996181_0001\n",
            "2024-03-09 19:25:57,122 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 19:25:57,409 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 19:25:57,411 INFO mapreduce.Job: Running job: job_local563996181_0001\n",
            "2024-03-09 19:25:57,421 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 19:25:57,423 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 19:25:57,432 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:57,432 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:57,499 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 19:25:57,512 INFO mapred.LocalJobRunner: Starting task: attempt_local563996181_0001_m_000000_0\n",
            "2024-03-09 19:25:57,556 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 19:25:57,557 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 19:25:57,589 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 19:25:57,602 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 19:25:57,622 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 19:25:57,637 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 19:25:57,644 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 19:25:57,645 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 19:25:57,645 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 19:25:57,646 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 19:25:57,646 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 19:25:57,647 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 19:25:57,648 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 19:25:57,648 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 19:25:57,648 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 19:25:57,649 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 19:25:57,649 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 19:25:57,650 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 19:25:57,671 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 19:25:57,675 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 19:25:57,681 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 19:25:57,681 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 19:25:57,685 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:57,696 INFO mapred.Task: Task:attempt_local563996181_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 19:25:57,698 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 19:25:57,698 INFO mapred.Task: Task attempt_local563996181_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 19:25:57,702 INFO output.FileOutputCommitter: Saved output of task 'attempt_local563996181_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 19:25:57,703 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 19:25:57,703 INFO mapred.Task: Task 'attempt_local563996181_0001_m_000000_0' done.\n",
            "2024-03-09 19:25:57,712 INFO mapred.Task: Final Counters for attempt_local563996181_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=387973120\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 19:25:57,712 INFO mapred.LocalJobRunner: Finishing task: attempt_local563996181_0001_m_000000_0\n",
            "2024-03-09 19:25:57,713 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 19:25:58,418 INFO mapreduce.Job: Job job_local563996181_0001 running in uber mode : false\n",
            "2024-03-09 19:25:58,421 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 19:25:58,424 INFO mapreduce.Job: Job job_local563996181_0001 completed successfully\n",
            "2024-03-09 19:25:58,433 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=387973120\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 19:25:58,433 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "id": "3pVHowpwYSuh"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-03-09T18:46:02.414191Z",
          "iopub.status.busy": "2024-03-09T18:46:02.412973Z",
          "iopub.status.idle": "2024-03-09T18:46:03.475567Z",
          "shell.execute_reply": "2024-03-09T18:46:03.476047Z"
        },
        "id": "G0V6lcScZQG5",
        "outputId": "251ede44-c7ed-4945-95bd-7ca720103779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "id": "G0V6lcScZQG5"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}