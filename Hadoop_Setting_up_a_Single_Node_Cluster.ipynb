{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oUuQjW2oNMcJ",
        "qFfOrktMPq8M",
        "KE7kSYSXQYLf",
        "lGI4TNXPamMr",
        "RlgP1ytnRtUK",
        "KLmxLQeJSb4A",
        "kXbSKFyeMqr2",
        "k2-Fdp73cF0V",
        "xMrEiLB_VAeR",
        "CKRRbwDFv3ZQ",
        "G3KBe4R65bl1",
        "yVJA-3jSATGV",
        "BbosNo0TD3oH",
        "Sam22f-YT1xR",
        "IF6-Z5RotAcO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPPzMhHOftwCZ5nXEoJqaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Hadoop_Setting_up_a_Single_Node_Cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "We're going to set up a single-node cluster (following the instructions on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html) and show how to run simple HDFS and MapReduce commands.\n",
        "\n",
        "After downloading the software, it is necessary to carry out some preliminary steps like setting environment variables, generating SSH keys, etc.). We grouped all these steps under \"Prologue\".\n",
        "\n",
        "Once done with the prologue, we are able to start a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "We are going to run some test HDFS commands and MapReduce jobs on the Hadoop cluster.\n",
        "\n",
        "Finally, the cluster will be shut down.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the available Java version\n",
        " Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only). See: https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "ae0df957-2708-460d-ad20-2cc6ae8bd5ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.18\" 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 8 || $JAVA_MAJOR_VERSION -eq 11 ]]\n",
        " then \n",
        " echo \"Java version is one of 8, 11 ✓\" \n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "80a42b4b-267f-42ab-a0d6-fb51c4f8ac31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 8, 11 ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the variable for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "pWROofISgKKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "af51c500-6acd-4154-b572-38fd917f2bb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract JAVA_HOME from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "aebfbcd7-005d-4682-c8a3-e126823c0a41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use `JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64`.\n",
        "\n",
        "Use `%env%` to set the variable for the current notebook session."
      ],
      "metadata": {
        "id": "Hck9zJ3kQK2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64"
      ],
      "metadata": {
        "id": "P88xcreEQBcx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download core Hadoop \n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
        "\n",
        "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.5.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LqS5Rkgyli",
        "outputId": "d90faaef-4d0f-4e4c-a365-283cc35dcb72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘hadoop-3.3.5.tar.gz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncompress archive"
      ],
      "metadata": {
        "id": "um2CARkgg22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzf hadoop-3.3.5.tar.gz "
      ],
      "metadata": {
        "id": "C17WYI0mQRE8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file \n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download sha512 file"
      ],
      "metadata": {
        "id": "ATofMJRXhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.5.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "ba3d997d-c1b9-4219-8966-0af7d89a908d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘hadoop-3.3.5.tar.gz.sha512’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare"
      ],
      "metadata": {
        "id": "eL8FxjalhFAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "A=$(sha512sum hadoop-3.3.5.tar.gz | cut - -d' ' -f1)\n",
        "B=$(cut hadoop-3.3.5.tar.gz.sha512 -d' ' -f4)\n",
        "printf \"%s\\n%s\\n\" $A $B\n",
        "[[ $A == $B ]] && echo \"True\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL302M1OhFMH",
        "outputId": "d08cd7b8-ea1c-4634-eb3b-bfdd4dd40cb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cc170df24976543a3e961a1353a194225e3ffc5d53d594dd63d71422765e1d816d1ffa877c02bf93f0183fcfbe4c10f4b7739deca69420ed83372a0b1f9d5dc7\n",
            "cc170df24976543a3e961a1353a194225e3ffc5d53d594dd63d71422765e1d816d1ffa877c02bf93f0183fcfbe4c10f4b7739deca69420ed83372a0b1f9d5dc7\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "cd46b5ea-d17b-4ade-fd5f-44f5a2a60028"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ['HADOOP_HOME'] = os.path.join(os.environ['HOME'], 'hadoop-3.3.5')\n",
        "os.environ['HADOOP_HOME'] = os.path.join('/content', 'hadoop-3.3.5')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "7a11cb62-74f4-45f0-fde7-729e5d385147"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '11.11.3.6-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'COLAB_JUPYTER_TRANSPORT': 'ipc', 'NV_NVML_DEV_VERSION': '11.8.86-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.16.2-1+cuda11.8', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.16.2-1', 'VM_GCE_METADATA_HOST': '169.254.169.253', 'HOSTNAME': '3e5ebbbceaa4', 'TBE_RUNTIME_ADDR': '172.28.0.1:8011', 'GCE_METADATA_TIMEOUT': '3', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.8 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-11-8=11.11.3.6-1', 'NV_NVTX_VERSION': '11.8.86-1', 'COLAB_JUPYTER_IP': '172.28.0.12', 'NV_CUDA_CUDART_DEV_VERSION': '11.8.89-1', 'NV_LIBCUSPARSE_VERSION': '11.7.5.86-1', 'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/', 'NV_LIBNPP_VERSION': '11.8.0.86-1', 'NCCL_VERSION': '2.16.2-1', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'ENV': '/root/.bashrc', 'PWD': '/', 'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s', 'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009', 'TBE_CREDS_ADDR': '172.28.0.1:8008', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.7.0.84-1+cuda11.8', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'COLAB_JUPYTER_TOKEN': '', 'LAST_FORCED_REBUILD': '20230428', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-11-8=11.8.87-1', 'NV_LIBNPP_PACKAGE': 'libnpp-11-8=11.8.0.86-1', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', 'NV_LIBCUBLAS_DEV_VERSION': '11.11.3.6-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-11-8', 'NV_CUDA_CUDART_VERSION': '11.8.89-1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '11.8.0', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-11-8=11.11.3.6-1', 'COLAB_RELEASE_TAG': 'release-colab-20230511-085804-RC00', 'KMP_TARGET_PORT': '9000', 'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-2s7cmphk4ebie --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-11-8=11.8.0.86-1', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-11-8', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '11.8.0.86-1', 'NO_GCE_CHECK': 'False', 'PYTHONPATH': '/env/python', 'NV_LIBCUSPARSE_DEV_VERSION': '11.7.5.86-1', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '8.7.0.84', 'SHLVL': '0', 'NV_CUDA_LIB_VERSION': '11.8.0-1', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NVARCH': 'x86_64', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn8-dev=8.7.0.84-1+cuda11.8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-8', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.16.2-1+cuda11.8', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'COLAB_GPU': '', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'NV_NVPROF_VERSION': '11.8.87-1', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'PATH': '/content/hadoop-3.3.5/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.16.2-1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'DEBIAN_FRONTEND': 'noninteractive', 'COLAB_BACKEND_VERSION': 'next', 'OLDPWD': '/', 'JPY_PARENT_PID': '74', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'ENABLE_DIRECTORYPREFETCHER': '1', 'USE_AUTH_EPHEM': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'HADOOP_HOME': '/content/hadoop-3.3.5', 'JAVA_HOME': '/usr/lib/jvm/java-11-openjdk-amd64'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "619883e4-1672-478c-c7f4-6ec2b1d859f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.3.5/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.3.5/etc/hadoop/core-site.xml\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.3.5/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat hadoop-3.3.5/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "1c0cadc2-59c1-4e15-db8f-600f976d8fb7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.3.5/sbin`).\n",
        "```\n",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"export HDFS_NAMENODE_USER=root \\n\\\n",
        "export HDFS_DATANODE_USER=root \\n\\\n",
        "export HDFS_SECONDARYNAMENODE_USER=root \\n\\\n",
        "export YARN_RESOURCEMANAGER_USER=root \\n\\\n",
        "export YARN_NODEMANAGER_USER=root\" >> hadoop-3.3.5/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless access to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Install `openssh` and start server\n",
        "\n",
        "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
        "\n",
        "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` seems the most elegant and I'm going to try it out some day.\n"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get install openssh-server\n",
        "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config\n",
        "/etc/init.d/ssh restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "65572e66-6df6-4dbc-d6fc-2c5ba59a6212"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  ncurses-term openssh-sftp-server python3-distro ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere ssh-askpass ufw\n",
            "The following NEW packages will be installed:\n",
            "  ncurses-term openssh-server openssh-sftp-server python3-distro ssh-import-id\n",
            "0 upgraded, 5 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 703 kB of archives.\n",
            "After this operation, 6,081 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 ncurses-term all 6.2-0ubuntu2 [249 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.7 [51.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.7 [377 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-distro all 1.4.0-1 [14.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]\n",
            "Preconfiguring packages ...\n",
            "Fetched 703 kB in 0s (5,700 kB/s)\n",
            "Selecting previously unselected package ncurses-term.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 122519 files and directories currently installed.)\r\n",
            "Preparing to unpack .../ncurses-term_6.2-0ubuntu2_all.deb ...\r\n",
            "Unpacking ncurses-term (6.2-0ubuntu2) ...\r\n",
            "Selecting previously unselected package openssh-sftp-server.\r\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.2p1-4ubuntu0.7_amd64.deb ...\r\n",
            "Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.7) ...\r\n",
            "Selecting previously unselected package openssh-server.\r\n",
            "Preparing to unpack .../openssh-server_1%3a8.2p1-4ubuntu0.7_amd64.deb ...\r\n",
            "Unpacking openssh-server (1:8.2p1-4ubuntu0.7) ...\r\n",
            "Selecting previously unselected package python3-distro.\r\n",
            "Preparing to unpack .../python3-distro_1.4.0-1_all.deb ...\r\n",
            "Unpacking python3-distro (1.4.0-1) ...\r\n",
            "Selecting previously unselected package ssh-import-id.\r\n",
            "Preparing to unpack .../ssh-import-id_5.10-0ubuntu1_all.deb ...\r\n",
            "Unpacking ssh-import-id (5.10-0ubuntu1) ...\r\n",
            "Setting up openssh-sftp-server (1:8.2p1-4ubuntu0.7) ...\r\n",
            "Setting up python3-distro (1.4.0-1) ...\r\n",
            "Setting up openssh-server (1:8.2p1-4ubuntu0.7) ...\r\n",
            "\r\n",
            "Creating config file /etc/ssh/sshd_config with new version\r\n",
            "Creating SSH2 RSA key; this may take some time ...\r\n",
            "3072 SHA256:JlwrEvzhfAL4BFr1FDVdWst332b+tfxw2TPJcnA2I5M root@3e5ebbbceaa4 (RSA)\r\n",
            "Creating SSH2 ECDSA key; this may take some time ...\r\n",
            "256 SHA256:ppyCoshdgSXisSB5N1qHmTzePO5QQs2DfvmM9sYNaNY root@3e5ebbbceaa4 (ECDSA)\r\n",
            "Creating SSH2 ED25519 key; this may take some time ...\r\n",
            "256 SHA256:EZgDMYWKjtL61C6ptRyWBHQERGXwuGHchnoQ9+sZEiE root@3e5ebbbceaa4 (ED25519)\r\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\r\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Setting up ssh-import-id (5.10-0ubuntu1) ...\r\n",
            "Attempting to convert /etc/ssh/ssh_import_id\r\n",
            "Setting up ncurses-term (6.2-0ubuntu2) ...\r\n",
            "Processing triggers for man-db (2.9.1-1) ...\r\n",
            "Processing triggers for systemd (245.4-4ubuntu3.21) ...\r\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate key\n",
        "Generate SSH key that does not require a password. \n",
        "\n",
        "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
        "\n",
        "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm /root/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "3d3eec04-7ccc-4347-f746-287b5069633d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:4aCvuG/To1nAsunGEohHkgLfOYi5+bQJ5cw9ADm1lCs root@3e5ebbbceaa4\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "| oo.             |\n",
            "|=...             |\n",
            "|.B.+ .. .        |\n",
            "|E B.+. o .       |\n",
            "|+@.o+.  S        |\n",
            "|B *+oo           |\n",
            "| Boo oo          |\n",
            "|..*.o+o          |\n",
            "| o++=o .         |\n",
            "+----[SHA256]-----+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "ec4cd104-2ae3-442a-ab9e-17eb98acead1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
            "hi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "8dd7f375-612b-44cd-ed12-b8ae5f5e23a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /content/hadoop-3.3.5/logs does not exist. Creating.\n",
            "2023-05-14 10:15:21,972 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 3e5ebbbceaa4/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -nonInteractive]\n",
            "STARTUP_MSG:   version = 3.3.5\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.3.5/etc/hadoop:/content/hadoop-3.3.5/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/curator-framework-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jersey-json-1.20.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-http-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/hadoop-annotations-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-memcache-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/avro-1.7.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-redis-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/json-smart-2.4.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-servlet-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-stomp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-mqtt-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/asm-5.0.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-ajax-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-compress-1.21.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-3.5.6.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-common-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/paranamer-2.3.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-webapp-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/curator-recipes-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-buffer-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-io-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-rxtx-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-socks-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-security-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/hadoop-auth-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-udt-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-util-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/zookeeper-jute-3.5.6.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http2-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-http-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jettison-1.5.3.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-sctp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-all-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-server-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-haproxy-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-xml-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-cli-1.2.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/curator-client-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-handler-proxy-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jetty-xml-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-smtp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-codec-dns-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/common/hadoop-registry-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/common/hadoop-common-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/common/hadoop-nfs-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/common/hadoop-kms-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-framework-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-http-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-annotations-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/avro-1.7.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-redis-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-servlet-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/asm-5.0.4.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-3.5.6.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-common-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/paranamer-2.3.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-webapp-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-recipes-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-buffer-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-io-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-socks-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.77.Final-osx-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-security-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-auth-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-udt-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-util-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/zookeeper-jute-3.5.6.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http2-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-http-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jettison-1.5.3.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/okio-2.8.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-all-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-server-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.77.Final-osx-aarch_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-xml-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/curator-client-4.2.0.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jetty-xml-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-codec-dns-4.1.77.Final.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.77.Final-linux-x86_64.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-client-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/hdfs/hadoop-hdfs-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5-tests.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-annotations-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/guice-4.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/snakeyaml-1.32.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-common-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/asm-commons-9.3.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/asm-analysis-9.3.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-api-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-client-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-jndi-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/asm-tree-9.3.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-server-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/websocket-servlet-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-plus-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jetty-client-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.48.v20220622.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.3.5/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-common-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-common-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-api-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-router-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-registry-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-api-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-services-core-3.3.5.jar:/content/hadoop-3.3.5/share/hadoop/yarn/hadoop-yarn-client-3.3.5.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 706d88266abcee09ed78fbaa0ad5f74d818ab0e9; compiled by 'stevel' on 2023-03-15T15:56Z\n",
            "STARTUP_MSG:   java = 11.0.18\n",
            "************************************************************/\n",
            "2023-05-14 10:15:22,084 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2023-05-14 10:15:22,250 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\n",
            "2023-05-14 10:15:22,996 INFO namenode.NameNode: Formatting using clusterid: CID-22f0d5a5-5e99-4d08-ba53-6906b3b0ca21\n",
            "2023-05-14 10:15:23,030 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2023-05-14 10:15:23,079 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2023-05-14 10:15:23,081 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2023-05-14 10:15:23,081 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2023-05-14 10:15:23,129 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2023-05-14 10:15:23,129 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2023-05-14 10:15:23,129 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2023-05-14 10:15:23,129 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2023-05-14 10:15:23,130 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2023-05-14 10:15:23,185 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2023-05-14 10:15:23,327 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2023-05-14 10:15:23,327 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2023-05-14 10:15:23,331 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2023-05-14 10:15:23,331 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 14 10:15:23\n",
            "2023-05-14 10:15:23,333 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2023-05-14 10:15:23,333 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-14 10:15:23,334 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2023-05-14 10:15:23,334 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2023-05-14 10:15:23,352 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2023-05-14 10:15:23,352 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2023-05-14 10:15:23,357 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2023-05-14 10:15:23,357 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2023-05-14 10:15:23,358 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2023-05-14 10:15:23,358 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2023-05-14 10:15:23,358 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2023-05-14 10:15:23,358 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2023-05-14 10:15:23,358 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2023-05-14 10:15:23,359 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2023-05-14 10:15:23,359 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2023-05-14 10:15:23,359 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2023-05-14 10:15:23,390 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2023-05-14 10:15:23,390 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2023-05-14 10:15:23,390 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2023-05-14 10:15:23,390 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2023-05-14 10:15:23,406 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2023-05-14 10:15:23,406 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-14 10:15:23,408 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
            "2023-05-14 10:15:23,408 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2023-05-14 10:15:23,415 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2023-05-14 10:15:23,415 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2023-05-14 10:15:23,415 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2023-05-14 10:15:23,415 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2023-05-14 10:15:23,421 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2023-05-14 10:15:23,422 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2023-05-14 10:15:23,426 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2023-05-14 10:15:23,426 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-14 10:15:23,427 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2023-05-14 10:15:23,427 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2023-05-14 10:15:23,436 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2023-05-14 10:15:23,436 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2023-05-14 10:15:23,436 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2023-05-14 10:15:23,440 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2023-05-14 10:15:23,440 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2023-05-14 10:15:23,441 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2023-05-14 10:15:23,442 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-14 10:15:23,442 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n",
            "2023-05-14 10:15:23,442 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2023-05-14 10:15:23,466 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1039803481-172.28.0.12-1684059323457\n",
            "2023-05-14 10:15:23,524 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2023-05-14 10:15:23,562 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2023-05-14 10:15:23,667 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2023-05-14 10:15:23,690 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2023-05-14 10:15:23,778 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2023-05-14 10:15:23,787 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2023-05-14 10:15:23,801 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2023-05-14 10:15:23,805 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 3e5ebbbceaa4/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo -e \"export JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//') \\n\\\n",
        "export HDFS_NAMENODE_USER=root \\n\\\n",
        "export HDFS_DATANODE_USER=root \\n\\\n",
        "export HDFS_SECONDARYNAMENODE_USER=root \\n\\\n",
        "export YARN_RESOURCEMANAGER_USER=root \\n\\\n",
        "export YARN_NODEMANAGER_USER=root\" >> hadoop-3.3.5/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "8cb7lXMptVHb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "3331b16f-ba62-4aea-b0cf-5291d9aa15c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [3e5ebbbceaa4]\n",
            "3e5ebbbceaa4: Warning: Permanently added '3e5ebbbceaa4,172.28.0.12' (ECDSA) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run some simple `hdfs` commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home \n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put /content/sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "9a7ceaf4-8e5d-4a87-86d7-a9c7aafc106c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2023-05-14 10:15 my_dir/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run some simple MrReduce jobs\n",
        "We're going to use the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library. \n",
        "WIth this utility any executable can be used as the mapper and/or the reducer. "
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simplest MapReduce job\n",
        "\n",
        "We are going to use the Unix commands `cat` as mapper and `wc` as reducer so we don't need to code anything. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in acordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job"
      ],
      "metadata": {
        "id": "o6ICcKO2jcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output_simplest \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280afe9d-6afa-4431-91cd-320966c2e6fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_simplest': No such file or directory\n",
            "namenode is running as process 2408.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "2023-05-14 10:15:57,343 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-05-14 10:15:57,447 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-05-14 10:15:57,447 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-05-14 10:15:57,461 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-05-14 10:15:57,858 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-14 10:15:57,881 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-05-14 10:15:58,161 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local356019698_0001\n",
            "2023-05-14 10:15:58,162 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-14 10:15:58,355 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-05-14 10:15:58,356 INFO mapreduce.Job: Running job: job_local356019698_0001\n",
            "2023-05-14 10:15:58,360 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-05-14 10:15:58,362 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2023-05-14 10:15:58,368 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:15:58,368 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:15:58,414 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-05-14 10:15:58,417 INFO mapred.LocalJobRunner: Starting task: attempt_local356019698_0001_m_000000_0\n",
            "2023-05-14 10:15:58,440 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:15:58,442 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:15:58,465 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-05-14 10:15:58,475 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+18289443\n",
            "2023-05-14 10:15:58,514 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2023-05-14 10:15:58,550 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-05-14 10:15:58,550 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-05-14 10:15:58,550 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-05-14 10:15:58,550 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-05-14 10:15:58,550 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-05-14 10:15:58,555 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-05-14 10:15:58,557 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2023-05-14 10:15:58,562 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2023-05-14 10:15:58,563 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2023-05-14 10:15:58,563 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2023-05-14 10:15:58,564 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2023-05-14 10:15:58,564 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2023-05-14 10:15:58,564 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2023-05-14 10:15:58,565 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2023-05-14 10:15:58,566 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2023-05-14 10:15:58,566 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2023-05-14 10:15:58,566 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2023-05-14 10:15:58,567 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-05-14 10:15:58,567 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2023-05-14 10:15:58,706 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:58,708 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:58,724 INFO streaming.PipeMapRed: Records R/W=98/1\n",
            "2023-05-14 10:15:58,725 INFO streaming.PipeMapRed: R/W/S=100/5/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:58,804 INFO streaming.PipeMapRed: R/W/S=1000/784/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:58,967 INFO streaming.PipeMapRed: R/W/S=10000/9962/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:58,971 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-05-14 10:15:58,972 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-05-14 10:15:58,976 INFO mapred.LocalJobRunner: \n",
            "2023-05-14 10:15:58,976 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-05-14 10:15:58,976 INFO mapred.MapTask: Spilling map output\n",
            "2023-05-14 10:15:58,976 INFO mapred.MapTask: bufstart = 0; bufend = 18319443; bufvoid = 104857600\n",
            "2023-05-14 10:15:58,976 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
            "2023-05-14 10:15:59,138 INFO mapred.MapTask: Finished spill 0\n",
            "2023-05-14 10:15:59,165 INFO mapred.Task: Task:attempt_local356019698_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-05-14 10:15:59,172 INFO mapred.LocalJobRunner: Records R/W=98/1\n",
            "2023-05-14 10:15:59,172 INFO mapred.Task: Task 'attempt_local356019698_0001_m_000000_0' done.\n",
            "2023-05-14 10:15:59,178 INFO mapred.Task: Final Counters for attempt_local356019698_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141431\n",
            "\t\tFILE: Number of bytes written=19140183\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=428867584\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "2023-05-14 10:15:59,178 INFO mapred.LocalJobRunner: Finishing task: attempt_local356019698_0001_m_000000_0\n",
            "2023-05-14 10:15:59,179 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-05-14 10:15:59,182 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-05-14 10:15:59,184 INFO mapred.LocalJobRunner: Starting task: attempt_local356019698_0001_r_000000_0\n",
            "2023-05-14 10:15:59,195 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:15:59,195 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:15:59,196 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-05-14 10:15:59,201 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c794c0a\n",
            "2023-05-14 10:15:59,203 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-05-14 10:15:59,224 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-05-14 10:15:59,244 INFO reduce.EventFetcher: attempt_local356019698_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-05-14 10:15:59,283 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local356019698_0001_m_000000_0 decomp: 18359445 len: 18359449 to MEMORY\n",
            "2023-05-14 10:15:59,300 INFO reduce.InMemoryMapOutput: Read 18359445 bytes from map-output for attempt_local356019698_0001_m_000000_0\n",
            "2023-05-14 10:15:59,303 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18359445, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18359445\n",
            "2023-05-14 10:15:59,305 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-05-14 10:15:59,306 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:15:59,306 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-05-14 10:15:59,314 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-05-14 10:15:59,314 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2023-05-14 10:15:59,359 INFO mapreduce.Job: Job job_local356019698_0001 running in uber mode : false\n",
            "2023-05-14 10:15:59,360 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-14 10:15:59,382 INFO reduce.MergeManagerImpl: Merged 1 segments, 18359445 bytes to disk to satisfy reduce memory limit\n",
            "2023-05-14 10:15:59,386 INFO reduce.MergeManagerImpl: Merging 1 files, 18359449 bytes from disk\n",
            "2023-05-14 10:15:59,387 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-05-14 10:15:59,387 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-05-14 10:15:59,388 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2023-05-14 10:15:59,388 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:15:59,389 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2023-05-14 10:15:59,393 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2023-05-14 10:15:59,395 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2023-05-14 10:15:59,431 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:59,432 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:59,440 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:59,471 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:59,621 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:15:59,623 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-05-14 10:15:59,623 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
            "2023-05-14 10:15:59,623 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-05-14 10:15:59,684 INFO mapred.Task: Task:attempt_local356019698_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-05-14 10:15:59,687 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:15:59,688 INFO mapred.Task: Task attempt_local356019698_0001_r_000000_0 is allowed to commit now\n",
            "2023-05-14 10:15:59,709 INFO output.FileOutputCommitter: Saved output of task 'attempt_local356019698_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n",
            "2023-05-14 10:15:59,710 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
            "2023-05-14 10:15:59,711 INFO mapred.Task: Task 'attempt_local356019698_0001_r_000000_0' done.\n",
            "2023-05-14 10:15:59,711 INFO mapred.Task: Final Counters for attempt_local356019698_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36860361\n",
            "\t\tFILE: Number of bytes written=37499632\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=10000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=428867584\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2023-05-14 10:15:59,712 INFO mapred.LocalJobRunner: Finishing task: attempt_local356019698_0001_r_000000_0\n",
            "2023-05-14 10:15:59,712 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-05-14 10:16:00,361 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-14 10:16:00,362 INFO mapreduce.Job: Job job_local356019698_0001 completed successfully\n",
            "2023-05-14 10:16:00,373 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=37001792\n",
            "\t\tFILE: Number of bytes written=56639815\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=36578886\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=20000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=20\n",
            "\t\tTotal committed heap usage (bytes)=857735168\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2023-05-14 10:16:00,373 INFO streaming.StreamJob: Output directory: output_simplest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful. "
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_simplest/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB7FXYTbwNzm",
        "outputId": "c3b92785-bb27-4067-8847-a29b48ee7a29"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000   10000 18299443\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. \n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4. \n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJIm4SPZFPxy",
        "outputId": "e09bca63-bb3e-464a-a0f3-c61b156fdf66"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-14 10:16:03--  https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 214486 (209K) [text/plain]\n",
            "Saving to: ‘Linux_2k.log’\n",
            "\n",
            "\rLinux_2k.log          0%[                    ]       0  --.-KB/s               \rLinux_2k.log        100%[===================>] 209.46K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-05-14 10:16:03 (21.8 MB/s) - ‘Linux_2k.log’ saved [214486/214486]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input || true\n",
        "hdfs dfs -put Linux_2k.log input/ || true"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-rraIUdfdj0",
        "outputId": "fa5bb1fa-307c-490e-e1b6-2634c8f49bb5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qf1dFqIKgoJ",
        "outputId": "fa6378c4-b65d-4db8-8b66-2b57e7992f48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ0kDRsigCC2",
        "outputId": "e48986fb-2eae-4d5a-91da-d0129e3d9c10"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\n",
            "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\n",
            "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_filter \n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output_filter \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7SEzMC2OqWW",
        "outputId": "a33634ab-fd85-49a6-9042-fff41c981760"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py] [] /tmp/streamjob6114287669120504456.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_filter': No such file or directory\n",
            "2023-05-14 10:16:12,724 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2023-05-14 10:16:13,808 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-05-14 10:16:13,935 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-05-14 10:16:13,935 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-05-14 10:16:13,954 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-05-14 10:16:14,322 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-14 10:16:14,337 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-05-14 10:16:14,561 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1956172642_0001\n",
            "2023-05-14 10:16:14,561 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-14 10:16:14,783 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1956172642_0001_a60a15e1-87e0-4f83-8ac6-15cbc3fbe01d/mapper.py\n",
            "2023-05-14 10:16:14,856 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-05-14 10:16:14,858 INFO mapreduce.Job: Running job: job_local1956172642_0001\n",
            "2023-05-14 10:16:14,862 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-05-14 10:16:14,864 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2023-05-14 10:16:14,869 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:16:14,869 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:16:14,916 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-05-14 10:16:14,919 INFO mapred.LocalJobRunner: Starting task: attempt_local1956172642_0001_m_000000_0\n",
            "2023-05-14 10:16:14,944 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:16:14,944 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:16:14,959 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-05-14 10:16:14,966 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+214486\n",
            "2023-05-14 10:16:15,009 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2023-05-14 10:16:15,053 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2023-05-14 10:16:15,059 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2023-05-14 10:16:15,061 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2023-05-14 10:16:15,061 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2023-05-14 10:16:15,061 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2023-05-14 10:16:15,062 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2023-05-14 10:16:15,062 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2023-05-14 10:16:15,063 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2023-05-14 10:16:15,063 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2023-05-14 10:16:15,064 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2023-05-14 10:16:15,064 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2023-05-14 10:16:15,064 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-05-14 10:16:15,065 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2023-05-14 10:16:15,173 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:15,178 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:15,180 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:15,201 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:15,205 INFO streaming.PipeMapRed: Records R/W=1202/1\n",
            "2023-05-14 10:16:15,227 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-05-14 10:16:15,238 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-05-14 10:16:15,241 INFO mapred.LocalJobRunner: \n",
            "2023-05-14 10:16:15,297 INFO mapred.Task: Task:attempt_local1956172642_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-05-14 10:16:15,302 INFO mapred.LocalJobRunner: \n",
            "2023-05-14 10:16:15,302 INFO mapred.Task: Task attempt_local1956172642_0001_m_000000_0 is allowed to commit now\n",
            "2023-05-14 10:16:15,323 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1956172642_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n",
            "2023-05-14 10:16:15,324 INFO mapred.LocalJobRunner: Records R/W=1202/1\n",
            "2023-05-14 10:16:15,324 INFO mapred.Task: Task 'attempt_local1956172642_0001_m_000000_0' done.\n",
            "2023-05-14 10:16:15,330 INFO mapred.Task: Final Counters for attempt_local1956172642_0001_m_000000_0: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=643337\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=405798912\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2023-05-14 10:16:15,330 INFO mapred.LocalJobRunner: Finishing task: attempt_local1956172642_0001_m_000000_0\n",
            "2023-05-14 10:16:15,330 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-05-14 10:16:15,861 INFO mapreduce.Job: Job job_local1956172642_0001 running in uber mode : false\n",
            "2023-05-14 10:16:15,863 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-14 10:16:15,865 INFO mapreduce.Job: Job job_local1956172642_0001 completed successfully\n",
            "2023-05-14 10:16:15,870 INFO mapreduce.Job: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=643337\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=405798912\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2023-05-14 10:16:15,870 INFO streaming.StreamJob: Output directory: output_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output_filter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhLA5HZEhfmT",
        "outputId": "9ab3f9ed-78b3-44c7-c02e-d2c419b8d5a7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-14 10:16 output_filter/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      85436 2023-05-14 10:16 output_filter/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_filter/part-00000 |head "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffi4RvXnPH14",
        "outputId": "e6381405-f810-4aa6-a087-bec9373c1e1d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/local/bin/python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMKEqUF1T-v9",
        "outputId": "838e3463-aecf-4c07-c872-7f8c46fdfd04"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper "
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-R7VNoTVRjL",
        "outputId": "8f7ce95e-22d7-44d5-8bc5-6b0bf26eedf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:logrotate:\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_aggregate \n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -file myAggregatorForKeyCount.py \\\n",
        "  -input input \\\n",
        "  -output output_aggregate \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwxHJ7yyVR34",
        "outputId": "f496e174-e4bd-4f03-bad2-a1f66b3e80c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py, myAggregatorForKeyCount.py] [] /tmp/streamjob15769272635333005703.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_aggregate': No such file or directory\n",
            "2023-05-14 10:16:25,086 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2023-05-14 10:16:26,084 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-05-14 10:16:26,208 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-05-14 10:16:26,208 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-05-14 10:16:26,226 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-05-14 10:16:26,542 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-14 10:16:26,562 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-05-14 10:16:26,787 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local244760557_0001\n",
            "2023-05-14 10:16:26,787 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-14 10:16:27,021 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local244760557_0001_baa69eaa-a3e1-449a-a484-ff2fc7125fea/mapper.py\n",
            "2023-05-14 10:16:27,026 INFO mapred.LocalDistributedCacheManager: Localized file:/content/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local244760557_0001_d271eda6-8476-4281-9933-bb553fdf8191/myAggregatorForKeyCount.py\n",
            "2023-05-14 10:16:27,148 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-05-14 10:16:27,150 INFO mapreduce.Job: Running job: job_local244760557_0001\n",
            "2023-05-14 10:16:27,153 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-05-14 10:16:27,162 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2023-05-14 10:16:27,171 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:16:27,171 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:16:27,236 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-05-14 10:16:27,240 INFO mapred.LocalJobRunner: Starting task: attempt_local244760557_0001_m_000000_0\n",
            "2023-05-14 10:16:27,267 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:16:27,267 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:16:27,290 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-05-14 10:16:27,299 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+214486\n",
            "2023-05-14 10:16:27,347 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2023-05-14 10:16:27,384 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-05-14 10:16:27,384 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-05-14 10:16:27,384 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-05-14 10:16:27,384 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-05-14 10:16:27,384 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-05-14 10:16:27,388 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-05-14 10:16:27,393 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./myAggregatorForKeyCount.py]\n",
            "2023-05-14 10:16:27,398 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2023-05-14 10:16:27,399 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2023-05-14 10:16:27,399 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2023-05-14 10:16:27,399 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2023-05-14 10:16:27,400 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2023-05-14 10:16:27,400 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2023-05-14 10:16:27,404 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2023-05-14 10:16:27,404 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2023-05-14 10:16:27,404 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2023-05-14 10:16:27,405 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2023-05-14 10:16:27,405 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-05-14 10:16:27,406 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2023-05-14 10:16:27,514 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:27,515 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:27,519 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:27,527 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2023-05-14 10:16:27,532 INFO streaming.PipeMapRed: Records R/W=1201/1\n",
            "2023-05-14 10:16:27,560 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2023-05-14 10:16:27,567 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2023-05-14 10:16:27,569 INFO mapred.LocalJobRunner: \n",
            "2023-05-14 10:16:27,569 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-05-14 10:16:27,569 INFO mapred.MapTask: Spilling map output\n",
            "2023-05-14 10:16:27,569 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n",
            "2023-05-14 10:16:27,569 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n",
            "2023-05-14 10:16:27,599 INFO mapred.MapTask: Finished spill 0\n",
            "2023-05-14 10:16:27,610 INFO mapred.Task: Task:attempt_local244760557_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-05-14 10:16:27,616 INFO mapred.LocalJobRunner: Records R/W=1201/1\n",
            "2023-05-14 10:16:27,616 INFO mapred.Task: Task 'attempt_local244760557_0001_m_000000_0' done.\n",
            "2023-05-14 10:16:27,622 INFO mapred.Task: Final Counters for attempt_local244760557_0001_m_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1533\n",
            "\t\tFILE: Number of bytes written=642417\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "2023-05-14 10:16:27,622 INFO mapred.LocalJobRunner: Finishing task: attempt_local244760557_0001_m_000000_0\n",
            "2023-05-14 10:16:27,622 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-05-14 10:16:27,625 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-05-14 10:16:27,625 INFO mapred.LocalJobRunner: Starting task: attempt_local244760557_0001_r_000000_0\n",
            "2023-05-14 10:16:27,635 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-05-14 10:16:27,635 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-05-14 10:16:27,636 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-05-14 10:16:27,641 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33d59621\n",
            "2023-05-14 10:16:27,642 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-05-14 10:16:27,660 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-05-14 10:16:27,671 INFO reduce.EventFetcher: attempt_local244760557_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-05-14 10:16:27,705 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local244760557_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n",
            "2023-05-14 10:16:27,708 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local244760557_0001_m_000000_0\n",
            "2023-05-14 10:16:27,709 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n",
            "2023-05-14 10:16:27,711 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-05-14 10:16:27,714 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:16:27,714 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-05-14 10:16:27,719 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-05-14 10:16:27,719 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2023-05-14 10:16:27,721 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n",
            "2023-05-14 10:16:27,721 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n",
            "2023-05-14 10:16:27,722 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-05-14 10:16:27,722 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-05-14 10:16:27,722 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2023-05-14 10:16:27,723 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:16:27,838 INFO mapred.Task: Task:attempt_local244760557_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-05-14 10:16:27,841 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-05-14 10:16:27,841 INFO mapred.Task: Task attempt_local244760557_0001_r_000000_0 is allowed to commit now\n",
            "2023-05-14 10:16:27,865 INFO output.FileOutputCommitter: Saved output of task 'attempt_local244760557_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n",
            "2023-05-14 10:16:27,866 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-05-14 10:16:27,866 INFO mapred.Task: Task 'attempt_local244760557_0001_r_000000_0' done.\n",
            "2023-05-14 10:16:27,867 INFO mapred.Task: Final Counters for attempt_local244760557_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3129\n",
            "\t\tFILE: Number of bytes written=643199\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=214486\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2023-05-14 10:16:27,867 INFO mapred.LocalJobRunner: Finishing task: attempt_local244760557_0001_r_000000_0\n",
            "2023-05-14 10:16:27,867 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-05-14 10:16:28,169 INFO mapreduce.Job: Job job_local244760557_0001 running in uber mode : false\n",
            "2023-05-14 10:16:28,170 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-14 10:16:28,171 INFO mapreduce.Job: Job job_local244760557_0001 completed successfully\n",
            "2023-05-14 10:16:28,182 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4662\n",
            "\t\tFILE: Number of bytes written=1285616\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=428972\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=60\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=687865856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=214486\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2023-05-14 10:16:28,182 INFO streaming.StreamJob: Output directory: output_aggregate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output_aggregate\n",
        "hdfs dfs -cat output_aggregate/part-00000 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET3KCfX1UC2u",
        "outputId": "2ef9fb2c-bdfd-42b0-c302-deb97397699d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-14 10:16 output_aggregate/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        326 2023-05-14 10:16 output_aggregate/part-00000\n",
            "--\t1\n",
            "bluetooth:\t2\n",
            "cups:\t12\n",
            "ftpd\t916\n",
            "gdm(pam_unix)\t2\n",
            "gdm-binary\t1\n",
            "gpm\t2\n",
            "hcid\t1\n",
            "irqbalance:\t1\n",
            "kernel:\t76\n",
            "klogind\t46\n",
            "login(pam_unix)\t2\n",
            "logrotate:\t43\n",
            "named\t16\n",
            "network:\t2\n",
            "nfslock:\t1\n",
            "portmap:\t1\n",
            "random:\t1\n",
            "rc:\t1\n",
            "rpc.statd\t1\n",
            "rpcidmapd:\t1\n",
            "sdpd\t1\n",
            "snmpd\t1\n",
            "sshd(pam_unix)\t677\n",
            "su(pam_unix)\t172\n",
            "sysctl:\t1\n",
            "syslog:\t2\n",
            "syslogd\t7\n",
            "udev\t8\n",
            "xinetd\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
        "column -t result|sort -k2nr # sort by field 2 numerically in descending order"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8IYl4hAZhZm",
        "outputId": "3a5ba47e-5cd4-489c-d181-cd577154635b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftpd             916\n",
            "sshd(pam_unix)   677\n",
            "su(pam_unix)     172\n",
            "kernel:          76\n",
            "klogind          46\n",
            "logrotate:       43\n",
            "named            16\n",
            "cups:            12\n",
            "udev             8\n",
            "syslogd          7\n",
            "bluetooth:       2\n",
            "gdm(pam_unix)    2\n",
            "gpm              2\n",
            "login(pam_unix)  2\n",
            "network:         2\n",
            "syslog:          2\n",
            "xinetd           2\n",
            "--               1\n",
            "gdm-binary       1\n",
            "hcid             1\n",
            "irqbalance:      1\n",
            "nfslock:         1\n",
            "portmap:         1\n",
            "random:          1\n",
            "rc:              1\n",
            "rpcidmapd:       1\n",
            "rpc.statd        1\n",
            "sdpd             1\n",
            "snmpd            1\n",
            "sysctl:          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.3.4/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoIYG5NlsIMv",
        "outputId": "5e176404-c6ec-4d84-9773-053fe6600692"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./hadoop-3.3.4/sbin/stop-dfs.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop the `sshd` daemon"
      ],
      "metadata": {
        "id": "RGj96_e2ccZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/init.d/ssh stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUvKMpy6chQ5",
        "outputId": "b50f6990-8978-4149-93e0-d22379bcb293"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concluding remarks\n",
        "\n",
        "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands. \n",
        "\n",
        "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
        "\n",
        "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). Alas, I was not able to get the MiniCluster to run due to some uncompatible libraries if I remember correctly. \n",
        "\n",
        "If on one hand it would be great to be able to start a Hadoop cluster with a single command, it is interesting to see how the single components of a Hadoop cluster come into play and it helps to learn about the Hadoop architecture.\n",
        "\n",
        "If you liked this notebook, check also:\n",
        " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
        " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
        " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5N7tb0HSbZB"
      }
    }
  ]
}