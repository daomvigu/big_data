{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53",
      "metadata": {
        "tags": [],
        "id": "a8a9e98d-ccce-4625-a9cd-d73e656f6b53"
      },
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using `IdentityMapper` `IdentityReducer`, thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop\n",
        "\n"
      ],
      "metadata": {
        "id": "Uo12bOJ45USo"
      },
      "id": "Uo12bOJ45USo"
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.3.6.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lGm2Lw-5h7c",
        "outputId": "f07ac812-bcb5-4c25-9efe-6e4288f85d08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.3.6\n"
          ]
        }
      ],
      "id": "5lGm2Lw-5h7c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "zKn5-bIX-yXW"
      },
      "id": "zKn5-bIX-yXW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "2zRCjJUsFlQ8"
      },
      "id": "2zRCjJUsFlQ8"
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAvCdRKI-3dr",
        "outputId": "bfbc29c8-d93f-472e-909b-8c8ceab3e4a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.3.6\n",
            "PATH is hadoop-3.3.6/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "id": "ZAvCdRKI-3dr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "Qnlmws4mAwbO"
      },
      "id": "Qnlmws4mAwbO"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTOc3dH5A5uG",
        "outputId": "0cdfe9e0-9216-4ab3-aaed-c146e74d9f6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ],
      "id": "GTOc3dH5A5uG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "a1eZ9qrpFsAd"
      },
      "id": "a1eZ9qrpFsAd"
    },
    {
      "cell_type": "markdown",
      "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2",
      "metadata": {
        "id": "2fae2658-d9b7-4786-b2e7-7936624d37c2"
      },
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c",
      "metadata": {
        "tags": [],
        "id": "2300d7ba-c485-48c7-948b-ef67fcac6d4c"
      },
      "outputs": [],
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56241317-96b3-40c8-b377-688df5dc0eeb",
      "metadata": {
        "id": "56241317-96b3-40c8-b377-688df5dc0eeb"
      },
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d40f86c-9287-442b-ba6b-b42489700910",
      "metadata": {
        "id": "1d40f86c-9287-442b-ba6b-b42489700910"
      },
      "source": [
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders bt design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a",
      "metadata": {
        "tags": [],
        "id": "0792c4c5-c0e3-4152-8bc0-726e5899109a",
        "outputId": "4b760dd2-a0ff-4271-bfef-915a9cec2c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-03-09 17:57:36,552 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 17:57:36,797 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 17:57:36,797 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 17:57:36,816 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 17:57:37,145 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 17:57:37,171 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 17:57:37,550 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1004027475_0001\n",
            "2024-03-09 17:57:37,550 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 17:57:37,859 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 17:57:37,861 INFO mapreduce.Job: Running job: job_local1004027475_0001\n",
            "2024-03-09 17:57:37,871 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 17:57:37,876 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 17:57:37,889 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:37,890 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:37,956 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 17:57:37,961 INFO mapred.LocalJobRunner: Starting task: attempt_local1004027475_0001_m_000000_0\n",
            "2024-03-09 17:57:37,999 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:37,999 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:38,019 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 17:57:38,030 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 17:57:38,050 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 17:57:38,124 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 17:57:38,124 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 17:57:38,124 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 17:57:38,124 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 17:57:38,124 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 17:57:38,127 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 17:57:38,130 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 17:57:38,137 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 17:57:38,138 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 17:57:38,138 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 17:57:38,139 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 17:57:38,139 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 17:57:38,140 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 17:57:38,141 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 17:57:38,142 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 17:57:38,142 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 17:57:38,142 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 17:57:38,143 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 17:57:38,144 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 17:57:38,169 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 17:57:38,172 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 17:57:38,172 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 17:57:38,173 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 17:57:38,176 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 17:57:38,176 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 17:57:38,176 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 17:57:38,176 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-03-09 17:57:38,176 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 17:57:38,185 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 17:57:38,204 INFO mapred.Task: Task:attempt_local1004027475_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 17:57:38,208 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 17:57:38,208 INFO mapred.Task: Task 'attempt_local1004027475_0001_m_000000_0' done.\n",
            "2024-03-09 17:57:38,216 INFO mapred.Task: Final Counters for attempt_local1004027475_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=781505\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 17:57:38,216 INFO mapred.LocalJobRunner: Finishing task: attempt_local1004027475_0001_m_000000_0\n",
            "2024-03-09 17:57:38,217 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 17:57:38,220 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 17:57:38,221 INFO mapred.LocalJobRunner: Starting task: attempt_local1004027475_0001_r_000000_0\n",
            "2024-03-09 17:57:38,232 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:38,232 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:38,233 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 17:57:38,237 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5054d79c\n",
            "2024-03-09 17:57:38,239 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 17:57:38,264 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 17:57:38,266 INFO reduce.EventFetcher: attempt_local1004027475_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 17:57:38,301 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1004027475_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-03-09 17:57:38,307 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1004027475_0001_m_000000_0\n",
            "2024-03-09 17:57:38,309 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-03-09 17:57:38,312 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 17:57:38,313 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:38,313 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 17:57:38,319 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 17:57:38,320 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 17:57:38,321 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 17:57:38,322 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-03-09 17:57:38,322 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 17:57:38,322 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 17:57:38,323 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-03-09 17:57:38,324 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:38,331 INFO mapred.Task: Task:attempt_local1004027475_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 17:57:38,332 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:38,333 INFO mapred.Task: Task attempt_local1004027475_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 17:57:38,334 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1004027475_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 17:57:38,338 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 17:57:38,338 INFO mapred.Task: Task 'attempt_local1004027475_0001_r_000000_0' done.\n",
            "2024-03-09 17:57:38,339 INFO mapred.Task: Final Counters for attempt_local1004027475_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141488\n",
            "\t\tFILE: Number of bytes written=781555\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 17:57:38,339 INFO mapred.LocalJobRunner: Finishing task: attempt_local1004027475_0001_r_000000_0\n",
            "2024-03-09 17:57:38,339 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 17:57:38,867 INFO mapreduce.Job: Job job_local1004027475_0001 running in uber mode : false\n",
            "2024-03-09 17:57:38,868 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 17:57:38,869 INFO mapreduce.Job: Job job_local1004027475_0001 completed successfully\n",
            "2024-03-09 17:57:38,879 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282898\n",
            "\t\tFILE: Number of bytes written=1563060\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=742391808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 17:57:38,879 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebec78b-fc7a-483d-9408-ed507ac81180",
      "metadata": {
        "id": "8ebec78b-fc7a-483d-9408-ed507ac81180"
      },
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37ca3290-a8d9-44f0-9970-3809fce1aace",
      "metadata": {
        "tags": [],
        "id": "37ca3290-a8d9-44f0-9970-3809fce1aace",
        "outputId": "4aa571d7-97d4-4373-ed7f-2fb34d785f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "4W1AYGwPMQZn"
      },
      "id": "4W1AYGwPMQZn"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E81rOKlPMB7U",
        "outputId": "b8a6752b-ce09-49bf-ee34-09bac0be43af"
      },
      "id": "E81rOKlPMB7U",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-03-09 17:57 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-03-09 17:57 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4tGI1CMdHS",
        "outputId": "9b6d7bc7-abed-483a-9ed9-0dc5c6be0abc"
      },
      "id": "PK4tGI1CMdHS",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Mar  9 17:57 part-00000\n",
            "-rw-r--r-- 1 root root  0 Mar  9 17:57 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "Q5gEVXjD1Wca"
      },
      "id": "Q5gEVXjD1Wca"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb0vmdyMOleV",
        "outputId": "dff21202-d528-40d7-89d9-ea418f08f2ba"
      },
      "id": "Zb0vmdyMOleV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "XHWsKFuHOrUh"
      },
      "id": "XHWsKFuHOrUh"
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geo7HmAq2Gov",
        "outputId": "e7d1e0e8-cc39-4f71-9d69-0c73b589736c"
      },
      "id": "Geo7HmAq2Gov",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-09 17:57:45,160 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR3Vo2YG5lw_",
        "outputId": "00169db4-c722-4eb0-bd51-4c78b848ab9f"
      },
      "id": "dR3Vo2YG5lw_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 17:57:47,091 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 17:57:49,112 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 17:57:49,316 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 17:57:49,316 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 17:57:49,341 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 17:57:49,642 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 17:57:49,675 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 17:57:49,971 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1926281581_0001\n",
            "2024-03-09 17:57:49,971 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 17:57:50,217 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 17:57:50,219 INFO mapreduce.Job: Running job: job_local1926281581_0001\n",
            "2024-03-09 17:57:50,228 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 17:57:50,231 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 17:57:50,240 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:50,245 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:50,320 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 17:57:50,328 INFO mapred.LocalJobRunner: Starting task: attempt_local1926281581_0001_m_000000_0\n",
            "2024-03-09 17:57:50,375 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:50,378 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:50,414 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 17:57:50,434 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 17:57:50,468 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-09 17:57:50,543 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-09 17:57:50,543 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-09 17:57:50,543 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-09 17:57:50,543 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-09 17:57:50,543 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-09 17:57:50,552 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-09 17:57:50,559 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 17:57:50,560 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-09 17:57:50,560 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-09 17:57:50,560 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-03-09 17:57:50,560 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-03-09 17:57:50,567 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-09 17:57:50,587 INFO mapred.Task: Task:attempt_local1926281581_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 17:57:50,591 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 17:57:50,591 INFO mapred.Task: Task 'attempt_local1926281581_0001_m_000000_0' done.\n",
            "2024-03-09 17:57:50,599 INFO mapred.Task: Final Counters for attempt_local1926281581_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779403\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=379584512\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-03-09 17:57:50,599 INFO mapred.LocalJobRunner: Finishing task: attempt_local1926281581_0001_m_000000_0\n",
            "2024-03-09 17:57:50,600 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 17:57:50,604 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-09 17:57:50,604 INFO mapred.LocalJobRunner: Starting task: attempt_local1926281581_0001_r_000000_0\n",
            "2024-03-09 17:57:50,618 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:50,618 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:50,621 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 17:57:50,624 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79672a70\n",
            "2024-03-09 17:57:50,626 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 17:57:50,656 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-09 17:57:50,668 INFO reduce.EventFetcher: attempt_local1926281581_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-09 17:57:50,701 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1926281581_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-03-09 17:57:50,705 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1926281581_0001_m_000000_0\n",
            "2024-03-09 17:57:50,710 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-03-09 17:57:50,713 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-09 17:57:50,719 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:50,719 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-09 17:57:50,727 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 17:57:50,727 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 17:57:50,728 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-09 17:57:50,729 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-03-09 17:57:50,730 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-09 17:57:50,730 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-09 17:57:50,731 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-03-09 17:57:50,731 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:50,739 INFO mapred.Task: Task:attempt_local1926281581_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 17:57:50,740 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-09 17:57:50,740 INFO mapred.Task: Task attempt_local1926281581_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-09 17:57:50,742 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1926281581_0001_r_000000_0' to file:/content/my_output\n",
            "2024-03-09 17:57:50,743 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-03-09 17:57:50,743 INFO mapred.Task: Task 'attempt_local1926281581_0001_r_000000_0' done.\n",
            "2024-03-09 17:57:50,744 INFO mapred.Task: Final Counters for attempt_local1926281581_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141502\n",
            "\t\tFILE: Number of bytes written=779461\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=379584512\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 17:57:50,744 INFO mapred.LocalJobRunner: Finishing task: attempt_local1926281581_0001_r_000000_0\n",
            "2024-03-09 17:57:50,744 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-09 17:57:51,225 INFO mapreduce.Job: Job job_local1926281581_0001 running in uber mode : false\n",
            "2024-03-09 17:57:51,226 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-09 17:57:51,228 INFO mapreduce.Job: Job job_local1926281581_0001 completed successfully\n",
            "2024-03-09 17:57:51,236 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282912\n",
            "\t\tFILE: Number of bytes written=1558864\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=759169024\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 17:57:51,237 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "rEY6DxF8QRak"
      },
      "id": "rEY6DxF8QRak"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7DvyWpZz0kO",
        "outputId": "478ac48e-e24d-43c4-8092-23cdc7540ec6"
      },
      "id": "y7DvyWpZz0kO",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "u7dRikhCQhsY"
      },
      "id": "u7dRikhCQhsY"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAoEeKpyQlAG",
        "outputId": "e4ec08e4-7c6a-43c5-a493-585606d69f69"
      },
      "id": "fAoEeKpyQlAG",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "U5s_wfkvQowG"
      },
      "id": "U5s_wfkvQowG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "48l28qe4SlSp"
      },
      "id": "48l28qe4SlSp"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs1DQ6OISoO6",
        "outputId": "0c0e6832-e1b7-4ff3-d8b3-e1cc9b22e975"
      },
      "id": "Bs1DQ6OISoO6",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 17:57:56,180 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 17:57:58,362 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 17:57:58,542 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 17:57:58,543 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 17:57:58,578 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 17:57:58,825 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 17:57:58,855 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 17:57:59,143 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2038038719_0001\n",
            "2024-03-09 17:57:59,143 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 17:57:59,393 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 17:57:59,395 INFO mapreduce.Job: Running job: job_local2038038719_0001\n",
            "2024-03-09 17:57:59,403 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 17:57:59,406 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 17:57:59,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:59,414 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:59,482 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 17:57:59,487 INFO mapred.LocalJobRunner: Starting task: attempt_local2038038719_0001_m_000000_0\n",
            "2024-03-09 17:57:59,517 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 17:57:59,520 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 17:57:59,549 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 17:57:59,559 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 17:57:59,583 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 17:57:59,616 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 17:57:59,626 INFO mapred.Task: Task:attempt_local2038038719_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 17:57:59,629 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 17:57:59,630 INFO mapred.Task: Task attempt_local2038038719_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 17:57:59,641 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2038038719_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 17:57:59,642 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-03-09 17:57:59,643 INFO mapred.Task: Task 'attempt_local2038038719_0001_m_000000_0' done.\n",
            "2024-03-09 17:57:59,652 INFO mapred.Task: Final Counters for attempt_local2038038719_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=416284672\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 17:57:59,652 INFO mapred.LocalJobRunner: Finishing task: attempt_local2038038719_0001_m_000000_0\n",
            "2024-03-09 17:57:59,656 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 17:58:00,402 INFO mapreduce.Job: Job job_local2038038719_0001 running in uber mode : false\n",
            "2024-03-09 17:58:00,404 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 17:58:00,407 INFO mapreduce.Job: Job job_local2038038719_0001 completed successfully\n",
            "2024-03-09 17:58:00,414 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779367\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=416284672\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-03-09 17:58:00,414 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "s8B4g65TU0vn"
      },
      "id": "s8B4g65TU0vn"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTC44BEmSuw8",
        "outputId": "23721f48-f90b-4bfc-a58a-06c02adc8f8c"
      },
      "id": "kTC44BEmSuw8",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted."
      ],
      "metadata": {
        "id": "h9CzGcNkVBzJ"
      },
      "id": "h9CzGcNkVBzJ"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2qCacnSLX8KT"
      },
      "id": "2qCacnSLX8KT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvoil63X8nQ"
      },
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application"
      ],
      "id": "afvoil63X8nQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29yfN39VYSuh"
      },
      "source": [
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "id": "29yfN39VYSuh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": [],
        "outputId": "3eec5e5c-7ce1-4eec-8354-896dca7bf9d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pVHowpwYSuh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-09 18:06:08,166 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-03-09 18:06:13,364 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-09 18:06:13,672 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-09 18:06:13,673 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-09 18:06:13,704 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-09 18:06:13,976 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-09 18:06:14,000 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-09 18:06:14,315 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local975674594_0001\n",
            "2024-03-09 18:06:14,315 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-09 18:06:14,608 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-09 18:06:14,611 INFO mapreduce.Job: Running job: job_local975674594_0001\n",
            "2024-03-09 18:06:14,619 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-09 18:06:14,623 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-09 18:06:14,635 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:06:14,635 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:06:14,688 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-09 18:06:14,692 INFO mapred.LocalJobRunner: Starting task: attempt_local975674594_0001_m_000000_0\n",
            "2024-03-09 18:06:14,726 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-09 18:06:14,726 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-09 18:06:14,767 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-09 18:06:14,778 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-03-09 18:06:14,797 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-03-09 18:06:14,820 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-03-09 18:06:14,827 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-09 18:06:14,829 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-09 18:06:14,829 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-09 18:06:14,829 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-09 18:06:14,830 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-09 18:06:14,830 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-09 18:06:14,831 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-09 18:06:14,831 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-09 18:06:14,832 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-09 18:06:14,832 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-09 18:06:14,832 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-09 18:06:14,833 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-09 18:06:14,854 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-09 18:06:14,858 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-03-09 18:06:14,863 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-09 18:06:14,863 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-09 18:06:14,866 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:06:14,875 INFO mapred.Task: Task:attempt_local975674594_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-09 18:06:14,877 INFO mapred.LocalJobRunner: \n",
            "2024-03-09 18:06:14,877 INFO mapred.Task: Task attempt_local975674594_0001_m_000000_0 is allowed to commit now\n",
            "2024-03-09 18:06:14,879 INFO output.FileOutputCommitter: Saved output of task 'attempt_local975674594_0001_m_000000_0' to file:/content/my_output\n",
            "2024-03-09 18:06:14,882 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-03-09 18:06:14,882 INFO mapred.Task: Task 'attempt_local975674594_0001_m_000000_0' done.\n",
            "2024-03-09 18:06:14,891 INFO mapred.Task: Final Counters for attempt_local975674594_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:06:14,891 INFO mapred.LocalJobRunner: Finishing task: attempt_local975674594_0001_m_000000_0\n",
            "2024-03-09 18:06:14,892 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-09 18:06:15,628 INFO mapreduce.Job: Job job_local975674594_0001 running in uber mode : false\n",
            "2024-03-09 18:06:15,630 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-09 18:06:15,633 INFO mapreduce.Job: Job job_local975674594_0001 completed successfully\n",
            "2024-03-09 18:06:15,640 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141410\n",
            "\t\tFILE: Number of bytes written=779235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-03-09 18:06:15,640 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "id": "3pVHowpwYSuh"
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0V6lcScZQG5",
        "outputId": "d9f1c489-9c68-496e-fccb-f0935440c9f5"
      },
      "id": "G0V6lcScZQG5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ADBS23)",
      "language": "python",
      "name": "python3_adbs23"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}